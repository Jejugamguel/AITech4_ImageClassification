{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchmetrics import F1Score\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import GPUtil\n",
    "import cv2\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 97% |\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "GPUtil.showUtilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 12\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000002</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>52</td>\n",
       "      <td>000002_female_Asian_52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000004</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>54</td>\n",
       "      <td>000004_male_Asian_54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000005</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>58</td>\n",
       "      <td>000005_female_Asian_58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000006</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>59</td>\n",
       "      <td>000006_female_Asian_59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>006954</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006954_male_Asian_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2696</th>\n",
       "      <td>006955</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006955_male_Asian_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>006956</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006956_male_Asian_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2698</th>\n",
       "      <td>006957</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>20</td>\n",
       "      <td>006957_male_Asian_20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2699</th>\n",
       "      <td>006959</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006959_male_Asian_19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2700 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  gender   race  age                    path\n",
       "0     000001  female  Asian   45  000001_female_Asian_45\n",
       "1     000002  female  Asian   52  000002_female_Asian_52\n",
       "2     000004    male  Asian   54    000004_male_Asian_54\n",
       "3     000005  female  Asian   58  000005_female_Asian_58\n",
       "4     000006  female  Asian   59  000006_female_Asian_59\n",
       "...      ...     ...    ...  ...                     ...\n",
       "2695  006954    male  Asian   19    006954_male_Asian_19\n",
       "2696  006955    male  Asian   19    006955_male_Asian_19\n",
       "2697  006956    male  Asian   19    006956_male_Asian_19\n",
       "2698  006957    male  Asian   20    006957_male_Asian_20\n",
       "2699  006959    male  Asian   19    006959_male_Asian_19\n",
       "\n",
       "[2700 rows x 5 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir_path = '/opt/ml/input/data/train/'\n",
    "train_image_path = '/opt/ml/input/data/train/images/'\n",
    "\n",
    "dt_train = pd.read_csv(train_dir_path+'train.csv')\n",
    "dt_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>path</th>\n",
       "      <th>age_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000002</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>52</td>\n",
       "      <td>000002_female_Asian_52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000004</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>54</td>\n",
       "      <td>000004_male_Asian_54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000005</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>58</td>\n",
       "      <td>000005_female_Asian_58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000006</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>59</td>\n",
       "      <td>000006_female_Asian_59</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>006954</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006954_male_Asian_19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2696</th>\n",
       "      <td>006955</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006955_male_Asian_19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>006956</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006956_male_Asian_19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2698</th>\n",
       "      <td>006957</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>20</td>\n",
       "      <td>006957_male_Asian_20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2699</th>\n",
       "      <td>006959</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006959_male_Asian_19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2700 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  gender   race  age                    path  age_range\n",
       "0     000001  female  Asian   45  000001_female_Asian_45          1\n",
       "1     000002  female  Asian   52  000002_female_Asian_52          1\n",
       "2     000004    male  Asian   54    000004_male_Asian_54          1\n",
       "3     000005  female  Asian   58  000005_female_Asian_58          1\n",
       "4     000006  female  Asian   59  000006_female_Asian_59          1\n",
       "...      ...     ...    ...  ...                     ...        ...\n",
       "2695  006954    male  Asian   19    006954_male_Asian_19          0\n",
       "2696  006955    male  Asian   19    006955_male_Asian_19          0\n",
       "2697  006956    male  Asian   19    006956_male_Asian_19          0\n",
       "2698  006957    male  Asian   20    006957_male_Asian_20          0\n",
       "2699  006959    male  Asian   19    006959_male_Asian_19          0\n",
       "\n",
       "[2700 rows x 6 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_age_range(age):\n",
    "    if age < 30:\n",
    "        return 0\n",
    "    elif 30 <= age < 60:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "    \n",
    "    \n",
    "dt_train['age_range'] = dt_train['age'].apply(lambda x : get_age_range(x))\n",
    "dt_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 데이터 증강은 여기서 쓰면 좋을듯?\n",
    "#### 60세 이상 데이터 6배 증강 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, valid_idx = train_test_split(np.arange(len(dt_train)),\n",
    "                                       test_size=0.2,\n",
    "                                       shuffle=True,\n",
    "                                       stratify=dt_train['age_range'])\n",
    "\n",
    "train_image, train_label = [], []\n",
    "valid_image, valid_label = [], []\n",
    "\n",
    "for idx in train_idx:\n",
    "    path = dt_train.iloc[idx]['path']\n",
    "    for file_name in [i for i in os.listdir(train_image_path+path) if i[0] != '.']:\n",
    "        train_image.append(train_image_path+path+'/'+file_name)\n",
    "        train_label.append((path.split('_')[1], path.split('_')[3], file_name.split('.')[0]))  \n",
    "        \n",
    "for idx in valid_idx:\n",
    "    path = dt_train.iloc[idx]['path']\n",
    "    for file_name in [i for i in os.listdir(train_image_path+path) if i[0] != '.']:\n",
    "        valid_image.append(train_image_path+path+'/'+file_name)\n",
    "        valid_label.append((path.split('_')[1], path.split('_')[3], file_name.split('.')[0])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.Series(train_image)\n",
    "train_label = pd.Series(train_label)\n",
    "\n",
    "valid_data = pd.Series(valid_image)\n",
    "valid_label = pd.Series(valid_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_enc(x):\n",
    "    def gender(i):\n",
    "        if i == 'male':\n",
    "            return 0\n",
    "        elif i == 'female':\n",
    "            return 3\n",
    "    def age(j):\n",
    "        j = int(j)\n",
    "        if j < 30:\n",
    "            return 0\n",
    "        elif j >= 30 and j < 60:\n",
    "            return 1\n",
    "        elif j >= 60:\n",
    "            return 2\n",
    "    def mask(k):\n",
    "        if k == 'normal':\n",
    "            return 12\n",
    "        elif 'incorrect' in k:\n",
    "            return 6\n",
    "        else:\n",
    "            return 0\n",
    "    return gender(x[0]) + age(x[1]) + mask(x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc_sepclass(x):\n",
    "    # x 입력형식 : (성별, 나이, 마스크) 튜플\n",
    "    # 출력형식 : (성별인코딩, 나이인코딩, 마스크인코딩)\n",
    "    def gender(i):\n",
    "        if i == 'male':\n",
    "            return 0\n",
    "        elif i == 'female':\n",
    "            return 1\n",
    "    def age(j):\n",
    "        j = int(j)\n",
    "        if j < 30:\n",
    "            return 0\n",
    "        elif j >= 30 and j < 60:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2\n",
    "    def mask(k):\n",
    "        if k == 'normal':\n",
    "            return 2\n",
    "        elif 'incorrect' in k:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    return gender(x[0]), age(x[1]), mask(x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_Mask(Dataset):\n",
    "    def __init__(self, data, label, encoding=True, midcrop=True, transform=None):\n",
    "        self.encoding = encoding\n",
    "        self.midcrop = midcrop\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.transform = transform\n",
    "        \n",
    "        if encoding:\n",
    "            self.label = self.label.apply(enc_sepclass)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X = cv2.cvtColor(cv2.imread(self.data[idx]), cv2.COLOR_BGR2RGB)\n",
    "        y = self.label[idx]\n",
    "        \n",
    "        if self.midcrop:\n",
    "            X = X[70:420, 17:367]\n",
    "        \n",
    "        if self.transform:\n",
    "            return self.transform(X), y\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_train_set = Dataset_Mask(data=train_data, label=train_label, transform = transforms.Compose([\n",
    "                                transforms.ToTensor()\n",
    "                            ]))\n",
    "mask_val_set = Dataset_Mask(data=valid_data, label=valid_label, transform = transforms.Compose([\n",
    "                                transforms.ToTensor()\n",
    "                            ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f7d85c02130>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def vec2label(x):\n",
    "    return 3*x[0] + x[1] + 6*x[2]\n",
    "\n",
    "ax1 = plt.subplot(121)\n",
    "ax2 = plt.subplot(122)\n",
    "\n",
    "t_image = [mask_train_set[i][1] for i in range(len(mask_train_set))]\n",
    "v_image = [mask_val_set[i][1] for i in range(len(mask_val_set))]\n",
    "\n",
    "t_Series = pd.Series(t_image).apply(vec2label).value_counts().sort_index()\n",
    "v_Series = pd.Series(v_image).apply(vec2label).value_counts().sort_index()\n",
    "\n",
    "t_Series.plot(kind='bar', ax = ax1)\n",
    "v_Series.plot(kind='bar', ax = ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data size : 15120\n",
      "validation data size : 3780\n"
     ]
    }
   ],
   "source": [
    "print(f'training data size : {len(mask_train_set)}')\n",
    "print(f'validation data size : {len(mask_val_set)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_dataloader_mask = DataLoader(dataset = mask_train_set, batch_size=batch_size, num_workers=2)\n",
    "val_dataloader_mask = DataLoader(dataset = mask_val_set, batch_size=batch_size, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained된 파라미터 불러오기 + out1, out2, out3 파라미터 추가\n",
    "weight = torchvision.models.ResNet50_Weights.IMAGENET1K_V1.get_state_dict(progress=True)\n",
    "for i in ['out1.weight', 'out1.bias', 'out2.weight', 'out2.bias', 'out3.weight', 'out3.bias']:\n",
    "    if 'bias' in i:\n",
    "        if 'out1' in i:\n",
    "            weight[i] = torch.randn(2)\n",
    "        else:\n",
    "            weight[i] = torch.rand(3)\n",
    "    else:\n",
    "        if 'out1' in i:\n",
    "            weight[i] = torch.randn(2, 2048)\n",
    "        else:\n",
    "            weight[i] = torch.randn(3, 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class R50_MultiClass(torchvision.models.ResNet):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.out1 = nn.Linear(2048, 2, bias=True)\n",
    "        self.out2 = nn.Linear(2048, 3, bias=True)\n",
    "        self.out3 = nn.Linear(2048, 3, bias=True)\n",
    "        \n",
    "    def _forward_impl(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        # out1 : gender class\n",
    "        # out2 : age class\n",
    "        # out3 : mask class\n",
    "        out1 = self.out1(x)\n",
    "        out2 = self.out2(x)\n",
    "        out3 = self.out3(x)\n",
    "        \n",
    "        return out1, out2, out3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R50_MultiClass(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
      "  (out1): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (out2): Linear(in_features=512, out_features=3, bias=True)\n",
      "  (out3): Linear(in_features=512, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "block = torchvision.models.resnet.Bottleneck\n",
    "layers = [3, 4, 6, 3]\n",
    "\n",
    "basemodel_resnet50 = R50_MultiClass(block = block, layers = layers)\n",
    "basemodel_resnet50.load_state_dict(weight)\n",
    "print(basemodel_resnet50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0303, -0.0255, -0.0251])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "nn.init.xavier_uniform_(basemodel_resnet50.out1.weight)\n",
    "stdv = 1. / math.sqrt(basemodel_resnet50.out1.weight.size(1))\n",
    "basemodel_resnet50.out1.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "nn.init.xavier_uniform_(basemodel_resnet50.out2.weight)\n",
    "stdv = 1. / math.sqrt(basemodel_resnet50.out2.weight.size(1))\n",
    "basemodel_resnet50.out2.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "nn.init.xavier_uniform_(basemodel_resnet50.out3.weight)\n",
    "stdv = 1. / math.sqrt(basemodel_resnet50.out3.weight.size(1))\n",
    "basemodel_resnet50.out3.bias.data.uniform_(-stdv, stdv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"using {device}\")\n",
    "\n",
    "basemodel_resnet50.to(device)\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_EPOCH = 50\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(basemodel_resnet50.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] name:[conv1.weight] shape:[(64, 3, 7, 7)].\n",
      "    val:[ 0.013  0.015 -0.015 -0.023 -0.041]\n",
      "[1] name:[bn1.weight] shape:[(64,)].\n",
      "    val:[0.239 0.291 0.316 0.271 0.217]\n",
      "[2] name:[bn1.bias] shape:[(64,)].\n",
      "    val:[0.225 0.606 0.012 0.133 0.18 ]\n",
      "[3] name:[layer1.0.conv1.weight] shape:[(64, 64, 1, 1)].\n",
      "    val:[ 0.004  0.04  -0.025 -0.028  0.089]\n",
      "[4] name:[layer1.0.bn1.weight] shape:[(64,)].\n",
      "    val:[0.213 0.188 0.141 0.153 0.132]\n",
      "[5] name:[layer1.0.bn1.bias] shape:[(64,)].\n",
      "    val:[ 0.433  0.047 -0.08   0.073  0.28 ]\n",
      "[6] name:[layer1.0.conv2.weight] shape:[(64, 64, 3, 3)].\n",
      "    val:[ 2.092e-09 -1.415e-09  6.585e-09  4.792e-09  3.282e-09]\n",
      "[7] name:[layer1.0.bn2.weight] shape:[(64,)].\n",
      "    val:[2.322e-08 1.381e-01 2.416e-01 1.363e-01 1.158e-01]\n",
      "[8] name:[layer1.0.bn2.bias] shape:[(64,)].\n",
      "    val:[-1.165e-07  3.803e-02  3.519e-01  1.243e-01  2.717e-01]\n",
      "[9] name:[layer1.0.conv3.weight] shape:[(256, 64, 1, 1)].\n",
      "    val:[-4.012e-09  1.773e-02  4.959e-02 -9.115e-03 -5.598e-02]\n",
      "[10] name:[layer1.0.bn3.weight] shape:[(256,)].\n",
      "    val:[0.061 0.    0.037 0.313 0.058]\n",
      "[11] name:[layer1.0.bn3.bias] shape:[(256,)].\n",
      "    val:[-0.005  0.073  0.03   0.054 -0.042]\n",
      "[12] name:[layer1.0.downsample.0.weight] shape:[(256, 64, 1, 1)].\n",
      "    val:[ 0.008 -0.166  0.009  0.009  0.006]\n",
      "[13] name:[layer1.0.downsample.1.weight] shape:[(256,)].\n",
      "    val:[0.245 0.23  0.146 0.343 0.026]\n",
      "[14] name:[layer1.0.downsample.1.bias] shape:[(256,)].\n",
      "    val:[-0.005  0.073  0.03   0.054 -0.042]\n",
      "[15] name:[layer1.1.conv1.weight] shape:[(64, 256, 1, 1)].\n",
      "    val:[0.025 0.003 0.011 0.005 0.009]\n",
      "[16] name:[layer1.1.bn1.weight] shape:[(64,)].\n",
      "    val:[0.154 0.324 0.248 0.211 0.178]\n",
      "[17] name:[layer1.1.bn1.bias] shape:[(64,)].\n",
      "    val:[ 0.058 -0.208 -0.11  -0.011 -0.052]\n",
      "[18] name:[layer1.1.conv2.weight] shape:[(64, 64, 3, 3)].\n",
      "    val:[ 0.015 -0.002  0.013  0.01  -0.013]\n",
      "[19] name:[layer1.1.bn2.weight] shape:[(64,)].\n",
      "    val:[0.3   0.188 0.168 0.17  0.251]\n",
      "[20] name:[layer1.1.bn2.bias] shape:[(64,)].\n",
      "    val:[-0.335 -0.101 -0.01   0.048 -0.096]\n",
      "[21] name:[layer1.1.conv3.weight] shape:[(256, 64, 1, 1)].\n",
      "    val:[ 0.062 -0.004 -0.001  0.001 -0.004]\n",
      "[22] name:[layer1.1.bn3.weight] shape:[(256,)].\n",
      "    val:[ 0.002 -0.007  0.095 -0.011  0.149]\n",
      "[23] name:[layer1.1.bn3.bias] shape:[(256,)].\n",
      "    val:[ 0.004  0.005 -0.066  0.002 -0.049]\n",
      "[24] name:[layer1.2.conv1.weight] shape:[(64, 256, 1, 1)].\n",
      "    val:[ 0.006  0.001 -0.004  0.007 -0.029]\n",
      "[25] name:[layer1.2.bn1.weight] shape:[(64,)].\n",
      "    val:[0.191 0.169 0.1   0.156 0.221]\n",
      "[26] name:[layer1.2.bn1.bias] shape:[(64,)].\n",
      "    val:[-0.021 -0.041 -0.011 -0.132 -0.103]\n",
      "[27] name:[layer1.2.conv2.weight] shape:[(64, 64, 3, 3)].\n",
      "    val:[ 0.033 -0.002 -0.026 -0.002 -0.014]\n",
      "[28] name:[layer1.2.bn2.weight] shape:[(64,)].\n",
      "    val:[0.216 0.251 0.223 0.229 0.221]\n",
      "[29] name:[layer1.2.bn2.bias] shape:[(64,)].\n",
      "    val:[-0.031 -0.092 -0.041 -0.065 -0.103]\n",
      "[30] name:[layer1.2.conv3.weight] shape:[(256, 64, 1, 1)].\n",
      "    val:[ 0.004 -0.003 -0.002  0.    -0.008]\n",
      "[31] name:[layer1.2.bn3.weight] shape:[(256,)].\n",
      "    val:[-0.002  0.004  0.242  0.013  0.2  ]\n",
      "[32] name:[layer1.2.bn3.bias] shape:[(256,)].\n",
      "    val:[ 0.002  0.005 -0.005  0.012 -0.088]\n",
      "[33] name:[layer2.0.conv1.weight] shape:[(128, 256, 1, 1)].\n",
      "    val:[ 0.01   0.015  0.026 -0.013 -0.012]\n",
      "[34] name:[layer2.0.bn1.weight] shape:[(128,)].\n",
      "    val:[0.264 0.187 0.203 0.202 0.203]\n",
      "[35] name:[layer2.0.bn1.bias] shape:[(128,)].\n",
      "    val:[-0.16  -0.092 -0.187 -0.097 -0.029]\n",
      "[36] name:[layer2.0.conv2.weight] shape:[(128, 128, 3, 3)].\n",
      "    val:[ 0.006  0.005  0.007 -0.001  0.009]\n",
      "[37] name:[layer2.0.bn2.weight] shape:[(128,)].\n",
      "    val:[0.225 0.193 0.146 0.247 0.225]\n",
      "[38] name:[layer2.0.bn2.bias] shape:[(128,)].\n",
      "    val:[-0.06   0.15   0.155 -0.06  -0.069]\n",
      "[39] name:[layer2.0.conv3.weight] shape:[(512, 128, 1, 1)].\n",
      "    val:[ 0.008 -0.006  0.013 -0.006  0.017]\n",
      "[40] name:[layer2.0.bn3.weight] shape:[(512,)].\n",
      "    val:[ 0.004  0.202 -0.001  0.139  0.006]\n",
      "[41] name:[layer2.0.bn3.bias] shape:[(512,)].\n",
      "    val:[ 0.05  -0.054 -0.005  0.021  0.047]\n",
      "[42] name:[layer2.0.downsample.0.weight] shape:[(512, 256, 1, 1)].\n",
      "    val:[ 0.07   0.004  0.013  0.013 -0.013]\n",
      "[43] name:[layer2.0.downsample.1.weight] shape:[(512,)].\n",
      "    val:[0.259 0.11  0.002 0.099 0.148]\n",
      "[44] name:[layer2.0.downsample.1.bias] shape:[(512,)].\n",
      "    val:[ 0.05  -0.054 -0.005  0.021  0.047]\n",
      "[45] name:[layer2.1.conv1.weight] shape:[(128, 512, 1, 1)].\n",
      "    val:[ 0.03   0.006 -0.001  0.007  0.034]\n",
      "[46] name:[layer2.1.bn1.weight] shape:[(128,)].\n",
      "    val:[0.114 0.076 0.091 0.112 0.116]\n",
      "[47] name:[layer2.1.bn1.bias] shape:[(128,)].\n",
      "    val:[ 0.069  0.147  0.083  0.041 -0.044]\n",
      "[48] name:[layer2.1.conv2.weight] shape:[(128, 128, 3, 3)].\n",
      "    val:[-0.027 -0.04  -0.017 -0.032 -0.005]\n",
      "[49] name:[layer2.1.bn2.weight] shape:[(128,)].\n",
      "    val:[0.178 0.125 0.182 0.2   0.189]\n",
      "[50] name:[layer2.1.bn2.bias] shape:[(128,)].\n",
      "    val:[-0.018  0.047  0.068  0.102 -0.008]\n",
      "[51] name:[layer2.1.conv3.weight] shape:[(512, 128, 1, 1)].\n",
      "    val:[-0.044  0.021 -0.004  0.019 -0.043]\n",
      "[52] name:[layer2.1.bn3.weight] shape:[(512,)].\n",
      "    val:[ 0.132 -0.004  0.24   0.02   0.141]\n",
      "[53] name:[layer2.1.bn3.bias] shape:[(512,)].\n",
      "    val:[-0.143 -0.041  0.096 -0.002 -0.064]\n",
      "[54] name:[layer2.2.conv1.weight] shape:[(128, 512, 1, 1)].\n",
      "    val:[ 0.007  0.006 -0.02  -0.023  0.008]\n",
      "[55] name:[layer2.2.bn1.weight] shape:[(128,)].\n",
      "    val:[0.107 0.133 0.197 0.204 0.123]\n",
      "[56] name:[layer2.2.bn1.bias] shape:[(128,)].\n",
      "    val:[ 0.137  0.023 -0.077  0.019  0.097]\n",
      "[57] name:[layer2.2.conv2.weight] shape:[(128, 128, 3, 3)].\n",
      "    val:[-0.003 -0.003  0.     0.011 -0.003]\n",
      "[58] name:[layer2.2.bn2.weight] shape:[(128,)].\n",
      "    val:[0.221 0.172 0.218 0.196 0.174]\n",
      "[59] name:[layer2.2.bn2.bias] shape:[(128,)].\n",
      "    val:[-0.069 -0.031 -0.073 -0.035 -0.019]\n",
      "[60] name:[layer2.2.conv3.weight] shape:[(512, 128, 1, 1)].\n",
      "    val:[-0.017  0.027 -0.006 -0.001 -0.018]\n",
      "[61] name:[layer2.2.bn3.weight] shape:[(512,)].\n",
      "    val:[0.057 0.051 0.07  0.201 0.213]\n",
      "[62] name:[layer2.2.bn3.bias] shape:[(512,)].\n",
      "    val:[-0.066  0.062 -0.109 -0.054 -0.129]\n",
      "[63] name:[layer2.3.conv1.weight] shape:[(128, 512, 1, 1)].\n",
      "    val:[ 0.004 -0.019  0.016  0.007 -0.017]\n",
      "[64] name:[layer2.3.bn1.weight] shape:[(128,)].\n",
      "    val:[0.193 0.174 0.202 0.143 0.198]\n",
      "[65] name:[layer2.3.bn1.bias] shape:[(128,)].\n",
      "    val:[-0.121 -0.008 -0.105  0.044 -0.088]\n",
      "[66] name:[layer2.3.conv2.weight] shape:[(128, 128, 3, 3)].\n",
      "    val:[ 0.006  0.008 -0.006 -0.003  0.005]\n",
      "[67] name:[layer2.3.bn2.weight] shape:[(128,)].\n",
      "    val:[0.207 0.247 0.168 0.243 0.216]\n",
      "[68] name:[layer2.3.bn2.bias] shape:[(128,)].\n",
      "    val:[-0.062 -0.164 -0.035 -0.12  -0.068]\n",
      "[69] name:[layer2.3.conv3.weight] shape:[(512, 128, 1, 1)].\n",
      "    val:[-0.002 -0.008 -0.022  0.    -0.011]\n",
      "[70] name:[layer2.3.bn3.weight] shape:[(512,)].\n",
      "    val:[0.012 0.117 0.014 0.098 0.062]\n",
      "[71] name:[layer2.3.bn3.bias] shape:[(512,)].\n",
      "    val:[-0.011  0.123  0.018 -0.147 -0.102]\n",
      "[72] name:[layer3.0.conv1.weight] shape:[(256, 512, 1, 1)].\n",
      "    val:[ 0.014  0.009  0.019  0.022 -0.027]\n",
      "[73] name:[layer3.0.bn1.weight] shape:[(256,)].\n",
      "    val:[0.195 0.26  0.203 0.175 0.222]\n",
      "[74] name:[layer3.0.bn1.bias] shape:[(256,)].\n",
      "    val:[-0.072 -0.212 -0.078  0.02  -0.161]\n",
      "[75] name:[layer3.0.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[-0.036 -0.044 -0.032 -0.03  -0.01 ]\n",
      "[76] name:[layer3.0.bn2.weight] shape:[(256,)].\n",
      "    val:[0.18  0.231 0.181 0.153 0.162]\n",
      "[77] name:[layer3.0.bn2.bias] shape:[(256,)].\n",
      "    val:[ 0.077 -0.044 -0.001  0.155 -0.034]\n",
      "[78] name:[layer3.0.conv3.weight] shape:[(1024, 256, 1, 1)].\n",
      "    val:[-0.01   0.02   0.037 -0.08   0.026]\n",
      "[79] name:[layer3.0.bn3.weight] shape:[(1024,)].\n",
      "    val:[0.161 0.116 0.089 0.118 0.087]\n",
      "[80] name:[layer3.0.bn3.bias] shape:[(1024,)].\n",
      "    val:[-0.014 -0.025 -0.027  0.003 -0.029]\n",
      "[81] name:[layer3.0.downsample.0.weight] shape:[(1024, 512, 1, 1)].\n",
      "    val:[ 0.029  0.021  0.008 -0.008 -0.002]\n",
      "[82] name:[layer3.0.downsample.1.weight] shape:[(1024,)].\n",
      "    val:[0.105 0.095 0.067 0.136 0.07 ]\n",
      "[83] name:[layer3.0.downsample.1.bias] shape:[(1024,)].\n",
      "    val:[-0.014 -0.025 -0.027  0.003 -0.029]\n",
      "[84] name:[layer3.1.conv1.weight] shape:[(256, 1024, 1, 1)].\n",
      "    val:[ 0.004 -0.013  0.002  0.002 -0.005]\n",
      "[85] name:[layer3.1.bn1.weight] shape:[(256,)].\n",
      "    val:[0.127 0.108 0.112 0.113 0.14 ]\n",
      "[86] name:[layer3.1.bn1.bias] shape:[(256,)].\n",
      "    val:[ 0.122  0.085  0.044  0.046 -0.021]\n",
      "[87] name:[layer3.1.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[-0.002  0.002 -0.009  0.003 -0.002]\n",
      "[88] name:[layer3.1.bn2.weight] shape:[(256,)].\n",
      "    val:[0.173 0.207 0.187 0.167 0.174]\n",
      "[89] name:[layer3.1.bn2.bias] shape:[(256,)].\n",
      "    val:[-0.068 -0.085 -0.039 -0.035 -0.09 ]\n",
      "[90] name:[layer3.1.conv3.weight] shape:[(1024, 256, 1, 1)].\n",
      "    val:[-0.006  0.024  0.002  0.014  0.003]\n",
      "[91] name:[layer3.1.bn3.weight] shape:[(1024,)].\n",
      "    val:[0.048 0.082 0.112 0.089 0.113]\n",
      "[92] name:[layer3.1.bn3.bias] shape:[(1024,)].\n",
      "    val:[-0.065 -0.026 -0.006 -0.038 -0.011]\n",
      "[93] name:[layer3.2.conv1.weight] shape:[(256, 1024, 1, 1)].\n",
      "    val:[ 0.003 -0.028 -0.003 -0.006 -0.021]\n",
      "[94] name:[layer3.2.bn1.weight] shape:[(256,)].\n",
      "    val:[0.223 0.187 0.117 0.134 0.177]\n",
      "[95] name:[layer3.2.bn1.bias] shape:[(256,)].\n",
      "    val:[-0.115 -0.199  0.063  0.042 -0.096]\n",
      "[96] name:[layer3.2.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[-0.004 -0.011  0.001  0.012  0.02 ]\n",
      "[97] name:[layer3.2.bn2.weight] shape:[(256,)].\n",
      "    val:[0.097 0.181 0.194 0.164 0.211]\n",
      "[98] name:[layer3.2.bn2.bias] shape:[(256,)].\n",
      "    val:[ 0.126 -0.059 -0.153 -0.047 -0.118]\n",
      "[99] name:[layer3.2.conv3.weight] shape:[(1024, 256, 1, 1)].\n",
      "    val:[-0.024  0.009  0.018 -0.008 -0.012]\n",
      "[100] name:[layer3.2.bn3.weight] shape:[(1024,)].\n",
      "    val:[0.087 0.082 0.131 0.068 0.106]\n",
      "[101] name:[layer3.2.bn3.bias] shape:[(1024,)].\n",
      "    val:[ 0.069 -0.059 -0.013 -0.048 -0.013]\n",
      "[102] name:[layer3.3.conv1.weight] shape:[(256, 1024, 1, 1)].\n",
      "    val:[-0.015 -0.01  -0.     0.002 -0.018]\n",
      "[103] name:[layer3.3.bn1.weight] shape:[(256,)].\n",
      "    val:[0.223 0.174 0.181 0.174 0.211]\n",
      "[104] name:[layer3.3.bn1.bias] shape:[(256,)].\n",
      "    val:[-0.149 -0.11  -0.135 -0.125 -0.153]\n",
      "[105] name:[layer3.3.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[-0.02  -0.019 -0.018  0.003  0.025]\n",
      "[106] name:[layer3.3.bn2.weight] shape:[(256,)].\n",
      "    val:[0.117 0.149 0.112 0.196 0.132]\n",
      "[107] name:[layer3.3.bn2.bias] shape:[(256,)].\n",
      "    val:[ 0.046  0.011  0.054 -0.103 -0.004]\n",
      "[108] name:[layer3.3.conv3.weight] shape:[(1024, 256, 1, 1)].\n",
      "    val:[ 0.003  0.015 -0.04  -0.016  0.025]\n",
      "[109] name:[layer3.3.bn3.weight] shape:[(1024,)].\n",
      "    val:[0.036 0.134 0.107 0.034 0.106]\n",
      "[110] name:[layer3.3.bn3.bias] shape:[(1024,)].\n",
      "    val:[ 0.004 -0.052 -0.089 -0.036 -0.052]\n",
      "[111] name:[layer3.4.conv1.weight] shape:[(256, 1024, 1, 1)].\n",
      "    val:[-0.034  0.032 -0.02  -0.018 -0.024]\n",
      "[112] name:[layer3.4.bn1.weight] shape:[(256,)].\n",
      "    val:[0.129 0.181 0.143 0.15  0.164]\n",
      "[113] name:[layer3.4.bn1.bias] shape:[(256,)].\n",
      "    val:[ 0.001 -0.072 -0.015 -0.066 -0.061]\n",
      "[114] name:[layer3.4.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[-0.009  0.008  0.01  -0.029 -0.009]\n",
      "[115] name:[layer3.4.bn2.weight] shape:[(256,)].\n",
      "    val:[0.201 0.152 0.126 0.187 0.192]\n",
      "[116] name:[layer3.4.bn2.bias] shape:[(256,)].\n",
      "    val:[-0.153  0.019  0.006 -0.108 -0.188]\n",
      "[117] name:[layer3.4.conv3.weight] shape:[(1024, 256, 1, 1)].\n",
      "    val:[-0.001  0.002  0.002  0.002  0.002]\n",
      "[118] name:[layer3.4.bn3.weight] shape:[(1024,)].\n",
      "    val:[0.001 0.183 0.151 0.037 0.11 ]\n",
      "[119] name:[layer3.4.bn3.bias] shape:[(1024,)].\n",
      "    val:[-0.017 -0.213 -0.143 -0.043 -0.102]\n",
      "[120] name:[layer3.5.conv1.weight] shape:[(256, 1024, 1, 1)].\n",
      "    val:[-0.001  0.018  0.047 -0.032  0.008]\n",
      "[121] name:[layer3.5.bn1.weight] shape:[(256,)].\n",
      "    val:[0.195 0.21  0.121 0.215 0.201]\n",
      "[122] name:[layer3.5.bn1.bias] shape:[(256,)].\n",
      "    val:[-0.179 -0.21   0.08  -0.166 -0.197]\n",
      "[123] name:[layer3.5.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[-0.018 -0.037 -0.024  0.003  0.026]\n",
      "[124] name:[layer3.5.bn2.weight] shape:[(256,)].\n",
      "    val:[0.18  0.16  0.178 0.206 0.17 ]\n",
      "[125] name:[layer3.5.bn2.bias] shape:[(256,)].\n",
      "    val:[-0.095 -0.043 -0.065 -0.111 -0.137]\n",
      "[126] name:[layer3.5.conv3.weight] shape:[(1024, 256, 1, 1)].\n",
      "    val:[-0.001  0.002  0.001 -0.003  0.003]\n",
      "[127] name:[layer3.5.bn3.weight] shape:[(1024,)].\n",
      "    val:[0.01  0.241 0.129 0.07  0.121]\n",
      "[128] name:[layer3.5.bn3.bias] shape:[(1024,)].\n",
      "    val:[-0.01  -0.278 -0.168 -0.087 -0.159]\n",
      "[129] name:[layer4.0.conv1.weight] shape:[(512, 1024, 1, 1)].\n",
      "    val:[-0.011  0.006  0.009 -0.024 -0.005]\n",
      "[130] name:[layer4.0.bn1.weight] shape:[(512,)].\n",
      "    val:[0.246 0.203 0.214 0.194 0.251]\n",
      "[131] name:[layer4.0.bn1.bias] shape:[(512,)].\n",
      "    val:[-0.231 -0.176 -0.094 -0.153 -0.271]\n",
      "[132] name:[layer4.0.conv2.weight] shape:[(512, 512, 3, 3)].\n",
      "    val:[-1.893e-05 -7.486e-03  9.649e-03 -2.248e-03 -2.001e-04]\n",
      "[133] name:[layer4.0.bn2.weight] shape:[(512,)].\n",
      "    val:[0.23  0.215 0.211 0.25  0.199]\n",
      "[134] name:[layer4.0.bn2.bias] shape:[(512,)].\n",
      "    val:[-0.092 -0.141 -0.052 -0.1   -0.065]\n",
      "[135] name:[layer4.0.conv3.weight] shape:[(2048, 512, 1, 1)].\n",
      "    val:[-0.001  0.003 -0.011 -0.012 -0.012]\n",
      "[136] name:[layer4.0.bn3.weight] shape:[(2048,)].\n",
      "    val:[0.314 0.425 0.397 0.362 0.493]\n",
      "[137] name:[layer4.0.bn3.bias] shape:[(2048,)].\n",
      "    val:[-0.059 -0.092 -0.07  -0.053 -0.066]\n",
      "[138] name:[layer4.0.downsample.0.weight] shape:[(2048, 1024, 1, 1)].\n",
      "    val:[-0.001 -0.005  0.002 -0.013 -0.005]\n",
      "[139] name:[layer4.0.downsample.1.weight] shape:[(2048,)].\n",
      "    val:[0.239 0.275 0.259 0.218 0.394]\n",
      "[140] name:[layer4.0.downsample.1.bias] shape:[(2048,)].\n",
      "    val:[-0.059 -0.092 -0.07  -0.053 -0.066]\n",
      "[141] name:[layer4.1.conv1.weight] shape:[(512, 2048, 1, 1)].\n",
      "    val:[-0.023 -0.001 -0.015  0.012 -0.038]\n",
      "[142] name:[layer4.1.bn1.weight] shape:[(512,)].\n",
      "    val:[0.188 0.193 0.172 0.2   0.205]\n",
      "[143] name:[layer4.1.bn1.bias] shape:[(512,)].\n",
      "    val:[-0.127 -0.109 -0.072 -0.147 -0.161]\n",
      "[144] name:[layer4.1.conv2.weight] shape:[(512, 512, 3, 3)].\n",
      "    val:[-0.007 -0.007 -0.003 -0.014 -0.016]\n",
      "[145] name:[layer4.1.bn2.weight] shape:[(512,)].\n",
      "    val:[0.188 0.226 0.186 0.223 0.229]\n",
      "[146] name:[layer4.1.bn2.bias] shape:[(512,)].\n",
      "    val:[-0.138 -0.144 -0.085 -0.15  -0.188]\n",
      "[147] name:[layer4.1.conv3.weight] shape:[(2048, 512, 1, 1)].\n",
      "    val:[ 0.001 -0.003  0.005 -0.013 -0.011]\n",
      "[148] name:[layer4.1.bn3.weight] shape:[(2048,)].\n",
      "    val:[0.287 0.502 0.329 0.263 0.383]\n",
      "[149] name:[layer4.1.bn3.bias] shape:[(2048,)].\n",
      "    val:[-0.092 -0.079 -0.074 -0.054 -0.05 ]\n",
      "[150] name:[layer4.2.conv1.weight] shape:[(512, 2048, 1, 1)].\n",
      "    val:[0.014 0.013 0.003 0.001 0.031]\n",
      "[151] name:[layer4.2.bn1.weight] shape:[(512,)].\n",
      "    val:[0.214 0.204 0.213 0.221 0.211]\n",
      "[152] name:[layer4.2.bn1.bias] shape:[(512,)].\n",
      "    val:[-0.156 -0.156 -0.187 -0.21  -0.167]\n",
      "[153] name:[layer4.2.conv2.weight] shape:[(512, 512, 3, 3)].\n",
      "    val:[0.004 0.006 0.008 0.006 0.009]\n",
      "[154] name:[layer4.2.bn2.weight] shape:[(512,)].\n",
      "    val:[0.226 0.181 0.184 0.235 0.244]\n",
      "[155] name:[layer4.2.bn2.bias] shape:[(512,)].\n",
      "    val:[-0.1   -0.009 -0.059 -0.143 -0.169]\n",
      "[156] name:[layer4.2.conv3.weight] shape:[(2048, 512, 1, 1)].\n",
      "    val:[ 0.005  0.008 -0.024 -0.003 -0.004]\n",
      "[157] name:[layer4.2.bn3.weight] shape:[(2048,)].\n",
      "    val:[0.643 0.832 0.786 0.675 0.673]\n",
      "[158] name:[layer4.2.bn3.bias] shape:[(2048,)].\n",
      "    val:[0.008 0.055 0.035 0.019 0.042]\n",
      "[159] name:[fc.weight] shape:[(1000, 2048)].\n",
      "    val:[-0.009  0.015 -0.037  0.002 -0.044]\n",
      "[160] name:[fc.bias] shape:[(1000,)].\n",
      "    val:[-0.009 -0.004 -0.021 -0.018  0.01 ]\n",
      "[161] name:[out1.weight] shape:[(2, 512)].\n",
      "    val:[ 0.092  0.048  0.058 -0.081 -0.1  ]\n",
      "[162] name:[out1.bias] shape:[(2,)].\n",
      "    val:[-0.022 -0.019]\n",
      "[163] name:[out2.weight] shape:[(3, 512)].\n",
      "    val:[-0.005  0.056  0.061 -0.1    0.015]\n",
      "[164] name:[out2.bias] shape:[(3,)].\n",
      "    val:[-0.03   0.003  0.028]\n",
      "[165] name:[out3.weight] shape:[(3, 512)].\n",
      "    val:[ 0.023  0.105  0.081  0.096 -0.007]\n",
      "[166] name:[out3.bias] shape:[(3,)].\n",
      "    val:[-0.03  -0.025 -0.025]\n",
      "Total number of parameters:[25,561,136].\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "n_param = 0\n",
    "for p_idx, (param_name, param) in enumerate(basemodel_resnet50.named_parameters()):\n",
    "    if param.requires_grad:\n",
    "        param_numpy = param.detach().cpu().numpy()\n",
    "        n_param += len(param_numpy.reshape(-1))\n",
    "        print (\"[%d] name:[%s] shape:[%s].\"%(p_idx,param_name,param_numpy.shape))\n",
    "        print (\"    val:%s\"%(param_numpy.reshape(-1)[:5]))\n",
    "print (\"Total number of parameters:[%s].\"%(format(n_param,',d')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 480.00 MiB (GPU 0; 31.75 GiB total capacity; 10.04 GiB already allocated; 352.50 MiB free; 10.06 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/opt/ml/cv-12/Ela_MultiClass/Res50_pre_DS.ipynb 셀 24\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2241495354414745227d/opt/ml/cv-12/Ela_MultiClass/Res50_pre_DS.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m age \u001b[39m=\u001b[39m age\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2241495354414745227d/opt/ml/cv-12/Ela_MultiClass/Res50_pre_DS.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m msk \u001b[39m=\u001b[39m msk\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2241495354414745227d/opt/ml/cv-12/Ela_MultiClass/Res50_pre_DS.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m p_gen, p_age, p_msk \u001b[39m=\u001b[39m basemodel_resnet50(inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2241495354414745227d/opt/ml/cv-12/Ela_MultiClass/Res50_pre_DS.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m p_gen, p_age, p_msk \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(p_gen), F\u001b[39m.\u001b[39msoftmax(p_age), F\u001b[39m.\u001b[39msoftmax(p_msk)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2241495354414745227d/opt/ml/cv-12/Ela_MultiClass/Res50_pre_DS.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(p_gen, gen) \u001b[39m+\u001b[39m \u001b[39m2\u001b[39m\u001b[39m*\u001b[39mcriterion(p_age, age) \u001b[39m+\u001b[39m criterion(p_msk, msk)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_impl(x)\n",
      "\u001b[1;32m/opt/ml/cv-12/Ela_MultiClass/Res50_pre_DS.ipynb 셀 24\u001b[0m in \u001b[0;36mR50_MultiClass._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2241495354414745227d/opt/ml/cv-12/Ela_MultiClass/Res50_pre_DS.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_forward_impl\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2241495354414745227d/opt/ml/cv-12/Ela_MultiClass/Res50_pre_DS.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2241495354414745227d/opt/ml/cv-12/Ela_MultiClass/Res50_pre_DS.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbn1(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2241495354414745227d/opt/ml/cv-12/Ela_MultiClass/Res50_pre_DS.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2241495354414745227d/opt/ml/cv-12/Ela_MultiClass/Res50_pre_DS.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxpool(x)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m    172\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    173\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[1;32m    175\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[1;32m    176\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    177\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    178\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    180\u001b[0m     bn_training,\n\u001b[1;32m    181\u001b[0m     exponential_average_factor,\n\u001b[1;32m    182\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[1;32m    183\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:2450\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2447\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[1;32m   2448\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m-> 2450\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m   2451\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[1;32m   2452\u001b[0m )\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 480.00 MiB (GPU 0; 31.75 GiB total capacity; 10.04 GiB already allocated; 352.50 MiB free; 10.06 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "best_val_acc = 0\n",
    "best_val_loss = np.inf\n",
    "\n",
    "patience = 10\n",
    "cur_count = 0\n",
    "class_num = 18\n",
    "\n",
    "#f1 = F1Score(num_classes = 4, average='macro', mdmc_reduce='samplewise').to(device)\n",
    "#best_f1_score = 0\n",
    "\n",
    "\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    matches = 0\n",
    "    loss_value = 0\n",
    "    basemodel_resnet50.train()\n",
    "\n",
    "    # 학습 시작!\n",
    "    for inputs, labels in train_dataloader_mask:\n",
    "        inputs = inputs.to(device)\n",
    "        gen, age, msk = labels\n",
    "        gen = gen.to(device)\n",
    "        age = age.to(device)\n",
    "        msk = msk.to(device)\n",
    "        \n",
    "        p_gen, p_age, p_msk = basemodel_resnet50(inputs)\n",
    "        p_gen, p_age, p_msk = F.softmax(p_gen), F.softmax(p_age), F.softmax(p_msk)\n",
    "        loss = criterion(p_gen, gen) + 2*criterion(p_age, age) + criterion(p_msk, msk)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # epoch 5마다 모델 저장\n",
    "        if (epoch+1) % 5 == 0:\n",
    "            torch.save(basemodel_resnet50, '/opt/ml/checkpoint/resnet50/checkpoint_ep_{}.pt'.format(epoch+1))\n",
    "\n",
    "        loss_value = loss.item()\n",
    "        matches = ((torch.argmax(p_gen, -1) == gen) & (torch.argmax(p_age, -1) == age) & (torch.argmax(p_msk, -1) == msk)).sum().item()\n",
    "\n",
    "        train_loss = loss_value / len(labels)\n",
    "        train_acc = matches / len(labels)\n",
    "    print(f\"epoch[{epoch+1}/{NUM_EPOCH}] training loss {train_loss:.3f}, training accuracy {train_acc:.3f}\")        \n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        basemodel_resnet50.eval()\n",
    "        val_loss_items = []\n",
    "        val_acc_items = []\n",
    "        \n",
    "        for inputs, labels in val_dataloader_mask:\n",
    "            inputs = inputs.to(device)\n",
    "            gen, age, msk = labels\n",
    "            gen = gen.to(device)\n",
    "            age = age.to(device)\n",
    "            msk = msk.to(device)\n",
    "            \n",
    "            result = basemodel_resnet50(inputs)\n",
    "            p_gen, p_age, p_msk = result\n",
    "            p_gen, p_age, p_msk = F.softmax(p_gen), F.softmax(p_age), F.softmax(p_msk)\n",
    "            loss = criterion(p_gen, gen) + 2*criterion(p_age, age) + criterion(p_msk, msk)\n",
    "            \n",
    "            loss_item = loss.item()\n",
    "            acc_item = ((torch.argmax(p_gen, -1) == gen) & (torch.argmax(p_age, -1) == age) & (torch.argmax(p_msk, -1) == msk)).sum().item()\n",
    "            val_loss_items.append(loss_item)\n",
    "            val_acc_items.append(acc_item)\n",
    "            \n",
    "        val_loss = np.sum(val_loss_items) / len(labels)\n",
    "        val_acc = np.sum(val_acc_items) / len(labels)\n",
    "        #f1_score = f1(result, labels)\n",
    "            \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "        #if f1_score > best_f1_score:\n",
    "        #    best_f1_score = f1_score\n",
    "        #    torch.save(basemodel_resnet50, '/opt/ml/checkpoint/resnet50/checkpoint_best.pt')\n",
    "        \n",
    "        print(f\"[val] acc : {val_acc:.3f}, loss : {val_loss:.3f}\")\n",
    "        print(f\"best acc : {best_val_acc:.3f}, best loss : {best_val_loss:.3f}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f'Best f1 score:{best_f1_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageID</th>\n",
       "      <th>ans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cbc5c6e168e63498590db46022617123f1fe1268.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0e72482bf56b3581c081f7da2a6180b8792c7089.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b549040c49190cedc41327748aeb197c1670f14d.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4f9cb2a045c6d5b9e50ad3459ea7b791eb6e18bc.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>248428d9a4a5b6229a7081c32851b90cb8d38d0c.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        ImageID  ans\n",
       "0  cbc5c6e168e63498590db46022617123f1fe1268.jpg    0\n",
       "1  0e72482bf56b3581c081f7da2a6180b8792c7089.jpg    0\n",
       "2  b549040c49190cedc41327748aeb197c1670f14d.jpg    0\n",
       "3  4f9cb2a045c6d5b9e50ad3459ea7b791eb6e18bc.jpg    0\n",
       "4  248428d9a4a5b6229a7081c32851b90cb8d38d0c.jpg    0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "test_dir_path = '/opt/ml/input/data/eval/'\n",
    "test_image_path = '/opt/ml/input/data/eval/images/'\n",
    "\n",
    "#basemodel_resnet50 = torch.load('/opt/ml/checkpoint/resnet50/checkpoint_ep_30.pt')\n",
    "submission = pd.read_csv(test_dir_path+'info.csv')\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = [os.path.join(test_image_path, img_id) for img_id in submission.ImageID]\n",
    "test_image = pd.Series(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test_Dataset(Dataset):\n",
    "    def __init__(self, midcrop=True, transform=None):\n",
    "        self.midcrop = midcrop\n",
    "        self.data = test_image\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(test_image)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.cvtColor(cv2.imread(self.data[idx]), cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.midcrop:\n",
    "            img = img[70:420, 17:367]\n",
    "            \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test inference is done!\n"
     ]
    }
   ],
   "source": [
    "dataset = Test_Dataset(transform = transforms.Compose([\n",
    "                            transforms.ToTensor()\n",
    "                        ]))\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "# 모델을 정의합니다. (학습한 모델이 있다면 torch.load로 모델을 불러주세요!)\n",
    "device = torch.device('cuda')\n",
    "model = basemodel_resnet50.to(device)\n",
    "model.eval()\n",
    "\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "all_predictions = []\n",
    "for images in loader:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        p_gen, p_age, p_msk = model(images)\n",
    "        \n",
    "        ar_gen = p_gen.argmax(dim=-1)\n",
    "        ar_age = p_age.argmax(dim=-1)\n",
    "        ar_msk = p_msk.argmax(dim=-1)\n",
    "        \n",
    "        total = ar_gen * 3 + ar_age + ar_msk * 6\n",
    "        \n",
    "        all_predictions.extend(total.cpu().numpy())\n",
    "submission['ans'] = all_predictions\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(test_dir_path, 'submission.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86.13492063492063"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 결과비교 (자체, 현재 제출된 것 중 가장 결과가 좋은 것과 비교)\n",
    "# 기준결과의 accuracy가 a%, 비교결과가 b%인 경우\n",
    "#  -> 현재 결과의 accuracy 범위 : a+b-100(%) ~ a-b+100(%)\n",
    "standard = pd.read_csv('~/cv-12/log/standard_1028.csv')['ans']\n",
    "100 * sum(standard == submission['ans']) / len(standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fdb5b7ddca0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test set과의 분포 비교\n",
    "\n",
    "ax11 = plt.subplot(121)\n",
    "ax22 = plt.subplot(122)\n",
    "\n",
    "# 왼쪽 : 평가데이터 결과  /  오른쪽 : 훈련데이터 일부 결과\n",
    "test_Series = submission['ans'].value_counts().sort_index()\n",
    "test_Series.plot(kind = 'bar', ax = ax11)\n",
    "t_Series.plot(kind='bar', ax = ax22)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)      361\n",
       "(4, 5)      255\n",
       "(2, 1)      163\n",
       "(1, 5)       96\n",
       "(13, 14)     92\n",
       "(0, 3)       70\n",
       "(2, 5)       68\n",
       "(7, 8)       64\n",
       "(1, 4)       63\n",
       "(8, 7)       49\n",
       "(3, 4)       45\n",
       "(10, 11)     40\n",
       "(0, 1)       35\n",
       "(14, 13)     34\n",
       "(5, 4)       32\n",
       "(16, 17)     28\n",
       "(13, 16)     15\n",
       "(7, 10)      14\n",
       "(1, 0)       14\n",
       "(4, 1)       13\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standard = pd.read_csv('~/cv-12/log/standard_1028.csv')\n",
    "submission = pd.read_csv('~/input/data/eval/submission.csv')\n",
    "\n",
    "False_idx = (standard['ans'] == submission['ans']) == False\n",
    "pd.Series(list(zip(submission[False_idx]['ans'], standard[False_idx]['ans']))).value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18776 18532 18884\n"
     ]
    }
   ],
   "source": [
    "# (자체) 각 class별로 정답개수 확인\n",
    "g_sum = 0\n",
    "a_sum = 0\n",
    "m_sum = 0\n",
    "device = torch.device('cuda')\n",
    "model = basemodel_resnet50.to(device)\n",
    "model.eval()\n",
    "\n",
    "for i in range(len(mask_train_set)):\n",
    "    image, label = mask_train_set[i]\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "    \n",
    "    result = model(image)\n",
    "    if torch.argmax(result[0]) == label[0]:\n",
    "        g_sum += 1\n",
    "    if torch.argmax(result[1]) == label[1]:\n",
    "        a_sum += 1\n",
    "    if torch.argmax(result[2]) == label[2]:\n",
    "        m_sum += 1\n",
    "        \n",
    "        \n",
    "for i in range(len(mask_val_set)):\n",
    "    image, label = mask_val_set[i]\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "    \n",
    "    result = model(image)\n",
    "    if torch.argmax(result[0]) == label[0]:\n",
    "        g_sum += 1\n",
    "    if torch.argmax(result[1]) == label[1]:\n",
    "        a_sum += 1\n",
    "    if torch.argmax(result[2]) == label[2]:\n",
    "        m_sum += 1\n",
    "        \n",
    "\n",
    "print(g_sum, a_sum , m_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
