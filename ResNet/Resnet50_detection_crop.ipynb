{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf046c11-3a01-4c6c-99dd-c062e21c7bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bad9118-3244-4b50-bce3-d9f0ca8dc7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% |  0% |\n"
     ]
    }
   ],
   "source": [
    "import GPUtil\n",
    "GPUtil.showUtilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2f667bb-73d0-494b-843e-f0260f6ac538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov  2 17:48:23 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| N/A   38C    P0    38W / 250W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b1d65a3-78de-4b01-a862-e6bd4c7ef135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from align_faces import warp_and_crop_face, get_reference_facial_points\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "from torchmetrics import F1Score\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa8d19de-5a8e-454f-a22b-129e94792395",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 12\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f69f93c-5675-433f-8a88-7bf8e411d51c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>path</th>\n",
       "      <th>age_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>52</td>\n",
       "      <td>000002_female_Asian_52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>54</td>\n",
       "      <td>000004_male_Asian_54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>58</td>\n",
       "      <td>000005_female_Asian_58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>59</td>\n",
       "      <td>000006_female_Asian_59</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>6954</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006954_male_Asian_19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2696</th>\n",
       "      <td>6955</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006955_male_Asian_19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>6956</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006956_male_Asian_19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2698</th>\n",
       "      <td>6957</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>20</td>\n",
       "      <td>006957_male_Asian_20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2699</th>\n",
       "      <td>6959</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006959_male_Asian_19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2700 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  gender   race  age                    path  age_range\n",
       "0        1  female  Asian   45  000001_female_Asian_45          1\n",
       "1        2  female  Asian   52  000002_female_Asian_52          1\n",
       "2        4    male  Asian   54    000004_male_Asian_54          1\n",
       "3        5  female  Asian   58  000005_female_Asian_58          1\n",
       "4        6  female  Asian   59  000006_female_Asian_59          1\n",
       "...    ...     ...    ...  ...                     ...        ...\n",
       "2695  6954    male  Asian   19    006954_male_Asian_19          0\n",
       "2696  6955    male  Asian   19    006955_male_Asian_19          0\n",
       "2697  6956    male  Asian   19    006956_male_Asian_19          0\n",
       "2698  6957    male  Asian   20    006957_male_Asian_20          0\n",
       "2699  6959    male  Asian   19    006959_male_Asian_19          0\n",
       "\n",
       "[2700 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir_path = '../input/data/train/'\n",
    "train_image_path = '../input/data/train/images/'\n",
    "\n",
    "dt_train = pd.read_csv(train_dir_path+'train.csv')\n",
    "exp_train = pd.read_csv(train_dir_path+'expanded_train.csv')\n",
    "\n",
    "def get_age_range(age):\n",
    "    if age < 30:\n",
    "        return 0\n",
    "    elif 30 <= age < 60:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "dt_train['age_range'] = dt_train['age'].apply(lambda x : get_age_range(x))\n",
    "dt_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c92579a9-3de4-463c-9041-53d5b1e403b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>path</th>\n",
       "      <th>age_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>690</th>\n",
       "      <td>1396</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45</td>\n",
       "      <td>001396_male_Asian_45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>829</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>55</td>\n",
       "      <td>000829_female_Asian_55</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>675</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>59</td>\n",
       "      <td>000675_female_Asian_59</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1394</th>\n",
       "      <td>3431</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>50</td>\n",
       "      <td>003431_female_Asian_50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1956</th>\n",
       "      <td>4487</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>18</td>\n",
       "      <td>004487_male_Asian_18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156</th>\n",
       "      <td>3108</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>003108_female_Asian_19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1682</th>\n",
       "      <td>3863</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>56</td>\n",
       "      <td>003863_female_Asian_56</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>66</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>53</td>\n",
       "      <td>000066_female_Asian_53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2194</th>\n",
       "      <td>5487</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>48</td>\n",
       "      <td>005487_female_Asian_48</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2200</th>\n",
       "      <td>5494</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>47</td>\n",
       "      <td>005494_female_Asian_47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  gender   race  age                    path  age_range\n",
       "690   1396    male  Asian   45    001396_male_Asian_45          1\n",
       "374    829  female  Asian   55  000829_female_Asian_55          1\n",
       "268    675  female  Asian   59  000675_female_Asian_59          1\n",
       "1394  3431  female  Asian   50  003431_female_Asian_50          1\n",
       "1956  4487    male  Asian   18    004487_male_Asian_18          0\n",
       "...    ...     ...    ...  ...                     ...        ...\n",
       "1156  3108  female  Asian   19  003108_female_Asian_19          0\n",
       "1682  3863  female  Asian   56  003863_female_Asian_56          1\n",
       "42      66  female  Asian   53  000066_female_Asian_53          1\n",
       "2194  5487  female  Asian   48  005487_female_Asian_48          1\n",
       "2200  5494  female  Asian   47  005494_female_Asian_47          1\n",
       "\n",
       "[70 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idx, valid_idx = train_test_split(np.arange(len(dt_train)),\n",
    "                                       test_size=0.2,\n",
    "                                       shuffle=True,\n",
    "                                       stratify=dt_train['age_range'])\n",
    "dt_train.iloc[train_idx].head(70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c3462fa-203a-40c4-989f-b702d707847f",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_train.drop(columns=['Unnamed: 0', 'Has_Face', 'Score', 'LE_X', 'LE_Y', 'RE_X', 'RE_Y', 'N_X', 'N_Y', 'LM_X', 'LM_Y', 'RM_X', 'RM_Y'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c2a0240-b785-4ffa-a64d-e71f2e7278ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_new_path(x):\n",
    "    return x.split(\"images\")[0] + 'new_images' + x.split(\"images\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44294a51-bf0c-4c5a-b6a2-e13e8d43aebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_train['new_path'] = exp_train['Path'].apply(make_new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52f0cd47-04e5-426f-bb72-531ce8be06da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PersonID</th>\n",
       "      <th>Path</th>\n",
       "      <th>Filename</th>\n",
       "      <th>Class</th>\n",
       "      <th>Mask</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Age_Class</th>\n",
       "      <th>BBoxX1</th>\n",
       "      <th>BBoxY1</th>\n",
       "      <th>BBoxX2</th>\n",
       "      <th>BBoxY2</th>\n",
       "      <th>new_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>../input/data/train/images/000001_female_Asian...</td>\n",
       "      <td>../input/data/train/images/000001_female_Asian...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>110</td>\n",
       "      <td>146</td>\n",
       "      <td>250</td>\n",
       "      <td>347</td>\n",
       "      <td>../input/data/train/new_images/000001_female_A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>../input/data/train/images/000001_female_Asian...</td>\n",
       "      <td>../input/data/train/images/000001_female_Asian...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>112</td>\n",
       "      <td>160</td>\n",
       "      <td>245</td>\n",
       "      <td>334</td>\n",
       "      <td>../input/data/train/new_images/000001_female_A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>../input/data/train/images/000001_female_Asian...</td>\n",
       "      <td>../input/data/train/images/000001_female_Asian...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>107</td>\n",
       "      <td>127</td>\n",
       "      <td>230</td>\n",
       "      <td>311</td>\n",
       "      <td>../input/data/train/new_images/000001_female_A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>../input/data/train/images/000001_female_Asian...</td>\n",
       "      <td>../input/data/train/images/000001_female_Asian...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>143</td>\n",
       "      <td>244</td>\n",
       "      <td>326</td>\n",
       "      <td>../input/data/train/new_images/000001_female_A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>../input/data/train/images/000001_female_Asian...</td>\n",
       "      <td>../input/data/train/images/000001_female_Asian...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>122</td>\n",
       "      <td>125</td>\n",
       "      <td>244</td>\n",
       "      <td>316</td>\n",
       "      <td>../input/data/train/new_images/000001_female_A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18895</th>\n",
       "      <td>6959</td>\n",
       "      <td>../input/data/train/images/006959_male_Asian_1...</td>\n",
       "      <td>../input/data/train/images/006959_male_Asian_1...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>116</td>\n",
       "      <td>181</td>\n",
       "      <td>246</td>\n",
       "      <td>340</td>\n",
       "      <td>../input/data/train/new_images/006959_male_Asi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18896</th>\n",
       "      <td>6959</td>\n",
       "      <td>../input/data/train/images/006959_male_Asian_1...</td>\n",
       "      <td>../input/data/train/images/006959_male_Asian_1...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>119</td>\n",
       "      <td>182</td>\n",
       "      <td>260</td>\n",
       "      <td>354</td>\n",
       "      <td>../input/data/train/new_images/006959_male_Asi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18897</th>\n",
       "      <td>6959</td>\n",
       "      <td>../input/data/train/images/006959_male_Asian_1...</td>\n",
       "      <td>../input/data/train/images/006959_male_Asian_1...</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>180</td>\n",
       "      <td>244</td>\n",
       "      <td>349</td>\n",
       "      <td>../input/data/train/new_images/006959_male_Asi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18898</th>\n",
       "      <td>6959</td>\n",
       "      <td>../input/data/train/images/006959_male_Asian_1...</td>\n",
       "      <td>../input/data/train/images/006959_male_Asian_1...</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "      <td>178</td>\n",
       "      <td>255</td>\n",
       "      <td>364</td>\n",
       "      <td>../input/data/train/new_images/006959_male_Asi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18899</th>\n",
       "      <td>6959</td>\n",
       "      <td>../input/data/train/images/006959_male_Asian_1...</td>\n",
       "      <td>../input/data/train/images/006959_male_Asian_1...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "      <td>173</td>\n",
       "      <td>268</td>\n",
       "      <td>360</td>\n",
       "      <td>../input/data/train/new_images/006959_male_Asi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18900 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      PersonID                                               Path  \\\n",
       "0            1  ../input/data/train/images/000001_female_Asian...   \n",
       "1            1  ../input/data/train/images/000001_female_Asian...   \n",
       "2            1  ../input/data/train/images/000001_female_Asian...   \n",
       "3            1  ../input/data/train/images/000001_female_Asian...   \n",
       "4            1  ../input/data/train/images/000001_female_Asian...   \n",
       "...        ...                                                ...   \n",
       "18895     6959  ../input/data/train/images/006959_male_Asian_1...   \n",
       "18896     6959  ../input/data/train/images/006959_male_Asian_1...   \n",
       "18897     6959  ../input/data/train/images/006959_male_Asian_1...   \n",
       "18898     6959  ../input/data/train/images/006959_male_Asian_1...   \n",
       "18899     6959  ../input/data/train/images/006959_male_Asian_1...   \n",
       "\n",
       "                                                Filename  Class  Mask  Gender  \\\n",
       "0      ../input/data/train/images/000001_female_Asian...      4     0       1   \n",
       "1      ../input/data/train/images/000001_female_Asian...      4     0       1   \n",
       "2      ../input/data/train/images/000001_female_Asian...      4     0       1   \n",
       "3      ../input/data/train/images/000001_female_Asian...      4     0       1   \n",
       "4      ../input/data/train/images/000001_female_Asian...     10     1       1   \n",
       "...                                                  ...    ...   ...     ...   \n",
       "18895  ../input/data/train/images/006959_male_Asian_1...      0     0       0   \n",
       "18896  ../input/data/train/images/006959_male_Asian_1...      0     0       0   \n",
       "18897  ../input/data/train/images/006959_male_Asian_1...      6     1       0   \n",
       "18898  ../input/data/train/images/006959_male_Asian_1...     12     2       0   \n",
       "18899  ../input/data/train/images/006959_male_Asian_1...      0     0       0   \n",
       "\n",
       "       Age  Age_Class  BBoxX1  BBoxY1  BBoxX2  BBoxY2  \\\n",
       "0       45          1     110     146     250     347   \n",
       "1       45          1     112     160     245     334   \n",
       "2       45          1     107     127     230     311   \n",
       "3       45          1     120     143     244     326   \n",
       "4       45          1     122     125     244     316   \n",
       "...    ...        ...     ...     ...     ...     ...   \n",
       "18895   19          0     116     181     246     340   \n",
       "18896   19          0     119     182     260     354   \n",
       "18897   19          0     111     180     244     349   \n",
       "18898   19          0     112     178     255     364   \n",
       "18899   19          0     112     173     268     360   \n",
       "\n",
       "                                                new_path  \n",
       "0      ../input/data/train/new_images/000001_female_A...  \n",
       "1      ../input/data/train/new_images/000001_female_A...  \n",
       "2      ../input/data/train/new_images/000001_female_A...  \n",
       "3      ../input/data/train/new_images/000001_female_A...  \n",
       "4      ../input/data/train/new_images/000001_female_A...  \n",
       "...                                                  ...  \n",
       "18895  ../input/data/train/new_images/006959_male_Asi...  \n",
       "18896  ../input/data/train/new_images/006959_male_Asi...  \n",
       "18897  ../input/data/train/new_images/006959_male_Asi...  \n",
       "18898  ../input/data/train/new_images/006959_male_Asi...  \n",
       "18899  ../input/data/train/new_images/006959_male_Asi...  \n",
       "\n",
       "[18900 rows x 13 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7cdaaac-b38a-471a-90b9-012521adb5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index size: 2160 == file estimate: 15120 == split size: 15120\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PersonID</th>\n",
       "      <th>Path</th>\n",
       "      <th>Filename</th>\n",
       "      <th>Class</th>\n",
       "      <th>Mask</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Age_Class</th>\n",
       "      <th>BBoxX1</th>\n",
       "      <th>BBoxY1</th>\n",
       "      <th>BBoxX2</th>\n",
       "      <th>BBoxY2</th>\n",
       "      <th>new_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>../input/data/train/images/000001_female_Asian...</td>\n",
       "      <td>../input/data/train/images/000001_female_Asian...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>110</td>\n",
       "      <td>146</td>\n",
       "      <td>250</td>\n",
       "      <td>347</td>\n",
       "      <td>../input/data/train/new_images/000001_female_A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>../input/data/train/images/000001_female_Asian...</td>\n",
       "      <td>../input/data/train/images/000001_female_Asian...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>112</td>\n",
       "      <td>160</td>\n",
       "      <td>245</td>\n",
       "      <td>334</td>\n",
       "      <td>../input/data/train/new_images/000001_female_A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>../input/data/train/images/000001_female_Asian...</td>\n",
       "      <td>../input/data/train/images/000001_female_Asian...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>107</td>\n",
       "      <td>127</td>\n",
       "      <td>230</td>\n",
       "      <td>311</td>\n",
       "      <td>../input/data/train/new_images/000001_female_A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>../input/data/train/images/000001_female_Asian...</td>\n",
       "      <td>../input/data/train/images/000001_female_Asian...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>143</td>\n",
       "      <td>244</td>\n",
       "      <td>326</td>\n",
       "      <td>../input/data/train/new_images/000001_female_A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>../input/data/train/images/000001_female_Asian...</td>\n",
       "      <td>../input/data/train/images/000001_female_Asian...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>122</td>\n",
       "      <td>125</td>\n",
       "      <td>244</td>\n",
       "      <td>316</td>\n",
       "      <td>../input/data/train/new_images/000001_female_A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18895</th>\n",
       "      <td>6959</td>\n",
       "      <td>../input/data/train/images/006959_male_Asian_1...</td>\n",
       "      <td>../input/data/train/images/006959_male_Asian_1...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>116</td>\n",
       "      <td>181</td>\n",
       "      <td>246</td>\n",
       "      <td>340</td>\n",
       "      <td>../input/data/train/new_images/006959_male_Asi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18896</th>\n",
       "      <td>6959</td>\n",
       "      <td>../input/data/train/images/006959_male_Asian_1...</td>\n",
       "      <td>../input/data/train/images/006959_male_Asian_1...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>119</td>\n",
       "      <td>182</td>\n",
       "      <td>260</td>\n",
       "      <td>354</td>\n",
       "      <td>../input/data/train/new_images/006959_male_Asi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18897</th>\n",
       "      <td>6959</td>\n",
       "      <td>../input/data/train/images/006959_male_Asian_1...</td>\n",
       "      <td>../input/data/train/images/006959_male_Asian_1...</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>180</td>\n",
       "      <td>244</td>\n",
       "      <td>349</td>\n",
       "      <td>../input/data/train/new_images/006959_male_Asi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18898</th>\n",
       "      <td>6959</td>\n",
       "      <td>../input/data/train/images/006959_male_Asian_1...</td>\n",
       "      <td>../input/data/train/images/006959_male_Asian_1...</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "      <td>178</td>\n",
       "      <td>255</td>\n",
       "      <td>364</td>\n",
       "      <td>../input/data/train/new_images/006959_male_Asi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18899</th>\n",
       "      <td>6959</td>\n",
       "      <td>../input/data/train/images/006959_male_Asian_1...</td>\n",
       "      <td>../input/data/train/images/006959_male_Asian_1...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "      <td>173</td>\n",
       "      <td>268</td>\n",
       "      <td>360</td>\n",
       "      <td>../input/data/train/new_images/006959_male_Asi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15120 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      PersonID                                               Path  \\\n",
       "0            1  ../input/data/train/images/000001_female_Asian...   \n",
       "1            1  ../input/data/train/images/000001_female_Asian...   \n",
       "2            1  ../input/data/train/images/000001_female_Asian...   \n",
       "3            1  ../input/data/train/images/000001_female_Asian...   \n",
       "4            1  ../input/data/train/images/000001_female_Asian...   \n",
       "...        ...                                                ...   \n",
       "18895     6959  ../input/data/train/images/006959_male_Asian_1...   \n",
       "18896     6959  ../input/data/train/images/006959_male_Asian_1...   \n",
       "18897     6959  ../input/data/train/images/006959_male_Asian_1...   \n",
       "18898     6959  ../input/data/train/images/006959_male_Asian_1...   \n",
       "18899     6959  ../input/data/train/images/006959_male_Asian_1...   \n",
       "\n",
       "                                                Filename  Class  Mask  Gender  \\\n",
       "0      ../input/data/train/images/000001_female_Asian...      4     0       1   \n",
       "1      ../input/data/train/images/000001_female_Asian...      4     0       1   \n",
       "2      ../input/data/train/images/000001_female_Asian...      4     0       1   \n",
       "3      ../input/data/train/images/000001_female_Asian...      4     0       1   \n",
       "4      ../input/data/train/images/000001_female_Asian...     10     1       1   \n",
       "...                                                  ...    ...   ...     ...   \n",
       "18895  ../input/data/train/images/006959_male_Asian_1...      0     0       0   \n",
       "18896  ../input/data/train/images/006959_male_Asian_1...      0     0       0   \n",
       "18897  ../input/data/train/images/006959_male_Asian_1...      6     1       0   \n",
       "18898  ../input/data/train/images/006959_male_Asian_1...     12     2       0   \n",
       "18899  ../input/data/train/images/006959_male_Asian_1...      0     0       0   \n",
       "\n",
       "       Age  Age_Class  BBoxX1  BBoxY1  BBoxX2  BBoxY2  \\\n",
       "0       45          1     110     146     250     347   \n",
       "1       45          1     112     160     245     334   \n",
       "2       45          1     107     127     230     311   \n",
       "3       45          1     120     143     244     326   \n",
       "4       45          1     122     125     244     316   \n",
       "...    ...        ...     ...     ...     ...     ...   \n",
       "18895   19          0     116     181     246     340   \n",
       "18896   19          0     119     182     260     354   \n",
       "18897   19          0     111     180     244     349   \n",
       "18898   19          0     112     178     255     364   \n",
       "18899   19          0     112     173     268     360   \n",
       "\n",
       "                                                new_path  \n",
       "0      ../input/data/train/new_images/000001_female_A...  \n",
       "1      ../input/data/train/new_images/000001_female_A...  \n",
       "2      ../input/data/train/new_images/000001_female_A...  \n",
       "3      ../input/data/train/new_images/000001_female_A...  \n",
       "4      ../input/data/train/new_images/000001_female_A...  \n",
       "...                                                  ...  \n",
       "18895  ../input/data/train/new_images/006959_male_Asi...  \n",
       "18896  ../input/data/train/new_images/006959_male_Asi...  \n",
       "18897  ../input/data/train/new_images/006959_male_Asi...  \n",
       "18898  ../input/data/train/new_images/006959_male_Asi...  \n",
       "18899  ../input/data/train/new_images/006959_male_Asi...  \n",
       "\n",
       "[15120 rows x 13 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_exp_train = exp_train[exp_train['PersonID'].isin(list(dt_train.iloc[train_idx][\"id\"]))]\n",
    "split_exp_valid = exp_train[exp_train['PersonID'].isin(list(dt_train.iloc[valid_idx][\"id\"]))]\n",
    "print(f\"index size: {len(train_idx)} == file estimate: {len(train_idx) * 7} == split size: {len(split_exp_train)}\")\n",
    "split_exp_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bfde03-9fef-4aa6-b15b-49a038e76d98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for idx in range(len(exp_train)):\n",
    "#     filename = exp_train.iloc[idx]['Path']\n",
    "#     new_filename = exp_train.iloc[idx]['new_path']\n",
    "#     parent_dir = '/'.join(new_filename.split(\"/\")[:-1])\n",
    "    \n",
    "#     if not os.path.exists(parent_dir):\n",
    "#         os.makedirs(parent_dir)\n",
    "    \n",
    "#     X = cv2.cvtColor(cv2.imread(filename), cv2.COLOR_BGR2RGB)\n",
    "#     x1 = exp_train.iloc[idx]['BBoxX1']\n",
    "#     x2 = exp_train.iloc[idx]['BBoxX2']\n",
    "#     y1 = exp_train.iloc[idx]['BBoxY1']\n",
    "#     y2 = exp_train.iloc[idx]['BBoxY2']\n",
    "#     w = x2 - x1\n",
    "#     h = y2 - y1\n",
    "#     w = w // 3\n",
    "#     h = h // 3\n",
    "\n",
    "#     nx1 = max(0, x1-w)\n",
    "#     nx2 = min(384, x2+w)\n",
    "#     ny1 = max(0, y1-h)\n",
    "#     ny2 = min(512, y2+h)\n",
    "#     X = X[ny1:ny2, nx1:nx2]\n",
    "    \n",
    "    \n",
    "#     X = cv2.resize(X, (350, 350))\n",
    "#     if idx % 3000 == 0:\n",
    "#         plt.imshow(X)\n",
    "#         plt.show()\n",
    "        \n",
    "#     X = cv2.cvtColor(X, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "#     cv2.imwrite(new_filename, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "304c6a61-ff17-4777-8132-18def61c8102",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image = split_exp_train.loc[:,\"new_path\"]\n",
    "train_label = split_exp_train.loc[:,\"Class\"]\n",
    "\n",
    "valid_image = split_exp_valid.loc[:,\"new_path\"]\n",
    "valid_label = split_exp_valid.loc[:,\"Class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84ec33c6-5a23-41ec-8980-2c7f0ecd1631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15120\n",
      "3780\n"
     ]
    }
   ],
   "source": [
    "print(len(train_image))\n",
    "print(len(valid_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "694474c7-7175-40e6-b071-e50218629f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.Series(train_image)\n",
    "train_label = pd.Series(train_label)\n",
    "\n",
    "valid_data = pd.Series(valid_image)\n",
    "valid_label = pd.Series(valid_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2cd7b29-5f03-444e-b781-902ca74f2085",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Resize, ToTensor, Normalize, Compose, CenterCrop, ColorJitter\n",
    "from PIL import Image\n",
    "\n",
    "class Dataset_Mask(Dataset):\n",
    "    def __init__(self, data, label, encoding=True, midcrop=True, transform=None, is_train=True):\n",
    "        self.encoding = encoding\n",
    "        self.midcrop = midcrop\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.is_train = is_train\n",
    "        self.label = label.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X = cv2.cvtColor(cv2.imread(self.data[idx]), cv2.COLOR_BGR2RGB)\n",
    "        y = self.label[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            return self.transform(X), y\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2a22aff-e634-4f15-877c-752d450d01da",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_train_set = Dataset_Mask(data=train_data, label=train_label, is_train=True, transform=transforms.Compose([\n",
    "                                    ToTensor(),\n",
    "                            ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "caf9fd0a-27a3-4ba1-ab39-fea87ac96227",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_val_set = Dataset_Mask(data=valid_data, label=valid_label, is_train=False, transform = transforms.Compose([\n",
    "                                    ToTensor(),\n",
    "                            ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37b1ced0-1e34-4d50-b2b2-dcdff8fd33f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_dataloader_mask = DataLoader(dataset = mask_train_set, batch_size=batch_size, num_workers=2, drop_last=True)\n",
    "val_dataloader_mask = DataLoader(dataset = mask_val_set, batch_size=batch_size, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af71a4ef-a137-47b0-af94-eebedaa36014",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.resnet50(pretrained=True)\n",
    "# print('필요 입력 채널 개수', model.conv1.weight.shape[1])\n",
    "# print('네트워크 출력 채널 개수', model.fc.weight.shape[0])\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11357fc4-548b-452e-bbd8-e60ef6261308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "필요 입력 채널 개수 3\n",
      "네트워크 출력 채널 개수 18\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "class_num = 18\n",
    "model.fc = nn.Linear(in_features=2048, out_features=class_num, bias=True)\n",
    "nn.init.xavier_uniform_(model.fc.weight)\n",
    "stdv = 1. / math.sqrt(model.fc.weight.size(1))\n",
    "model.fc.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "print('필요 입력 채널 개수', model.conv1.weight.shape[1])\n",
    "print('네트워크 출력 채널 개수', model.fc.weight.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a1a2262-f820-4294-8086-3a48a5e5d686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"using {device}\")\n",
    "\n",
    "model = torch.nn.DataParallel(model)\n",
    "model.to(device)\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_EPOCH = 50\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "#lr = 1e-3\n",
    "#betas = (0.9, 0.999)\n",
    "#weight_decay = 0.5e-4\n",
    "#eps = 1e-8\n",
    "#optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay, eps=eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64a36f92-aad5-4bbb-a920-29d50f0f058b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "n_param = 0\n",
    "for p_idx, (param_name, param) in enumerate(model.named_parameters()):\n",
    "    if param.requires_grad:\n",
    "        \n",
    "        #if param_name.startswith('module.fc') or param_name.startswith('module.layer4') or param_name.startswith('module.layer3'):\n",
    "        #    param.requires_grad = True  # Train\n",
    "        #else:\n",
    "        #    param.requires_grad = False # Freeze\n",
    "            \n",
    "        param_numpy = param.detach().cpu().numpy()\n",
    "        n_param += len(param_numpy.reshape(-1))\n",
    "#         print (\"[%d] name:[%s] shape:[%s].\"%(p_idx,param_name,param_numpy.shape))\n",
    "#         print (\"    val:%s\"%(param_numpy.reshape(-1)[:5]))\n",
    "#         print(f\"    --[{param.requires_grad}].\")\n",
    "# print (\"Total number of parameters:[%s].\"%(format(n_param,',d')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c067d0e-38c9-4ab8-baba-88cf8dcb8c6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 236/236 [04:14<00:00,  1.08s/it]\n",
      "  0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[0/50] training loss 1.352, training accuracy 0.576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:12<00:00,  4.90it/s]\n",
      "  0%|          | 0/236 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update checkpoint!!!\n",
      "[val] acc : 0.282, loss : 2.103, f1 score: 0.182 -- size(60)\n",
      "best acc : 0.282, best loss : 2.103, best f1 : 0.182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 236/236 [04:09<00:00,  1.06s/it]\n",
      "  0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[1/50] training loss 1.059, training accuracy 0.654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:12<00:00,  4.87it/s]\n",
      "  0%|          | 0/236 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update checkpoint!!!\n",
      "[val] acc : 0.469, loss : 1.531, f1 score: 0.360 -- size(60)\n",
      "best acc : 0.469, best loss : 1.531, best f1 : 0.360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 236/236 [04:10<00:00,  1.06s/it]\n",
      "  0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[2/50] training loss 0.745, training accuracy 0.753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:12<00:00,  4.82it/s]\n",
      "  0%|          | 0/236 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update checkpoint!!!\n",
      "[val] acc : 0.583, loss : 1.071, f1 score: 0.422 -- size(60)\n",
      "best acc : 0.583, best loss : 1.071, best f1 : 0.422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 236/236 [04:18<00:00,  1.10s/it]\n",
      "  0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[3/50] training loss 0.485, training accuracy 0.832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:14<00:00,  4.17it/s]\n",
      "  0%|          | 0/236 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update checkpoint!!!\n",
      "[val] acc : 0.663, loss : 0.953, f1 score: 0.547 -- size(60)\n",
      "best acc : 0.663, best loss : 0.953, best f1 : 0.547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 236/236 [04:12<00:00,  1.07s/it]\n",
      "  0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[4/50] training loss 0.271, training accuracy 0.912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:12<00:00,  4.88it/s]\n",
      "  0%|          | 0/236 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] acc : 0.694, loss : 0.925, f1 score: 0.514 -- size(60)\n",
      "best acc : 0.694, best loss : 0.925, best f1 : 0.547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 236/236 [04:10<00:00,  1.06s/it]\n",
      "  0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[5/50] training loss 0.142, training accuracy 0.961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:12<00:00,  4.87it/s]\n",
      "  0%|          | 0/236 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] acc : 0.695, loss : 1.041, f1 score: 0.530 -- size(60)\n",
      "best acc : 0.695, best loss : 0.925, best f1 : 0.547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 236/236 [04:09<00:00,  1.06s/it]\n",
      "  0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[6/50] training loss 0.087, training accuracy 0.977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:13<00:00,  4.50it/s]\n",
      "  0%|          | 0/236 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update checkpoint!!!\n",
      "[val] acc : 0.792, loss : 0.770, f1 score: 0.628 -- size(60)\n",
      "best acc : 0.792, best loss : 0.770, best f1 : 0.628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 236/236 [04:12<00:00,  1.07s/it]\n",
      "  0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[7/50] training loss 0.053, training accuracy 0.985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:12<00:00,  4.76it/s]\n",
      "  0%|          | 0/236 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update checkpoint!!!\n",
      "[val] acc : 0.834, loss : 0.680, f1 score: 0.705 -- size(60)\n",
      "best acc : 0.834, best loss : 0.680, best f1 : 0.705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 236/236 [04:11<00:00,  1.07s/it]\n",
      "  0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[8/50] training loss 0.044, training accuracy 0.987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:12<00:00,  4.83it/s]\n",
      "  0%|          | 0/236 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update checkpoint!!!\n",
      "[val] acc : 0.849, loss : 0.602, f1 score: 0.708 -- size(60)\n",
      "best acc : 0.849, best loss : 0.602, best f1 : 0.708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 236/236 [04:10<00:00,  1.06s/it]\n",
      "  0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[9/50] training loss 0.039, training accuracy 0.990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:12<00:00,  4.70it/s]\n",
      "  0%|          | 0/236 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] acc : 0.797, loss : 0.847, f1 score: 0.650 -- size(60)\n",
      "best acc : 0.849, best loss : 0.602, best f1 : 0.708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 236/236 [04:13<00:00,  1.07s/it]\n",
      "  0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[10/50] training loss 0.019, training accuracy 0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:12<00:00,  4.69it/s]\n",
      "  0%|          | 0/236 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update checkpoint!!!\n",
      "[val] acc : 0.847, loss : 0.650, f1 score: 0.717 -- size(60)\n",
      "best acc : 0.849, best loss : 0.602, best f1 : 0.717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 236/236 [04:18<00:00,  1.09s/it]\n",
      "  0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[11/50] training loss 0.014, training accuracy 0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:12<00:00,  4.72it/s]\n",
      "  0%|          | 0/236 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update checkpoint!!!\n",
      "[val] acc : 0.837, loss : 0.690, f1 score: 0.731 -- size(60)\n",
      "best acc : 0.849, best loss : 0.602, best f1 : 0.731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 236/236 [04:08<00:00,  1.05s/it]\n",
      "  0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[12/50] training loss 0.007, training accuracy 0.998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:12<00:00,  4.70it/s]\n",
      "  0%|          | 0/236 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update checkpoint!!!\n",
      "[val] acc : 0.857, loss : 0.648, f1 score: 0.766 -- size(60)\n",
      "best acc : 0.857, best loss : 0.602, best f1 : 0.766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 236/236 [04:09<00:00,  1.06s/it]\n",
      "  0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[13/50] training loss 0.003, training accuracy 0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:12<00:00,  4.83it/s]\n",
      "  0%|          | 0/236 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] acc : 0.815, loss : 0.843, f1 score: 0.702 -- size(60)\n",
      "best acc : 0.857, best loss : 0.602, best f1 : 0.766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 236/236 [04:11<00:00,  1.06s/it]\n",
      "  0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[14/50] training loss 0.002, training accuracy 0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:12<00:00,  4.86it/s]\n",
      "  0%|          | 0/236 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] acc : 0.847, loss : 0.713, f1 score: 0.737 -- size(60)\n",
      "best acc : 0.857, best loss : 0.602, best f1 : 0.766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 236/236 [04:10<00:00,  1.06s/it]\n",
      "  0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[16/50] training loss 0.001, training accuracy 0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:12<00:00,  4.89it/s]\n",
      "  0%|          | 0/236 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update checkpoint!!!\n",
      "[val] acc : 0.883, loss : 0.535, f1 score: 0.796 -- size(60)\n",
      "best acc : 0.883, best loss : 0.535, best f1 : 0.796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 236/236 [04:17<00:00,  1.09s/it]\n",
      "  0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[17/50] training loss 0.000, training accuracy 0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:12<00:00,  4.68it/s]\n",
      "  0%|          | 0/236 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update checkpoint!!!\n",
      "[val] acc : 0.886, loss : 0.526, f1 score: 0.801 -- size(60)\n",
      "best acc : 0.886, best loss : 0.526, best f1 : 0.801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 236/236 [04:23<00:00,  1.12s/it]\n",
      "  0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[18/50] training loss 0.000, training accuracy 0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:12<00:00,  4.77it/s]\n",
      "  0%|          | 0/236 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update checkpoint!!!\n",
      "[val] acc : 0.888, loss : 0.523, f1 score: 0.801 -- size(60)\n",
      "best acc : 0.888, best loss : 0.523, best f1 : 0.801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 236/236 [04:13<00:00,  1.07s/it]\n",
      "  0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[19/50] training loss 0.000, training accuracy 0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:12<00:00,  4.88it/s]\n",
      "  0%|          | 0/236 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] acc : 0.887, loss : 0.521, f1 score: 0.798 -- size(60)\n",
      "best acc : 0.888, best loss : 0.521, best f1 : 0.801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 236/236 [04:15<00:00,  1.08s/it]\n",
      "  0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[20/50] training loss 0.000, training accuracy 0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:13<00:00,  4.58it/s]\n",
      "  0%|          | 0/236 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] acc : 0.889, loss : 0.520, f1 score: 0.799 -- size(60)\n",
      "best acc : 0.889, best loss : 0.520, best f1 : 0.801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 236/236 [04:19<00:00,  1.10s/it]\n",
      "  0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[21/50] training loss 0.000, training accuracy 0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:12<00:00,  4.75it/s]\n",
      "  0%|          | 0/236 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update checkpoint!!!\n",
      "[val] acc : 0.890, loss : 0.520, f1 score: 0.806 -- size(60)\n",
      "best acc : 0.890, best loss : 0.520, best f1 : 0.806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 236/236 [04:14<00:00,  1.08s/it]\n",
      "  0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[22/50] training loss 0.000, training accuracy 0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:12<00:00,  4.64it/s]\n",
      "  0%|          | 0/236 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update checkpoint!!!\n",
      "[val] acc : 0.889, loss : 0.519, f1 score: 0.807 -- size(60)\n",
      "best acc : 0.890, best loss : 0.519, best f1 : 0.807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 236/236 [04:15<00:00,  1.08s/it]\n",
      "  0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[23/50] training loss 0.000, training accuracy 0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:12<00:00,  4.73it/s]\n",
      "  0%|          | 0/236 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] acc : 0.889, loss : 0.519, f1 score: 0.806 -- size(60)\n",
      "best acc : 0.890, best loss : 0.519, best f1 : 0.807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 236/236 [04:15<00:00,  1.08s/it]\n",
      "  0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[24/50] training loss 0.000, training accuracy 0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:12<00:00,  4.69it/s]\n",
      "  0%|          | 0/236 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update checkpoint!!!\n",
      "[val] acc : 0.889, loss : 0.520, f1 score: 0.807 -- size(60)\n",
      "best acc : 0.890, best loss : 0.519, best f1 : 0.807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 236/236 [04:18<00:00,  1.10s/it]\n",
      "  0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[25/50] training loss 0.000, training accuracy 0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:12<00:00,  4.71it/s]\n",
      "  0%|          | 0/236 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] acc : 0.890, loss : 0.520, f1 score: 0.805 -- size(60)\n",
      "best acc : 0.890, best loss : 0.519, best f1 : 0.807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 33/236 [00:34<03:32,  1.05s/it]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 236/236 [04:12<00:00,  1.07s/it]\n",
      "  0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[49/50] training loss 0.000, training accuracy 0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:12<00:00,  4.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] acc : 0.898, loss : 0.575, f1 score: 0.816 -- size(60)\n",
      "best acc : 0.898, best loss : 0.519, best f1 : 0.817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_val_acc = 0\n",
    "best_val_loss = np.inf\n",
    "patience = 10\n",
    "cur_count = 0\n",
    "\n",
    "f1 = F1Score(num_classes=class_num, average='macro').to(device)\n",
    "best_f1_score = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    model.train()\n",
    "    loss_value = 0\n",
    "    matches = 0\n",
    "    for train_batch in tqdm(train_dataloader_mask):\n",
    "        inputs, labels = train_batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outs = model(inputs)\n",
    "        preds = torch.argmax(outs, dim=-1)\n",
    "        loss = criterion(outs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        torch.save(model, '../checkpoint/resnet50_crop_detection/checkpoint_ep_%d.pth'% epoch)\n",
    "        \n",
    "        loss_value += loss.item()\n",
    "        matches += (preds == labels).sum().item()\n",
    "        \n",
    "    train_loss = loss_value / len(train_dataloader_mask)\n",
    "    train_acc = matches / len(mask_train_set)\n",
    "    \n",
    "    print(f\"epoch[{epoch}/{NUM_EPOCH}] training loss {train_loss:.3f}, training accuracy {train_acc:.3f}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        val_loss_items = []\n",
    "        val_acc_items = []\n",
    "        f1_score = 0\n",
    "        \n",
    "        for val_batch in tqdm(val_dataloader_mask):\n",
    "            inputs, labels = val_batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outs = model(inputs)\n",
    "            preds = torch.argmax(outs, dim=-1)\n",
    "            \n",
    "            loss_item = criterion(outs, labels).item()\n",
    "            acc_item = (labels==preds).sum().item()\n",
    "            val_loss_items.append(loss_item)\n",
    "            val_acc_items.append(acc_item)\n",
    "            f1_score += f1(outs, labels)\n",
    "            \n",
    "        val_loss = np.sum(val_loss_items) / len(val_dataloader_mask)\n",
    "        val_acc = np.sum(val_acc_items) / len(mask_val_set)\n",
    "\n",
    "        f1_score /= len(val_dataloader_mask)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            \n",
    "        if f1_score > best_f1_score:\n",
    "            best_f1_score = f1_score\n",
    "            cur_count = 0\n",
    "            torch.save(model, '../checkpoint/resnet50_crop_detection/checkpoint_best.pth')\n",
    "            print(\"Update checkpoint!!!\")\n",
    "        \n",
    "        else:\n",
    "            cur_count += 1\n",
    "            if cur_count >= patience:\n",
    "                print(\"Early Stopping!\")\n",
    "                break\n",
    "\n",
    "            \n",
    "        print(f\"[val] acc : {val_acc:.3f}, loss : {val_loss:.3f}, f1 score: {f1_score:.3f} -- size({len(val_dataloader_mask)})\")\n",
    "        print(f\"best acc : {best_val_acc:.3f}, best loss : {best_val_loss:.3f}, best f1 : {best_f1_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "36aba938-2065-4e7f-8ee5-4dee873bbee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f5214e73d60>\n"
     ]
    }
   ],
   "source": [
    "print(train_dataloader_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c2c15c-dbcb-4a3b-a6a7-28f7a45017ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8232b6a-1e46-43f2-8c35-4f4a63b0b478",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53605ae4-079a-4f2c-b06b-db356fc7acd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "70295eb5-704c-4e13-8616-c0d859889998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageID</th>\n",
       "      <th>ans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cbc5c6e168e63498590db46022617123f1fe1268.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0e72482bf56b3581c081f7da2a6180b8792c7089.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b549040c49190cedc41327748aeb197c1670f14d.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4f9cb2a045c6d5b9e50ad3459ea7b791eb6e18bc.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>248428d9a4a5b6229a7081c32851b90cb8d38d0c.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        ImageID  ans\n",
       "0  cbc5c6e168e63498590db46022617123f1fe1268.jpg    0\n",
       "1  0e72482bf56b3581c081f7da2a6180b8792c7089.jpg    0\n",
       "2  b549040c49190cedc41327748aeb197c1670f14d.jpg    0\n",
       "3  4f9cb2a045c6d5b9e50ad3459ea7b791eb6e18bc.jpg    0\n",
       "4  248428d9a4a5b6229a7081c32851b90cb8d38d0c.jpg    0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "test_dir_path = '/opt/ml/input/data/eval/'\n",
    "test_image_path = '/opt/ml/input/data/new_images/'\n",
    "\n",
    "model = torch.load('/opt/ml/checkpoint/resnet50_crop_detection/checkpoint_best.pth')\n",
    "submission = pd.read_csv(test_dir_path+'info.csv')\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "101ae6c5-6b88-47da-95c3-07c226a6b430",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = [os.path.join(test_image_path, img_id) for img_id in submission.ImageID]\n",
    "test_image = pd.Series(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0f28ee53-ecda-421c-8b09-d3be8a22324f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/ml/input/data/new_images/cbc5c6e168e63498590db46022617123f1fe1268.jpg'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "94502bcf-b429-4dd9-8da5-d7943bee5295",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test_Dataset(Dataset):\n",
    "    def __init__(self, midcrop=True, transform=None):\n",
    "        self.midcrop = midcrop\n",
    "        self.data = test_image\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(test_image)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.cvtColor(cv2.imread(self.data[idx]), cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "263522b6-32ab-4a7d-81ae-2c4b45a232fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "48e83da5-f10c-4a8d-9661-0106dac9927f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test inference is done!\n"
     ]
    }
   ],
   "source": [
    "dataset = Test_Dataset(transform = transforms.Compose([\n",
    "                            transforms.ToTensor()\n",
    "                        ]))\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "# 모델을 정의합니다. (학습한 모델이 있다면 torch.load로 모델을 불러주세요!)\n",
    "device = torch.device('cuda')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "all_predictions = []\n",
    "for images in loader:\n",
    "\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        pred = model(images)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "        all_predictions.extend(pred.cpu().numpy())\n",
    "submission['ans'] = all_predictions\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(test_dir_path, 'submission_detection_crop2.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d72715-8549-4db3-aa19-7e0dc5d2c7c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
