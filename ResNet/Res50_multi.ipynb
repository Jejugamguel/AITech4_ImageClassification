{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bef3aa5d-b523-4283-90f1-f863a4de8596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "113be85a-d27d-435f-9552-ceaffdaaef77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% |  0% |\n"
     ]
    }
   ],
   "source": [
    "import GPUtil\n",
    "GPUtil.showUtilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59a8a533-558c-4657-a7b3-92acc957082e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, sampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import os\n",
    "from torchmetrics import F1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fa563a8-0bdb-4330-a29b-faeaeed31517",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 12\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "338fdea2-af82-44d9-a493-f1b7b036d1f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>52</td>\n",
       "      <td>000002_female_Asian_52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>54</td>\n",
       "      <td>000004_male_Asian_54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>58</td>\n",
       "      <td>000005_female_Asian_58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>59</td>\n",
       "      <td>000006_female_Asian_59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>6954</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006954_male_Asian_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2696</th>\n",
       "      <td>6955</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006955_male_Asian_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>6956</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006956_male_Asian_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2698</th>\n",
       "      <td>6957</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>20</td>\n",
       "      <td>006957_male_Asian_20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2699</th>\n",
       "      <td>6959</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006959_male_Asian_19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2700 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  gender   race  age                    path\n",
       "0        1  female  Asian   45  000001_female_Asian_45\n",
       "1        2  female  Asian   52  000002_female_Asian_52\n",
       "2        4    male  Asian   54    000004_male_Asian_54\n",
       "3        5  female  Asian   58  000005_female_Asian_58\n",
       "4        6  female  Asian   59  000006_female_Asian_59\n",
       "...    ...     ...    ...  ...                     ...\n",
       "2695  6954    male  Asian   19    006954_male_Asian_19\n",
       "2696  6955    male  Asian   19    006955_male_Asian_19\n",
       "2697  6956    male  Asian   19    006956_male_Asian_19\n",
       "2698  6957    male  Asian   20    006957_male_Asian_20\n",
       "2699  6959    male  Asian   19    006959_male_Asian_19\n",
       "\n",
       "[2700 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir_path = '/opt/ml/input/data/train/'\n",
    "train_image_path = '/opt/ml/input/data/train/images/'\n",
    "\n",
    "dt_train = pd.read_csv(train_dir_path+'train.csv')\n",
    "dt_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "407a6eec-79bd-456a-ace9-ead98bab9dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_age_range(age):\n",
    "    if age < 30:\n",
    "        return 0\n",
    "    elif 30 <= age < 60:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f2692d4-9644-4621-9985-ff4e6fc35f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_train['age_range'] = dt_train['age'].apply(lambda x : get_age_range(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9eff3bc-7d7b-40fa-9a46-843dcef4acd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, valid_idx = train_test_split(np.arange(len(dt_train)),\n",
    "                                       test_size=0.2,\n",
    "                                       shuffle=True,\n",
    "                                       stratify=dt_train['age_range'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38e5d448-8936-45d0-9b15-9bd5def95bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image = []\n",
    "train_label = []\n",
    "\n",
    "for idx in train_idx:\n",
    "    path = dt_train.iloc[idx]['path']\n",
    "    for file_name in [i for i in os.listdir(train_image_path+path) if i[0] != '.']:\n",
    "        train_image.append(train_image_path+path+'/'+file_name)\n",
    "        train_label.append((path.split('_')[1], path.split('_')[3], file_name.split('.')[0]))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6051e6a-7ead-4714-8fbc-b5b30b741883",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_image = []\n",
    "valid_label = []\n",
    "\n",
    "for idx in valid_idx:\n",
    "    path = dt_train.iloc[idx]['path']\n",
    "    for file_name in [i for i in os.listdir(train_image_path+path) if i[0] != '.']:\n",
    "        valid_image.append(train_image_path+path+'/'+file_name)\n",
    "        valid_label.append((path.split('_')[1], path.split('_')[3], file_name.split('.')[0]))                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d0aae4d-6168-4ba0-acb1-88556e44eb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_enc(x):\n",
    "    def gender(i):\n",
    "        if i == 'male':\n",
    "            return 0\n",
    "        elif i == 'female':\n",
    "            return 1\n",
    "    def age(j):\n",
    "        j = int(j)\n",
    "        if j < 30:\n",
    "            return 0\n",
    "        elif j >= 30 and j < 60:\n",
    "            return 1\n",
    "        elif j >= 60:\n",
    "            return 2\n",
    "    def mask(k):\n",
    "        if k == 'normal':\n",
    "            return 2\n",
    "        elif 'incorrect' in k:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    return (mask(x[2]), gender(x[0]), age(x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b87a0f65-b621-4405-ac3f-283865b5f280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_preds(mask, gender, age):\n",
    "    return 6*mask + 3*gender + age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae81ab36-e190-4d1c-b007-1cac057e44bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.Series(train_image)\n",
    "train_label = pd.Series(train_label)\n",
    "\n",
    "valid_data = pd.Series(valid_image)\n",
    "valid_label = pd.Series(valid_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e80daf58-391b-443c-86f5-23dfa8457338",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_Mask(Dataset):\n",
    "    def __init__(self, data, label, encoding=True, transform=None):\n",
    "        self.encoding = encoding\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.transform = transform\n",
    "        \n",
    "        if encoding:\n",
    "            self.label = self.label.apply(onehot_enc)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X = Image.open(self.data[idx])\n",
    "        y = self.label[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            return self.transform(X), y\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b855ada-65f6-4822-8941-7387f4195f74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask_train_set = Dataset_Mask(data=train_data, label=train_label, transform = transforms.Compose([\n",
    "                                transforms.CenterCrop(350),\n",
    "                                transforms.RandomHorizontalFlip(p=0.5), \n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "                            ]))\n",
    "mask_val_set = Dataset_Mask(data=valid_data, label=valid_label, transform = transforms.Compose([\n",
    "                                transforms.CenterCrop(350),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "                            ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d5397ae-af79-49fd-a8b8-c4102a99a86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data size : 15120\n",
      "validation data size : 3780\n"
     ]
    }
   ],
   "source": [
    "print(f'training data size : {len(mask_train_set)}')\n",
    "print(f'validation data size : {len(mask_val_set)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e8a2393-a438-4e6e-aa49-efd4607dc625",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_dataloader_mask = DataLoader(dataset = mask_train_set, batch_size=batch_size, num_workers=2)\n",
    "val_dataloader_mask = DataLoader(dataset = mask_val_set, batch_size=batch_size, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85c1939c-a40c-4de1-bb82-d488af5fb0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "필요 입력 채널 개수 3\n",
      "네트워크 출력 채널 개수 1000\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.resnet50(pretrained=True)\n",
    "print('필요 입력 채널 개수', model.conv1.weight.shape[1])\n",
    "print('네트워크 출력 채널 개수', model.fc.weight.shape[0])\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7c71cb2-ed74-4672-a131-c2659f4bd22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "필요 입력 채널 개수 3\n",
      "네트워크 출력 채널 개수 8\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "class_num = 8\n",
    "model.fc = nn.Linear(in_features=2048, out_features=8, bias=True)\n",
    "nn.init.xavier_uniform_(model.fc.weight)\n",
    "stdv = 1. / math.sqrt(model.fc.weight.size(1))\n",
    "model.fc.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "\n",
    "print('필요 입력 채널 개수', model.conv1.weight.shape[1])\n",
    "print('네트워크 출력 채널 개수', model.fc.weight.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1998cd5e-319e-4483-afc0-1063c5b744f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"using {device}\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_EPOCH = 100\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9542e06d-5279-4a7b-b667-cb6aee332c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] name:[conv1.weight] shape:[(64, 3, 7, 7)].\n",
      "    val:[ 0.013  0.015 -0.015 -0.023 -0.041]\n",
      "[1] name:[bn1.weight] shape:[(64,)].\n",
      "    val:[0.239 0.291 0.316 0.271 0.217]\n",
      "[2] name:[bn1.bias] shape:[(64,)].\n",
      "    val:[0.225 0.606 0.012 0.133 0.18 ]\n",
      "[3] name:[layer1.0.conv1.weight] shape:[(64, 64, 1, 1)].\n",
      "    val:[ 0.004  0.04  -0.025 -0.028  0.089]\n",
      "[4] name:[layer1.0.bn1.weight] shape:[(64,)].\n",
      "    val:[0.213 0.188 0.141 0.153 0.132]\n",
      "[5] name:[layer1.0.bn1.bias] shape:[(64,)].\n",
      "    val:[ 0.433  0.047 -0.08   0.073  0.28 ]\n",
      "[6] name:[layer1.0.conv2.weight] shape:[(64, 64, 3, 3)].\n",
      "    val:[ 2.092e-09 -1.415e-09  6.585e-09  4.792e-09  3.282e-09]\n",
      "[7] name:[layer1.0.bn2.weight] shape:[(64,)].\n",
      "    val:[2.322e-08 1.381e-01 2.416e-01 1.363e-01 1.158e-01]\n",
      "[8] name:[layer1.0.bn2.bias] shape:[(64,)].\n",
      "    val:[-1.165e-07  3.803e-02  3.519e-01  1.243e-01  2.717e-01]\n",
      "[9] name:[layer1.0.conv3.weight] shape:[(256, 64, 1, 1)].\n",
      "    val:[-4.012e-09  1.773e-02  4.959e-02 -9.115e-03 -5.598e-02]\n",
      "[10] name:[layer1.0.bn3.weight] shape:[(256,)].\n",
      "    val:[0.061 0.    0.037 0.313 0.058]\n",
      "[11] name:[layer1.0.bn3.bias] shape:[(256,)].\n",
      "    val:[-0.005  0.073  0.03   0.054 -0.042]\n",
      "[12] name:[layer1.0.downsample.0.weight] shape:[(256, 64, 1, 1)].\n",
      "    val:[ 0.008 -0.166  0.009  0.009  0.006]\n",
      "[13] name:[layer1.0.downsample.1.weight] shape:[(256,)].\n",
      "    val:[0.245 0.23  0.146 0.343 0.026]\n",
      "[14] name:[layer1.0.downsample.1.bias] shape:[(256,)].\n",
      "    val:[-0.005  0.073  0.03   0.054 -0.042]\n",
      "[15] name:[layer1.1.conv1.weight] shape:[(64, 256, 1, 1)].\n",
      "    val:[0.025 0.003 0.011 0.005 0.009]\n",
      "[16] name:[layer1.1.bn1.weight] shape:[(64,)].\n",
      "    val:[0.154 0.324 0.248 0.211 0.178]\n",
      "[17] name:[layer1.1.bn1.bias] shape:[(64,)].\n",
      "    val:[ 0.058 -0.208 -0.11  -0.011 -0.052]\n",
      "[18] name:[layer1.1.conv2.weight] shape:[(64, 64, 3, 3)].\n",
      "    val:[ 0.015 -0.002  0.013  0.01  -0.013]\n",
      "[19] name:[layer1.1.bn2.weight] shape:[(64,)].\n",
      "    val:[0.3   0.188 0.168 0.17  0.251]\n",
      "[20] name:[layer1.1.bn2.bias] shape:[(64,)].\n",
      "    val:[-0.335 -0.101 -0.01   0.048 -0.096]\n",
      "[21] name:[layer1.1.conv3.weight] shape:[(256, 64, 1, 1)].\n",
      "    val:[ 0.062 -0.004 -0.001  0.001 -0.004]\n",
      "[22] name:[layer1.1.bn3.weight] shape:[(256,)].\n",
      "    val:[ 0.002 -0.007  0.095 -0.011  0.149]\n",
      "[23] name:[layer1.1.bn3.bias] shape:[(256,)].\n",
      "    val:[ 0.004  0.005 -0.066  0.002 -0.049]\n",
      "[24] name:[layer1.2.conv1.weight] shape:[(64, 256, 1, 1)].\n",
      "    val:[ 0.006  0.001 -0.004  0.007 -0.029]\n",
      "[25] name:[layer1.2.bn1.weight] shape:[(64,)].\n",
      "    val:[0.191 0.169 0.1   0.156 0.221]\n",
      "[26] name:[layer1.2.bn1.bias] shape:[(64,)].\n",
      "    val:[-0.021 -0.041 -0.011 -0.132 -0.103]\n",
      "[27] name:[layer1.2.conv2.weight] shape:[(64, 64, 3, 3)].\n",
      "    val:[ 0.033 -0.002 -0.026 -0.002 -0.014]\n",
      "[28] name:[layer1.2.bn2.weight] shape:[(64,)].\n",
      "    val:[0.216 0.251 0.223 0.229 0.221]\n",
      "[29] name:[layer1.2.bn2.bias] shape:[(64,)].\n",
      "    val:[-0.031 -0.092 -0.041 -0.065 -0.103]\n",
      "[30] name:[layer1.2.conv3.weight] shape:[(256, 64, 1, 1)].\n",
      "    val:[ 0.004 -0.003 -0.002  0.    -0.008]\n",
      "[31] name:[layer1.2.bn3.weight] shape:[(256,)].\n",
      "    val:[-0.002  0.004  0.242  0.013  0.2  ]\n",
      "[32] name:[layer1.2.bn3.bias] shape:[(256,)].\n",
      "    val:[ 0.002  0.005 -0.005  0.012 -0.088]\n",
      "[33] name:[layer2.0.conv1.weight] shape:[(128, 256, 1, 1)].\n",
      "    val:[ 0.01   0.015  0.026 -0.013 -0.012]\n",
      "[34] name:[layer2.0.bn1.weight] shape:[(128,)].\n",
      "    val:[0.264 0.187 0.203 0.202 0.203]\n",
      "[35] name:[layer2.0.bn1.bias] shape:[(128,)].\n",
      "    val:[-0.16  -0.092 -0.187 -0.097 -0.029]\n",
      "[36] name:[layer2.0.conv2.weight] shape:[(128, 128, 3, 3)].\n",
      "    val:[ 0.006  0.005  0.007 -0.001  0.009]\n",
      "[37] name:[layer2.0.bn2.weight] shape:[(128,)].\n",
      "    val:[0.225 0.193 0.146 0.247 0.225]\n",
      "[38] name:[layer2.0.bn2.bias] shape:[(128,)].\n",
      "    val:[-0.06   0.15   0.155 -0.06  -0.069]\n",
      "[39] name:[layer2.0.conv3.weight] shape:[(512, 128, 1, 1)].\n",
      "    val:[ 0.008 -0.006  0.013 -0.006  0.017]\n",
      "[40] name:[layer2.0.bn3.weight] shape:[(512,)].\n",
      "    val:[ 0.004  0.202 -0.001  0.139  0.006]\n",
      "[41] name:[layer2.0.bn3.bias] shape:[(512,)].\n",
      "    val:[ 0.05  -0.054 -0.005  0.021  0.047]\n",
      "[42] name:[layer2.0.downsample.0.weight] shape:[(512, 256, 1, 1)].\n",
      "    val:[ 0.07   0.004  0.013  0.013 -0.013]\n",
      "[43] name:[layer2.0.downsample.1.weight] shape:[(512,)].\n",
      "    val:[0.259 0.11  0.002 0.099 0.148]\n",
      "[44] name:[layer2.0.downsample.1.bias] shape:[(512,)].\n",
      "    val:[ 0.05  -0.054 -0.005  0.021  0.047]\n",
      "[45] name:[layer2.1.conv1.weight] shape:[(128, 512, 1, 1)].\n",
      "    val:[ 0.03   0.006 -0.001  0.007  0.034]\n",
      "[46] name:[layer2.1.bn1.weight] shape:[(128,)].\n",
      "    val:[0.114 0.076 0.091 0.112 0.116]\n",
      "[47] name:[layer2.1.bn1.bias] shape:[(128,)].\n",
      "    val:[ 0.069  0.147  0.083  0.041 -0.044]\n",
      "[48] name:[layer2.1.conv2.weight] shape:[(128, 128, 3, 3)].\n",
      "    val:[-0.027 -0.04  -0.017 -0.032 -0.005]\n",
      "[49] name:[layer2.1.bn2.weight] shape:[(128,)].\n",
      "    val:[0.178 0.125 0.182 0.2   0.189]\n",
      "[50] name:[layer2.1.bn2.bias] shape:[(128,)].\n",
      "    val:[-0.018  0.047  0.068  0.102 -0.008]\n",
      "[51] name:[layer2.1.conv3.weight] shape:[(512, 128, 1, 1)].\n",
      "    val:[-0.044  0.021 -0.004  0.019 -0.043]\n",
      "[52] name:[layer2.1.bn3.weight] shape:[(512,)].\n",
      "    val:[ 0.132 -0.004  0.24   0.02   0.141]\n",
      "[53] name:[layer2.1.bn3.bias] shape:[(512,)].\n",
      "    val:[-0.143 -0.041  0.096 -0.002 -0.064]\n",
      "[54] name:[layer2.2.conv1.weight] shape:[(128, 512, 1, 1)].\n",
      "    val:[ 0.007  0.006 -0.02  -0.023  0.008]\n",
      "[55] name:[layer2.2.bn1.weight] shape:[(128,)].\n",
      "    val:[0.107 0.133 0.197 0.204 0.123]\n",
      "[56] name:[layer2.2.bn1.bias] shape:[(128,)].\n",
      "    val:[ 0.137  0.023 -0.077  0.019  0.097]\n",
      "[57] name:[layer2.2.conv2.weight] shape:[(128, 128, 3, 3)].\n",
      "    val:[-0.003 -0.003  0.     0.011 -0.003]\n",
      "[58] name:[layer2.2.bn2.weight] shape:[(128,)].\n",
      "    val:[0.221 0.172 0.218 0.196 0.174]\n",
      "[59] name:[layer2.2.bn2.bias] shape:[(128,)].\n",
      "    val:[-0.069 -0.031 -0.073 -0.035 -0.019]\n",
      "[60] name:[layer2.2.conv3.weight] shape:[(512, 128, 1, 1)].\n",
      "    val:[-0.017  0.027 -0.006 -0.001 -0.018]\n",
      "[61] name:[layer2.2.bn3.weight] shape:[(512,)].\n",
      "    val:[0.057 0.051 0.07  0.201 0.213]\n",
      "[62] name:[layer2.2.bn3.bias] shape:[(512,)].\n",
      "    val:[-0.066  0.062 -0.109 -0.054 -0.129]\n",
      "[63] name:[layer2.3.conv1.weight] shape:[(128, 512, 1, 1)].\n",
      "    val:[ 0.004 -0.019  0.016  0.007 -0.017]\n",
      "[64] name:[layer2.3.bn1.weight] shape:[(128,)].\n",
      "    val:[0.193 0.174 0.202 0.143 0.198]\n",
      "[65] name:[layer2.3.bn1.bias] shape:[(128,)].\n",
      "    val:[-0.121 -0.008 -0.105  0.044 -0.088]\n",
      "[66] name:[layer2.3.conv2.weight] shape:[(128, 128, 3, 3)].\n",
      "    val:[ 0.006  0.008 -0.006 -0.003  0.005]\n",
      "[67] name:[layer2.3.bn2.weight] shape:[(128,)].\n",
      "    val:[0.207 0.247 0.168 0.243 0.216]\n",
      "[68] name:[layer2.3.bn2.bias] shape:[(128,)].\n",
      "    val:[-0.062 -0.164 -0.035 -0.12  -0.068]\n",
      "[69] name:[layer2.3.conv3.weight] shape:[(512, 128, 1, 1)].\n",
      "    val:[-0.002 -0.008 -0.022  0.    -0.011]\n",
      "[70] name:[layer2.3.bn3.weight] shape:[(512,)].\n",
      "    val:[0.012 0.117 0.014 0.098 0.062]\n",
      "[71] name:[layer2.3.bn3.bias] shape:[(512,)].\n",
      "    val:[-0.011  0.123  0.018 -0.147 -0.102]\n",
      "[72] name:[layer3.0.conv1.weight] shape:[(256, 512, 1, 1)].\n",
      "    val:[ 0.014  0.009  0.019  0.022 -0.027]\n",
      "[73] name:[layer3.0.bn1.weight] shape:[(256,)].\n",
      "    val:[0.195 0.26  0.203 0.175 0.222]\n",
      "[74] name:[layer3.0.bn1.bias] shape:[(256,)].\n",
      "    val:[-0.072 -0.212 -0.078  0.02  -0.161]\n",
      "[75] name:[layer3.0.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[-0.036 -0.044 -0.032 -0.03  -0.01 ]\n",
      "[76] name:[layer3.0.bn2.weight] shape:[(256,)].\n",
      "    val:[0.18  0.231 0.181 0.153 0.162]\n",
      "[77] name:[layer3.0.bn2.bias] shape:[(256,)].\n",
      "    val:[ 0.077 -0.044 -0.001  0.155 -0.034]\n",
      "[78] name:[layer3.0.conv3.weight] shape:[(1024, 256, 1, 1)].\n",
      "    val:[-0.01   0.02   0.037 -0.08   0.026]\n",
      "[79] name:[layer3.0.bn3.weight] shape:[(1024,)].\n",
      "    val:[0.161 0.116 0.089 0.118 0.087]\n",
      "[80] name:[layer3.0.bn3.bias] shape:[(1024,)].\n",
      "    val:[-0.014 -0.025 -0.027  0.003 -0.029]\n",
      "[81] name:[layer3.0.downsample.0.weight] shape:[(1024, 512, 1, 1)].\n",
      "    val:[ 0.029  0.021  0.008 -0.008 -0.002]\n",
      "[82] name:[layer3.0.downsample.1.weight] shape:[(1024,)].\n",
      "    val:[0.105 0.095 0.067 0.136 0.07 ]\n",
      "[83] name:[layer3.0.downsample.1.bias] shape:[(1024,)].\n",
      "    val:[-0.014 -0.025 -0.027  0.003 -0.029]\n",
      "[84] name:[layer3.1.conv1.weight] shape:[(256, 1024, 1, 1)].\n",
      "    val:[ 0.004 -0.013  0.002  0.002 -0.005]\n",
      "[85] name:[layer3.1.bn1.weight] shape:[(256,)].\n",
      "    val:[0.127 0.108 0.112 0.113 0.14 ]\n",
      "[86] name:[layer3.1.bn1.bias] shape:[(256,)].\n",
      "    val:[ 0.122  0.085  0.044  0.046 -0.021]\n",
      "[87] name:[layer3.1.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[-0.002  0.002 -0.009  0.003 -0.002]\n",
      "[88] name:[layer3.1.bn2.weight] shape:[(256,)].\n",
      "    val:[0.173 0.207 0.187 0.167 0.174]\n",
      "[89] name:[layer3.1.bn2.bias] shape:[(256,)].\n",
      "    val:[-0.068 -0.085 -0.039 -0.035 -0.09 ]\n",
      "[90] name:[layer3.1.conv3.weight] shape:[(1024, 256, 1, 1)].\n",
      "    val:[-0.006  0.024  0.002  0.014  0.003]\n",
      "[91] name:[layer3.1.bn3.weight] shape:[(1024,)].\n",
      "    val:[0.048 0.082 0.112 0.089 0.113]\n",
      "[92] name:[layer3.1.bn3.bias] shape:[(1024,)].\n",
      "    val:[-0.065 -0.026 -0.006 -0.038 -0.011]\n",
      "[93] name:[layer3.2.conv1.weight] shape:[(256, 1024, 1, 1)].\n",
      "    val:[ 0.003 -0.028 -0.003 -0.006 -0.021]\n",
      "[94] name:[layer3.2.bn1.weight] shape:[(256,)].\n",
      "    val:[0.223 0.187 0.117 0.134 0.177]\n",
      "[95] name:[layer3.2.bn1.bias] shape:[(256,)].\n",
      "    val:[-0.115 -0.199  0.063  0.042 -0.096]\n",
      "[96] name:[layer3.2.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[-0.004 -0.011  0.001  0.012  0.02 ]\n",
      "[97] name:[layer3.2.bn2.weight] shape:[(256,)].\n",
      "    val:[0.097 0.181 0.194 0.164 0.211]\n",
      "[98] name:[layer3.2.bn2.bias] shape:[(256,)].\n",
      "    val:[ 0.126 -0.059 -0.153 -0.047 -0.118]\n",
      "[99] name:[layer3.2.conv3.weight] shape:[(1024, 256, 1, 1)].\n",
      "    val:[-0.024  0.009  0.018 -0.008 -0.012]\n",
      "[100] name:[layer3.2.bn3.weight] shape:[(1024,)].\n",
      "    val:[0.087 0.082 0.131 0.068 0.106]\n",
      "[101] name:[layer3.2.bn3.bias] shape:[(1024,)].\n",
      "    val:[ 0.069 -0.059 -0.013 -0.048 -0.013]\n",
      "[102] name:[layer3.3.conv1.weight] shape:[(256, 1024, 1, 1)].\n",
      "    val:[-0.015 -0.01  -0.     0.002 -0.018]\n",
      "[103] name:[layer3.3.bn1.weight] shape:[(256,)].\n",
      "    val:[0.223 0.174 0.181 0.174 0.211]\n",
      "[104] name:[layer3.3.bn1.bias] shape:[(256,)].\n",
      "    val:[-0.149 -0.11  -0.135 -0.125 -0.153]\n",
      "[105] name:[layer3.3.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[-0.02  -0.019 -0.018  0.003  0.025]\n",
      "[106] name:[layer3.3.bn2.weight] shape:[(256,)].\n",
      "    val:[0.117 0.149 0.112 0.196 0.132]\n",
      "[107] name:[layer3.3.bn2.bias] shape:[(256,)].\n",
      "    val:[ 0.046  0.011  0.054 -0.103 -0.004]\n",
      "[108] name:[layer3.3.conv3.weight] shape:[(1024, 256, 1, 1)].\n",
      "    val:[ 0.003  0.015 -0.04  -0.016  0.025]\n",
      "[109] name:[layer3.3.bn3.weight] shape:[(1024,)].\n",
      "    val:[0.036 0.134 0.107 0.034 0.106]\n",
      "[110] name:[layer3.3.bn3.bias] shape:[(1024,)].\n",
      "    val:[ 0.004 -0.052 -0.089 -0.036 -0.052]\n",
      "[111] name:[layer3.4.conv1.weight] shape:[(256, 1024, 1, 1)].\n",
      "    val:[-0.034  0.032 -0.02  -0.018 -0.024]\n",
      "[112] name:[layer3.4.bn1.weight] shape:[(256,)].\n",
      "    val:[0.129 0.181 0.143 0.15  0.164]\n",
      "[113] name:[layer3.4.bn1.bias] shape:[(256,)].\n",
      "    val:[ 0.001 -0.072 -0.015 -0.066 -0.061]\n",
      "[114] name:[layer3.4.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[-0.009  0.008  0.01  -0.029 -0.009]\n",
      "[115] name:[layer3.4.bn2.weight] shape:[(256,)].\n",
      "    val:[0.201 0.152 0.126 0.187 0.192]\n",
      "[116] name:[layer3.4.bn2.bias] shape:[(256,)].\n",
      "    val:[-0.153  0.019  0.006 -0.108 -0.188]\n",
      "[117] name:[layer3.4.conv3.weight] shape:[(1024, 256, 1, 1)].\n",
      "    val:[-0.001  0.002  0.002  0.002  0.002]\n",
      "[118] name:[layer3.4.bn3.weight] shape:[(1024,)].\n",
      "    val:[0.001 0.183 0.151 0.037 0.11 ]\n",
      "[119] name:[layer3.4.bn3.bias] shape:[(1024,)].\n",
      "    val:[-0.017 -0.213 -0.143 -0.043 -0.102]\n",
      "[120] name:[layer3.5.conv1.weight] shape:[(256, 1024, 1, 1)].\n",
      "    val:[-0.001  0.018  0.047 -0.032  0.008]\n",
      "[121] name:[layer3.5.bn1.weight] shape:[(256,)].\n",
      "    val:[0.195 0.21  0.121 0.215 0.201]\n",
      "[122] name:[layer3.5.bn1.bias] shape:[(256,)].\n",
      "    val:[-0.179 -0.21   0.08  -0.166 -0.197]\n",
      "[123] name:[layer3.5.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[-0.018 -0.037 -0.024  0.003  0.026]\n",
      "[124] name:[layer3.5.bn2.weight] shape:[(256,)].\n",
      "    val:[0.18  0.16  0.178 0.206 0.17 ]\n",
      "[125] name:[layer3.5.bn2.bias] shape:[(256,)].\n",
      "    val:[-0.095 -0.043 -0.065 -0.111 -0.137]\n",
      "[126] name:[layer3.5.conv3.weight] shape:[(1024, 256, 1, 1)].\n",
      "    val:[-0.001  0.002  0.001 -0.003  0.003]\n",
      "[127] name:[layer3.5.bn3.weight] shape:[(1024,)].\n",
      "    val:[0.01  0.241 0.129 0.07  0.121]\n",
      "[128] name:[layer3.5.bn3.bias] shape:[(1024,)].\n",
      "    val:[-0.01  -0.278 -0.168 -0.087 -0.159]\n",
      "[129] name:[layer4.0.conv1.weight] shape:[(512, 1024, 1, 1)].\n",
      "    val:[-0.011  0.006  0.009 -0.024 -0.005]\n",
      "[130] name:[layer4.0.bn1.weight] shape:[(512,)].\n",
      "    val:[0.246 0.203 0.214 0.194 0.251]\n",
      "[131] name:[layer4.0.bn1.bias] shape:[(512,)].\n",
      "    val:[-0.231 -0.176 -0.094 -0.153 -0.271]\n",
      "[132] name:[layer4.0.conv2.weight] shape:[(512, 512, 3, 3)].\n",
      "    val:[-1.893e-05 -7.486e-03  9.649e-03 -2.248e-03 -2.001e-04]\n",
      "[133] name:[layer4.0.bn2.weight] shape:[(512,)].\n",
      "    val:[0.23  0.215 0.211 0.25  0.199]\n",
      "[134] name:[layer4.0.bn2.bias] shape:[(512,)].\n",
      "    val:[-0.092 -0.141 -0.052 -0.1   -0.065]\n",
      "[135] name:[layer4.0.conv3.weight] shape:[(2048, 512, 1, 1)].\n",
      "    val:[-0.001  0.003 -0.011 -0.012 -0.012]\n",
      "[136] name:[layer4.0.bn3.weight] shape:[(2048,)].\n",
      "    val:[0.314 0.425 0.397 0.362 0.493]\n",
      "[137] name:[layer4.0.bn3.bias] shape:[(2048,)].\n",
      "    val:[-0.059 -0.092 -0.07  -0.053 -0.066]\n",
      "[138] name:[layer4.0.downsample.0.weight] shape:[(2048, 1024, 1, 1)].\n",
      "    val:[-0.001 -0.005  0.002 -0.013 -0.005]\n",
      "[139] name:[layer4.0.downsample.1.weight] shape:[(2048,)].\n",
      "    val:[0.239 0.275 0.259 0.218 0.394]\n",
      "[140] name:[layer4.0.downsample.1.bias] shape:[(2048,)].\n",
      "    val:[-0.059 -0.092 -0.07  -0.053 -0.066]\n",
      "[141] name:[layer4.1.conv1.weight] shape:[(512, 2048, 1, 1)].\n",
      "    val:[-0.023 -0.001 -0.015  0.012 -0.038]\n",
      "[142] name:[layer4.1.bn1.weight] shape:[(512,)].\n",
      "    val:[0.188 0.193 0.172 0.2   0.205]\n",
      "[143] name:[layer4.1.bn1.bias] shape:[(512,)].\n",
      "    val:[-0.127 -0.109 -0.072 -0.147 -0.161]\n",
      "[144] name:[layer4.1.conv2.weight] shape:[(512, 512, 3, 3)].\n",
      "    val:[-0.007 -0.007 -0.003 -0.014 -0.016]\n",
      "[145] name:[layer4.1.bn2.weight] shape:[(512,)].\n",
      "    val:[0.188 0.226 0.186 0.223 0.229]\n",
      "[146] name:[layer4.1.bn2.bias] shape:[(512,)].\n",
      "    val:[-0.138 -0.144 -0.085 -0.15  -0.188]\n",
      "[147] name:[layer4.1.conv3.weight] shape:[(2048, 512, 1, 1)].\n",
      "    val:[ 0.001 -0.003  0.005 -0.013 -0.011]\n",
      "[148] name:[layer4.1.bn3.weight] shape:[(2048,)].\n",
      "    val:[0.287 0.502 0.329 0.263 0.383]\n",
      "[149] name:[layer4.1.bn3.bias] shape:[(2048,)].\n",
      "    val:[-0.092 -0.079 -0.074 -0.054 -0.05 ]\n",
      "[150] name:[layer4.2.conv1.weight] shape:[(512, 2048, 1, 1)].\n",
      "    val:[0.014 0.013 0.003 0.001 0.031]\n",
      "[151] name:[layer4.2.bn1.weight] shape:[(512,)].\n",
      "    val:[0.214 0.204 0.213 0.221 0.211]\n",
      "[152] name:[layer4.2.bn1.bias] shape:[(512,)].\n",
      "    val:[-0.156 -0.156 -0.187 -0.21  -0.167]\n",
      "[153] name:[layer4.2.conv2.weight] shape:[(512, 512, 3, 3)].\n",
      "    val:[0.004 0.006 0.008 0.006 0.009]\n",
      "[154] name:[layer4.2.bn2.weight] shape:[(512,)].\n",
      "    val:[0.226 0.181 0.184 0.235 0.244]\n",
      "[155] name:[layer4.2.bn2.bias] shape:[(512,)].\n",
      "    val:[-0.1   -0.009 -0.059 -0.143 -0.169]\n",
      "[156] name:[layer4.2.conv3.weight] shape:[(2048, 512, 1, 1)].\n",
      "    val:[ 0.005  0.008 -0.024 -0.003 -0.004]\n",
      "[157] name:[layer4.2.bn3.weight] shape:[(2048,)].\n",
      "    val:[0.643 0.832 0.786 0.675 0.673]\n",
      "[158] name:[layer4.2.bn3.bias] shape:[(2048,)].\n",
      "    val:[0.008 0.055 0.035 0.019 0.042]\n",
      "[159] name:[fc.weight] shape:[(8, 2048)].\n",
      "    val:[ 0.048 -0.028  0.048 -0.026 -0.05 ]\n",
      "[160] name:[fc.bias] shape:[(8,)].\n",
      "    val:[ 0.005  0.001 -0.005  0.002 -0.003]\n",
      "Total number of parameters:[23,524,424].\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "n_param = 0\n",
    "for p_idx, (param_name, param) in enumerate(model.named_parameters()):\n",
    "    if param.requires_grad:\n",
    "        param_numpy = param.detach().cpu().numpy()\n",
    "        n_param += len(param_numpy.reshape(-1))\n",
    "        print (\"[%d] name:[%s] shape:[%s].\"%(p_idx,param_name,param_numpy.shape))\n",
    "        print (\"    val:%s\"%(param_numpy.reshape(-1)[:5]))\n",
    "print (\"Total number of parameters:[%s].\"%(format(n_param,',d')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "360fe81b-b8ef-4c3a-a016-fefb8c3e85e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6840439883544ec8b55aef19fec52b29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=237.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-32ee4850441f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             torch.save({\n\u001b[0m\u001b[1;32m     40\u001b[0m                 \u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0;34m'optimizer_state_dict'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;31m# Now that it is on the CPU we can directly copy it into the zip file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         \u001b[0mnum_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m         \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_val_acc = 0\n",
    "best_val_loss = np.inf\n",
    "patience = 10\n",
    "cur_count = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    model.train()\n",
    "    loss_value = 0\n",
    "    matches = 0\n",
    "    for train_batch in tqdm(train_dataloader_mask):\n",
    "        inputs, (mask_labels, gender_labels, age_labels) = train_batch\n",
    "        inputs = inputs.to(device)\n",
    "        mask_labels = mask_labels.to(device)\n",
    "        gender_labels = gender_labels.to(device)\n",
    "        age_labels = age_labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outs = model(inputs)\n",
    "        (mask_outs, gender_outs, age_outs) = torch.split(outs, [3,2,3], dim=1)\n",
    "        \n",
    "        mask_loss = criterion(mask_outs, mask_labels)\n",
    "        gender_loss = criterion(gender_outs, gender_labels)\n",
    "        age_loss = criterion(age_outs, age_labels)\n",
    "        \n",
    "        loss = mask_loss + gender_loss + 1.5*age_loss\n",
    "        \n",
    "        mask_preds = torch.argmax(mask_outs, dim=-1)\n",
    "        gender_preds = torch.argmax(gender_outs, dim=-1)\n",
    "        age_preds = torch.argmax(age_outs, dim=-1)\n",
    "        \n",
    "        preds = final_preds(mask_preds, gender_preds, age_preds)\n",
    "        labels = final_preds(mask_labels, gender_labels, age_labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        if epoch % 10 == 0:\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'epoch' : epoch\n",
    "                }, f'/opt/ml/checkpoint/res50_multi/checkpoint_ep_{epoch}.tar')\n",
    "        \n",
    "        loss_value += loss.item()\n",
    "        matches += (preds == labels).sum().item()\n",
    "        \n",
    "    train_loss = loss_value / len(train_dataloader_mask)\n",
    "    train_acc = matches / len(mask_train_set)\n",
    "        \n",
    "    print(f\"epoch[{epoch}/{NUM_EPOCH}] training loss {train_loss:.5f}, training accuracy {train_acc:.5f}\")\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        val_loss_items = []\n",
    "        val_acc_items = []\n",
    "        for val_batch in tqdm(val_dataloader_mask):\n",
    "            inputs, (mask_labels, gender_labels, age_labels) = val_batch\n",
    "            inputs = inputs.to(device)\n",
    "            mask_labels = mask_labels.to(device)\n",
    "            gender_labels = gender_labels.to(device)\n",
    "            age_labels = age_labels.to(device)\n",
    "            \n",
    "            outs = model(inputs)\n",
    "            \n",
    "            (mask_outs, gender_outs, age_outs) = torch.split(outs, [3,2,3], dim=1)\n",
    "        \n",
    "            mask_loss = criterion(mask_outs, mask_labels)\n",
    "            gender_loss = criterion(gender_outs, gender_labels)\n",
    "            age_loss = criterion(age_outs, age_labels)\n",
    "            \n",
    "            loss = mask_loss + gender_loss + 1.5*age_loss\n",
    "            \n",
    "            mask_preds = torch.argmax(mask_outs, dim=-1)\n",
    "            gender_preds = torch.argmax(gender_outs, dim=-1)\n",
    "            age_preds = torch.argmax(age_outs, dim=-1)\n",
    "            \n",
    "            preds = final_preds(mask_preds, gender_preds, age_preds)\n",
    "            labels = final_preds(mask_labels, gender_labels, age_labels)\n",
    "            \n",
    "            loss_item = loss.item()\n",
    "            acc_item = (labels==preds).sum().item()\n",
    "            \n",
    "            val_loss_items.append(loss_item)\n",
    "            val_acc_items.append(acc_item)\n",
    "            \n",
    "        val_loss = np.sum(val_loss_items) / len(val_dataloader_mask)\n",
    "        val_acc = np.sum(val_acc_items) / len(mask_val_set)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'epoch' : epoch\n",
    "                }, f'/opt/ml/checkpoint/res50_multi/best_checkpoint.tar')\n",
    "            print('checkpoint saved!')\n",
    "            cur_count = 0\n",
    "        else:\n",
    "            cur_count += 1\n",
    "            if cur_count >= patience:\n",
    "                print(\"Early Stopping!\")\n",
    "                break\n",
    "            \n",
    "        print(f\"[val] acc : {val_acc:.5f}, loss : {val_loss:.5f}\")\n",
    "        print(f\"best acc : {best_val_acc:.5f}, best loss : {best_val_loss:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6c9e998a-55fe-4212-b548-a154d1edb257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageID</th>\n",
       "      <th>ans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cbc5c6e168e63498590db46022617123f1fe1268.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0e72482bf56b3581c081f7da2a6180b8792c7089.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b549040c49190cedc41327748aeb197c1670f14d.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4f9cb2a045c6d5b9e50ad3459ea7b791eb6e18bc.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>248428d9a4a5b6229a7081c32851b90cb8d38d0c.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        ImageID  ans\n",
       "0  cbc5c6e168e63498590db46022617123f1fe1268.jpg    0\n",
       "1  0e72482bf56b3581c081f7da2a6180b8792c7089.jpg    0\n",
       "2  b549040c49190cedc41327748aeb197c1670f14d.jpg    0\n",
       "3  4f9cb2a045c6d5b9e50ad3459ea7b791eb6e18bc.jpg    0\n",
       "4  248428d9a4a5b6229a7081c32851b90cb8d38d0c.jpg    0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "test_dir_path = '/opt/ml/input/data/eval/'\n",
    "test_image_path = '/opt/ml/input/data/eval/images/'\n",
    "\n",
    "checkpoint = torch.load('/opt/ml/checkpoint/res50_multi/checkpoint_ep_0.tar')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "submission = pd.read_csv(test_dir_path+'info.csv')\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c54bcbdf-efa5-4a36-9784-154591cd52cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = [os.path.join(test_image_path, img_id) for img_id in submission.ImageID]\n",
    "test_image = pd.Series(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2688e0f0-0a73-4bf5-8978-18f3d775c4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test_Dataset(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.data = test_image\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(test_image)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.data[idx])\n",
    "            \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c626e1f9-b3d1-4373-82e3-a1a99e16c2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torchvision/transforms/transforms.py:329: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test inference is done!\n"
     ]
    }
   ],
   "source": [
    "dataset = Test_Dataset(transform = transforms.Compose([\n",
    "                            transforms.CenterCrop(350),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "                        ]))\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "# 모델을 정의합니다. (학습한 모델이 있다면 torch.load로 모델을 불러주세요!)\n",
    "device = torch.device('cuda')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "all_predictions = []\n",
    "for images in loader:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        outs = model(images)\n",
    "        (mask_outs, gender_outs, age_outs) = torch.split(outs, [3,2,3], dim=1)\n",
    "        mask_preds = torch.argmax(mask_outs, dim=-1)\n",
    "        gender_preds = torch.argmax(gender_outs, dim=-1)\n",
    "        age_preds = torch.argmax(age_outs, dim=-1)\n",
    "        pred = final_preds(mask_preds, gender_preds, age_preds)\n",
    "        all_predictions.extend(pred.cpu().numpy())\n",
    "submission['ans'] = all_predictions\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(test_dir_path, 'submission_res50_multi_2.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6609b8b-e3ce-4897-8f6c-e0d132729141",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
