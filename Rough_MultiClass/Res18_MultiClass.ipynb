{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b1d65a3-78de-4b01-a862-e6bd4c7ef135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15beaba-39cc-48da-8165-9f2abc16322a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('input/data')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b3beb99-f9c7-4026-989a-cbaa8de1f632",
   "metadata": {},
   "outputs": [],
   "source": [
    "!find . -regex \".*\\.\\_[a-zA-Z0-9._]+\" -delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fa8d19de-5a8e-454f-a22b-129e94792395",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 12\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6f69f93c-5675-433f-8a88-7bf8e411d51c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000002</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>52</td>\n",
       "      <td>000002_female_Asian_52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000004</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>54</td>\n",
       "      <td>000004_male_Asian_54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000005</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>58</td>\n",
       "      <td>000005_female_Asian_58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000006</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>59</td>\n",
       "      <td>000006_female_Asian_59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>006954</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006954_male_Asian_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2696</th>\n",
       "      <td>006955</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006955_male_Asian_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>006956</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006956_male_Asian_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2698</th>\n",
       "      <td>006957</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>20</td>\n",
       "      <td>006957_male_Asian_20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2699</th>\n",
       "      <td>006959</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006959_male_Asian_19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2700 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  gender   race  age                    path\n",
       "0     000001  female  Asian   45  000001_female_Asian_45\n",
       "1     000002  female  Asian   52  000002_female_Asian_52\n",
       "2     000004    male  Asian   54    000004_male_Asian_54\n",
       "3     000005  female  Asian   58  000005_female_Asian_58\n",
       "4     000006  female  Asian   59  000006_female_Asian_59\n",
       "...      ...     ...    ...  ...                     ...\n",
       "2695  006954    male  Asian   19    006954_male_Asian_19\n",
       "2696  006955    male  Asian   19    006955_male_Asian_19\n",
       "2697  006956    male  Asian   19    006956_male_Asian_19\n",
       "2698  006957    male  Asian   20    006957_male_Asian_20\n",
       "2699  006959    male  Asian   19    006959_male_Asian_19\n",
       "\n",
       "[2700 rows x 5 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir_path = '/opt/ml/input/data/train/'\n",
    "train_image_path = '/opt/ml/input/data/train/images/'\n",
    "\n",
    "dt_train = pd.read_csv(train_dir_path+'train.csv')\n",
    "dt_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "304c6a61-ff17-4777-8132-18def61c8102",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_image_path = []\n",
    "whole_target_label = []\n",
    "\n",
    "for path in dt_train['path']:\n",
    "    for file_name in [i for i in os.listdir(train_image_path+path) if '._' not in i]:\n",
    "        whole_image_path.append(train_image_path+path+'/'+file_name)\n",
    "        whole_target_label.append((path.split('_')[1], path.split('_')[3], file_name.split('.')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "98de1eb4-55d4-41ba-8541-70fff9bb011e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_enc(x):\n",
    "    def gender(i):\n",
    "        if i == 'male':\n",
    "            return 0\n",
    "        elif i == 'female':\n",
    "            return 3\n",
    "    def age(j):\n",
    "        j = int(j)\n",
    "        if j < 30:\n",
    "            return 0\n",
    "        elif j >= 30 and j < 60:\n",
    "            return 1\n",
    "        elif j >= 60:\n",
    "            return 2\n",
    "    def mask(k):\n",
    "        if k == 'normal':\n",
    "            return 12\n",
    "        elif 'incorrect' in k:\n",
    "            return 6\n",
    "        else:\n",
    "            return 0\n",
    "    return gender(x[0]) + age(x[1]) + mask(x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "af245588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc_genmsk(x):\n",
    "    # x 입력형식 : (성별, 나이, 마스크) 튜플\n",
    "    # 출력형식 : (나이, 성별-마스크 인코딩) 튜플\n",
    "    def gender(i):\n",
    "        if i == 'male':\n",
    "            return 0\n",
    "        elif i == 'female':\n",
    "            return 3\n",
    "    def mask(k):\n",
    "        if k == 'normal':\n",
    "            return 12\n",
    "        elif 'incorrect' in k:\n",
    "            return 6\n",
    "        else:\n",
    "            return 0\n",
    "    if int(x[1]) == 60:\n",
    "        return 70, (gender(x[0])+mask(x[2]))//3\n",
    "    return int(x[1]), (gender(x[0])+mask(x[2]))//3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c0013779",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc_sepclass(x):\n",
    "    # x 입력형식 : (성별, 나이, 마스크) 튜플\n",
    "    # 출력형식 : (성별인코딩, 나이인코딩, 마스크인코딩)\n",
    "    def gender(i):\n",
    "        if i == 'male':\n",
    "            return 0\n",
    "        elif i == 'female':\n",
    "            return 1\n",
    "    def age(j):\n",
    "        j = int(j)\n",
    "        if j < 30:\n",
    "            return 0\n",
    "        elif j >= 30 and j < 60:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2\n",
    "    def mask(k):\n",
    "        if k == 'normal':\n",
    "            return 2\n",
    "        elif 'incorrect' in k:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    return gender(x[0]), age(x[1]), mask(x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "694474c7-7175-40e6-b071-e50218629f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_data = pd.Series(whole_image_path)\n",
    "sr_label = pd.Series(whole_target_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e2cd7b29-5f03-444e-b781-902ca74f2085",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_Mask(Dataset):\n",
    "    def __init__(self, encoding=True, midcrop=True, transform=None):\n",
    "        self.encoding = encoding\n",
    "        self.midcrop = midcrop\n",
    "        self.data = sr_data\n",
    "        self.label = sr_label\n",
    "        self.transform = transform\n",
    "        \n",
    "        if encoding:\n",
    "            self.label = self.label.apply(enc_sepclass)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(sr_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X = cv2.cvtColor(cv2.imread(self.data[idx]), cv2.COLOR_BGR2RGB)\n",
    "        y = self.label[idx]\n",
    "        \n",
    "        if self.midcrop:\n",
    "            X = X[85:435, 17:367]\n",
    "        \n",
    "        if self.transform:\n",
    "            return self.transform(X), y\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e2a22aff-e634-4f15-877c-752d450d01da",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mask = Dataset_Mask(transform = transforms.Compose([\n",
    "                                transforms.ToTensor()\n",
    "                            ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5788c44b-07ac-4820-a22d-b6ffa3fc81f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(dataset_mask) * 0.8)\n",
    "val_size = int(len(dataset_mask) * 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0e285cb6-ee37-4a0c-a804-7506dcfebc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data size : 15120\n",
      "validation data size : 3780\n"
     ]
    }
   ],
   "source": [
    "mask_train_set, mask_val_set = torch.utils.data.random_split(dataset_mask, [train_size, val_size])\n",
    "print(f'training data size : {len(mask_train_set)}')\n",
    "print(f'validation data size : {len(mask_val_set)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "37b1ced0-1e34-4d50-b2b2-dcdff8fd33f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_dataloader_mask = DataLoader(dataset = mask_train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_dataloader_mask = DataLoader(dataset = mask_val_set, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7dc2e58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class R18_RegCls(torchvision.models.ResNet):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.out1 = nn.Linear(512, 2, bias=True)\n",
    "        self.out2 = nn.Linear(512, 3, bias=True)\n",
    "        self.out3 = nn.Linear(512, 3, bias=True)\n",
    "        \n",
    "    def _forward_impl(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        # out1 : gender class\n",
    "        # out2 : age class\n",
    "        # out3 : mask class\n",
    "        out1 = self.out1(x)\n",
    "        out2 = self.out2(x)\n",
    "        out3 = self.out3(x)\n",
    "        \n",
    "        return out1, out2, out3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af71a4ef-a137-47b0-af94-eebedaa36014",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torchvision' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/opt/ml/cv-12/Rough_MultiClass/Res18_MultiClass.ipynb 셀 17\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2241495354414745227d/opt/ml/cv-12/Rough_MultiClass/Res18_MultiClass.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m block \u001b[39m=\u001b[39m torchvision\u001b[39m.\u001b[39mmodels\u001b[39m.\u001b[39mresnet\u001b[39m.\u001b[39mBasicBlock\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2241495354414745227d/opt/ml/cv-12/Rough_MultiClass/Res18_MultiClass.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m layers \u001b[39m=\u001b[39m [\u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2241495354414745227d/opt/ml/cv-12/Rough_MultiClass/Res18_MultiClass.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m basemodel_resnet18 \u001b[39m=\u001b[39m R18_RegCls(block \u001b[39m=\u001b[39m block, layers \u001b[39m=\u001b[39m layers)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torchvision' is not defined"
     ]
    }
   ],
   "source": [
    "block = torchvision.models.resnet.BasicBlock\n",
    "layers = [2, 2, 2, 2]\n",
    "\n",
    "basemodel_resnet18 = R18_RegCls(block = block, layers = layers)\n",
    "print('필요 입력 채널 개수', basemodel_resnet18.conv1.weight.shape[1])\n",
    "print('네트워크 출력 채널 개수_gen/msk', basemodel_resnet18.out1.weight.shape[0])\n",
    "print('네트워크 출력 채널 개수_age', basemodel_resnet18.out2.weight.shape[0])\n",
    "print(basemodel_resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b0effe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11357fc4-548b-452e-bbd8-e60ef6261308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0438, -0.0340, -0.0307])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "nn.init.xavier_uniform_(basemodel_resnet18.out1.weight)\n",
    "stdv = 1. / math.sqrt(basemodel_resnet18.out1.weight.size(1))\n",
    "basemodel_resnet18.out1.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "nn.init.xavier_uniform_(basemodel_resnet18.out2.weight)\n",
    "stdv = 1. / math.sqrt(basemodel_resnet18.out2.weight.size(1))\n",
    "basemodel_resnet18.out2.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "nn.init.xavier_uniform_(basemodel_resnet18.out3.weight)\n",
    "stdv = 1. / math.sqrt(basemodel_resnet18.out3.weight.size(1))\n",
    "basemodel_resnet18.out3.bias.data.uniform_(-stdv, stdv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a1a2262-f820-4294-8086-3a48a5e5d686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"using {device}\")\n",
    "\n",
    "basemodel_resnet18.to(device)\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_EPOCH = 30\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(basemodel_resnet18.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64a36f92-aad5-4bbb-a920-29d50f0f058b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] name:[conv1.weight] shape:[(64, 3, 7, 7)].\n",
      "    val:[-0.036  0.019 -0.024  0.011  0.018]\n",
      "[1] name:[bn1.weight] shape:[(64,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[2] name:[bn1.bias] shape:[(64,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[3] name:[layer1.0.conv1.weight] shape:[(64, 64, 3, 3)].\n",
      "    val:[ 0.006  0.04  -0.02   0.003  0.014]\n",
      "[4] name:[layer1.0.bn1.weight] shape:[(64,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[5] name:[layer1.0.bn1.bias] shape:[(64,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[6] name:[layer1.0.conv2.weight] shape:[(64, 64, 3, 3)].\n",
      "    val:[ 0.097  0.086  0.035  0.029 -0.021]\n",
      "[7] name:[layer1.0.bn2.weight] shape:[(64,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[8] name:[layer1.0.bn2.bias] shape:[(64,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[9] name:[layer1.1.conv1.weight] shape:[(64, 64, 3, 3)].\n",
      "    val:[-0.069 -0.038 -0.112 -0.014  0.071]\n",
      "[10] name:[layer1.1.bn1.weight] shape:[(64,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[11] name:[layer1.1.bn1.bias] shape:[(64,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[12] name:[layer1.1.conv2.weight] shape:[(64, 64, 3, 3)].\n",
      "    val:[-0.007 -0.041 -0.133  0.16   0.031]\n",
      "[13] name:[layer1.1.bn2.weight] shape:[(64,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[14] name:[layer1.1.bn2.bias] shape:[(64,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[15] name:[layer2.0.conv1.weight] shape:[(128, 64, 3, 3)].\n",
      "    val:[ 0.064  0.016 -0.014  0.001  0.042]\n",
      "[16] name:[layer2.0.bn1.weight] shape:[(128,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[17] name:[layer2.0.bn1.bias] shape:[(128,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[18] name:[layer2.0.conv2.weight] shape:[(128, 128, 3, 3)].\n",
      "    val:[ 0.022 -0.021  0.008 -0.098 -0.012]\n",
      "[19] name:[layer2.0.bn2.weight] shape:[(128,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[20] name:[layer2.0.bn2.bias] shape:[(128,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[21] name:[layer2.0.downsample.0.weight] shape:[(128, 64, 1, 1)].\n",
      "    val:[ 0.053  0.05  -0.041 -0.075 -0.017]\n",
      "[22] name:[layer2.0.downsample.1.weight] shape:[(128,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[23] name:[layer2.0.downsample.1.bias] shape:[(128,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[24] name:[layer2.1.conv1.weight] shape:[(128, 128, 3, 3)].\n",
      "    val:[-0.039 -0.04  -0.061 -0.072  0.142]\n",
      "[25] name:[layer2.1.bn1.weight] shape:[(128,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[26] name:[layer2.1.bn1.bias] shape:[(128,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[27] name:[layer2.1.conv2.weight] shape:[(128, 128, 3, 3)].\n",
      "    val:[-0.057 -0.046 -0.001  0.083  0.104]\n",
      "[28] name:[layer2.1.bn2.weight] shape:[(128,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[29] name:[layer2.1.bn2.bias] shape:[(128,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[30] name:[layer3.0.conv1.weight] shape:[(256, 128, 3, 3)].\n",
      "    val:[ 0.003 -0.053 -0.02   0.001  0.022]\n",
      "[31] name:[layer3.0.bn1.weight] shape:[(256,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[32] name:[layer3.0.bn1.bias] shape:[(256,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[33] name:[layer3.0.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[-0.005  0.01   0.006  0.016  0.009]\n",
      "[34] name:[layer3.0.bn2.weight] shape:[(256,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[35] name:[layer3.0.bn2.bias] shape:[(256,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[36] name:[layer3.0.downsample.0.weight] shape:[(256, 128, 1, 1)].\n",
      "    val:[-0.045  0.032  0.168  0.008 -0.143]\n",
      "[37] name:[layer3.0.downsample.1.weight] shape:[(256,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[38] name:[layer3.0.downsample.1.bias] shape:[(256,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[39] name:[layer3.1.conv1.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[-0.044 -0.056  0.028  0.025  0.057]\n",
      "[40] name:[layer3.1.bn1.weight] shape:[(256,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[41] name:[layer3.1.bn1.bias] shape:[(256,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[42] name:[layer3.1.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[ 0.01  -0.041 -0.047 -0.01  -0.011]\n",
      "[43] name:[layer3.1.bn2.weight] shape:[(256,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[44] name:[layer3.1.bn2.bias] shape:[(256,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[45] name:[layer4.0.conv1.weight] shape:[(512, 256, 3, 3)].\n",
      "    val:[-0.016  0.027 -0.033 -0.019 -0.017]\n",
      "[46] name:[layer4.0.bn1.weight] shape:[(512,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[47] name:[layer4.0.bn1.bias] shape:[(512,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[48] name:[layer4.0.conv2.weight] shape:[(512, 512, 3, 3)].\n",
      "    val:[ 0.001  0.005 -0.021  0.003  0.016]\n",
      "[49] name:[layer4.0.bn2.weight] shape:[(512,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[50] name:[layer4.0.bn2.bias] shape:[(512,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[51] name:[layer4.0.downsample.0.weight] shape:[(512, 256, 1, 1)].\n",
      "    val:[ 0.023 -0.07  -0.004  0.061 -0.035]\n",
      "[52] name:[layer4.0.downsample.1.weight] shape:[(512,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[53] name:[layer4.0.downsample.1.bias] shape:[(512,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[54] name:[layer4.1.conv1.weight] shape:[(512, 512, 3, 3)].\n",
      "    val:[ 0.018 -0.016  0.03   0.007 -0.009]\n",
      "[55] name:[layer4.1.bn1.weight] shape:[(512,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[56] name:[layer4.1.bn1.bias] shape:[(512,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[57] name:[layer4.1.conv2.weight] shape:[(512, 512, 3, 3)].\n",
      "    val:[-0.008 -0.017  0.022  0.004 -0.006]\n",
      "[58] name:[layer4.1.bn2.weight] shape:[(512,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[59] name:[layer4.1.bn2.bias] shape:[(512,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[60] name:[fc.weight] shape:[(1000, 512)].\n",
      "    val:[-0.009 -0.038  0.04  -0.019  0.02 ]\n",
      "[61] name:[fc.bias] shape:[(1000,)].\n",
      "    val:[-0.031 -0.007 -0.006  0.035  0.025]\n",
      "[62] name:[out1.weight] shape:[(2, 512)].\n",
      "    val:[ 0.087 -0.016 -0.025 -0.046  0.014]\n",
      "[63] name:[out1.bias] shape:[(2,)].\n",
      "    val:[-0.019  0.039]\n",
      "[64] name:[out2.weight] shape:[(3, 512)].\n",
      "    val:[ 0.009  0.042  0.029 -0.086 -0.104]\n",
      "[65] name:[out2.bias] shape:[(3,)].\n",
      "    val:[-0.04  -0.025  0.003]\n",
      "[66] name:[out3.weight] shape:[(3, 512)].\n",
      "    val:[-0.064  0.058 -0.047  0.08   0.063]\n",
      "[67] name:[out3.bias] shape:[(3,)].\n",
      "    val:[-0.044 -0.034 -0.031]\n",
      "Total number of parameters:[11,693,616].\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "n_param = 0\n",
    "for p_idx, (param_name, param) in enumerate(basemodel_resnet18.named_parameters()):\n",
    "    if param.requires_grad:\n",
    "        param_numpy = param.detach().cpu().numpy()\n",
    "        n_param += len(param_numpy.reshape(-1))\n",
    "        print (\"[%d] name:[%s] shape:[%s].\"%(p_idx,param_name,param_numpy.shape))\n",
    "        print (\"    val:%s\"%(param_numpy.reshape(-1)[:5]))\n",
    "print (\"Total number of parameters:[%s].\"%(format(n_param,',d')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c067d0e-38c9-4ab8-baba-88cf8dcb8c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12316/1203383015.py:17: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  p_gen, p_age, p_msk = F.softmax(p_gen), F.softmax(p_age), F.softmax(p_msk)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> train_loss : 0.11551938951015472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12316/1203383015.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  p_gen, p_age, p_msk = F.softmax(p_gen), F.softmax(p_age), F.softmax(p_msk)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> valid_loss : 0.011530001647770405\n",
      "valid acc : 0.7108843537414966\n",
      "epoch[1/30] completed\n",
      "> train_loss : 0.11112791299819946\n",
      "> valid_loss : 0.00984954833984375\n",
      "valid acc : 0.8248299319727891\n",
      "epoch[2/30] completed\n",
      "> train_loss : 0.10257168859243393\n",
      "> valid_loss : 0.008521397598087788\n",
      "valid acc : 0.9132653061224489\n",
      "epoch[3/30] completed\n",
      "> train_loss : 0.09676290303468704\n",
      "> valid_loss : 0.008162219077348709\n",
      "valid acc : 0.9421768707482994\n",
      "epoch[4/30] completed\n",
      "> train_loss : 0.10086077451705933\n",
      "> valid_loss : 0.008644751273095608\n",
      "valid acc : 0.9030612244897959\n",
      "epoch[5/30] completed\n",
      "> train_loss : 0.1067771315574646\n",
      "> valid_loss : 0.008891502395272255\n",
      "valid acc : 0.891156462585034\n",
      "epoch[6/30] completed\n",
      "> train_loss : 0.09402496367692947\n",
      "> valid_loss : 0.009136253036558628\n",
      "valid acc : 0.8690476190476191\n",
      "epoch[7/30] completed\n",
      "> train_loss : 0.09693405032157898\n",
      "> valid_loss : 0.008172174915671349\n",
      "valid acc : 0.9370748299319728\n",
      "epoch[8/30] completed\n",
      "> train_loss : 0.09279365837574005\n",
      "> valid_loss : 0.007982751354575157\n",
      "valid acc : 0.9523809523809523\n",
      "epoch[9/30] completed\n",
      "> train_loss : 0.09882351756095886\n",
      "> valid_loss : 0.00839958619326353\n",
      "valid acc : 0.923469387755102\n",
      "epoch[10/30] completed\n",
      "> train_loss : 0.09831530600786209\n",
      "> valid_loss : 0.00825384259223938\n",
      "valid acc : 0.9319727891156463\n",
      "epoch[11/30] completed\n",
      "> train_loss : 0.092747762799263\n",
      "> valid_loss : 0.008331581950187683\n",
      "valid acc : 0.9319727891156463\n",
      "epoch[12/30] completed\n",
      "> train_loss : 0.09103038161993027\n",
      "> valid_loss : 0.010670294053852558\n",
      "valid acc : 0.7653061224489796\n",
      "epoch[13/30] completed\n",
      "> train_loss : 0.09782727062702179\n",
      "> valid_loss : 0.008246143348515034\n",
      "valid acc : 0.9302721088435374\n",
      "epoch[14/30] completed\n",
      "> train_loss : 0.10048907995223999\n",
      "> valid_loss : 0.00797653291374445\n",
      "valid acc : 0.9489795918367347\n",
      "epoch[15/30] completed\n",
      "> train_loss : 0.09449605643749237\n",
      "> valid_loss : 0.009634862653911114\n",
      "valid acc : 0.8350340136054422\n",
      "epoch[16/30] completed\n",
      "> train_loss : 0.08937862515449524\n",
      "> valid_loss : 0.008314126171171665\n",
      "valid acc : 0.9285714285714286\n",
      "epoch[17/30] completed\n",
      "> train_loss : 0.09695769846439362\n",
      "> valid_loss : 0.008373181335628033\n",
      "valid acc : 0.923469387755102\n",
      "epoch[18/30] completed\n",
      "> train_loss : 0.09652908891439438\n",
      "> valid_loss : 0.007996969856321812\n",
      "valid acc : 0.9506802721088435\n",
      "epoch[19/30] completed\n",
      "> train_loss : 0.093199223279953\n",
      "> valid_loss : 0.00791984610259533\n",
      "valid acc : 0.9540816326530612\n",
      "epoch[20/30] completed\n",
      "> train_loss : 0.09317922592163086\n",
      "> valid_loss : 0.007894501090049744\n",
      "valid acc : 0.95578231292517\n",
      "epoch[21/30] completed\n",
      "> train_loss : 0.10026509314775467\n",
      "> valid_loss : 0.008390014991164207\n",
      "valid acc : 0.9251700680272109\n",
      "epoch[22/30] completed\n",
      "> train_loss : 0.09618569910526276\n",
      "> valid_loss : 0.00819576159119606\n",
      "valid acc : 0.9387755102040817\n",
      "epoch[23/30] completed\n",
      "> train_loss : 0.09058767557144165\n",
      "> valid_loss : 0.008146344684064388\n",
      "valid acc : 0.9387755102040817\n",
      "epoch[24/30] completed\n",
      "> train_loss : 0.08862454444169998\n",
      "> valid_loss : 0.009102145209908485\n",
      "valid acc : 0.8758503401360545\n",
      "epoch[25/30] completed\n",
      "> train_loss : 0.10403764247894287\n",
      "> valid_loss : 0.008191102184355259\n",
      "valid acc : 0.935374149659864\n",
      "epoch[26/30] completed\n",
      "> train_loss : 0.09264703094959259\n",
      "> valid_loss : 0.008184020407497883\n",
      "valid acc : 0.9387755102040817\n",
      "epoch[27/30] completed\n",
      "> train_loss : 0.09731411933898926\n",
      "> valid_loss : 0.007899339310824871\n",
      "valid acc : 0.9574829931972789\n",
      "epoch[28/30] completed\n",
      "> train_loss : 0.0925481989979744\n",
      "> valid_loss : 0.008345928974449635\n",
      "valid acc : 0.9268707482993197\n",
      "epoch[29/30] completed\n",
      "> train_loss : 0.0932106152176857\n",
      "> valid_loss : 0.008026858791708946\n",
      "valid acc : 0.9506802721088435\n",
      "epoch[30/30] completed\n"
     ]
    }
   ],
   "source": [
    "# print되는 값들은 정확하지 않을 수도 있습니다 (진행상황 보려고 대강 확인안하고 만들었어요)\n",
    "\n",
    "total_sum = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    loss_value = 0\n",
    "    basemodel_resnet18.train()\n",
    "\n",
    "    for inputs, labels in train_dataloader_mask:\n",
    "        inputs = inputs.to(device)\n",
    "        gen, age, msk = labels\n",
    "        gen = gen.to(device)\n",
    "        age = age.to(device)\n",
    "        msk = msk.to(device)\n",
    "        \n",
    "        p_gen, p_age, p_msk = basemodel_resnet18(inputs)\n",
    "        p_gen, p_age, p_msk = F.softmax(p_gen), F.softmax(p_age), F.softmax(p_msk)\n",
    "        loss = criterion(p_gen, gen) + criterion(p_age, age) + criterion(p_msk, msk)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss = loss.item() / len(inputs)\n",
    "    print('> train_loss : {}'.format(train_loss))     \n",
    "    \n",
    "    \n",
    "    basemodel_resnet18.eval()\n",
    "    for idx, (inputs, labels) in enumerate(val_dataloader_mask):\n",
    "        inputs = inputs.to(device)\n",
    "        gen, age, msk = labels\n",
    "        gen = gen.to(device)\n",
    "        age = age.to(device)\n",
    "        msk = msk.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            p_gen, p_age, p_msk = basemodel_resnet18(inputs)\n",
    "            p_gen, p_age, p_msk = F.softmax(p_gen), F.softmax(p_age), F.softmax(p_msk)\n",
    "        loss = criterion(p_gen, gen) + criterion(p_age, age) + criterion(p_msk, msk)\n",
    "        \n",
    "        acc_1 = (gen == torch.argmax(p_gen, -1)).sum().item()\n",
    "        acc_2 = (age == torch.argmax(p_age, -1)).sum().item()\n",
    "        acc_3 = (msk == torch.argmax(p_msk, -1)).sum().item()\n",
    "    \n",
    "    valid_loss = loss / len(inputs)\n",
    "    print('> valid_loss : {}'.format(valid_loss))    \n",
    "    print(f'valid acc : {(acc_1 + acc_2 + acc_3) / (3 * len(inputs))}')        \n",
    "        \n",
    "    print(f\"epoch[{epoch+1}/{NUM_EPOCH}] completed\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afda23d0-6ad9-408b-99c9-705ae5fd8347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageID</th>\n",
       "      <th>ans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cbc5c6e168e63498590db46022617123f1fe1268.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0e72482bf56b3581c081f7da2a6180b8792c7089.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b549040c49190cedc41327748aeb197c1670f14d.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4f9cb2a045c6d5b9e50ad3459ea7b791eb6e18bc.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>248428d9a4a5b6229a7081c32851b90cb8d38d0c.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        ImageID  ans\n",
       "0  cbc5c6e168e63498590db46022617123f1fe1268.jpg    0\n",
       "1  0e72482bf56b3581c081f7da2a6180b8792c7089.jpg    0\n",
       "2  b549040c49190cedc41327748aeb197c1670f14d.jpg    0\n",
       "3  4f9cb2a045c6d5b9e50ad3459ea7b791eb6e18bc.jpg    0\n",
       "4  248428d9a4a5b6229a7081c32851b90cb8d38d0c.jpg    0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "test_dir_path = '/opt/ml/input/data/eval/'\n",
    "test_image_path = '/opt/ml/input/data/eval/images/'\n",
    "\n",
    "submission = pd.read_csv(test_dir_path+'info.csv')\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "566c2df1-a083-4e27-a655-f2c73d15f762",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = [os.path.join(test_image_path, img_id) for img_id in submission.ImageID]\n",
    "test_image = pd.Series(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9c14322-45a8-40a5-aded-f335204484b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test_Dataset(Dataset):\n",
    "    def __init__(self, midcrop=True, transform=None):\n",
    "        self.midcrop = midcrop\n",
    "        self.data = test_image\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(test_image)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.cvtColor(cv2.imread(self.data[idx]), cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.midcrop:\n",
    "            img = img[85:435, 17:367]\n",
    "            \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "138db932-cc09-4f85-89d8-e7485659f7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test inference is done!\n"
     ]
    }
   ],
   "source": [
    "dataset = Test_Dataset(transform = transforms.Compose([\n",
    "                            transforms.ToTensor()\n",
    "                        ]))\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "# 모델을 정의합니다. (학습한 모델이 있다면 torch.load로 모델을 불러주세요!)\n",
    "device = torch.device('cuda')\n",
    "model = basemodel_resnet18.to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "all_predictions = []\n",
    "for images in loader:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        p_gen, p_age, p_msk = model(images)\n",
    "        \n",
    "        ar_gen = p_gen.argmax(dim=-1)\n",
    "        ar_age = p_age.argmax(dim=-1)\n",
    "        ar_msk = p_msk.argmax(dim=-1)\n",
    "        \n",
    "        total = ar_gen * 3 + ar_age + ar_msk * 6\n",
    "        \n",
    "        all_predictions.extend(total.cpu().numpy())\n",
    "submission['ans'] = all_predictions\n",
    "\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(test_dir_path, 'submission.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ddc8bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9bf9802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72.11904761904762"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 결과비교 (자체, 현재 제출된 것 중 가장 결과가 좋은 것과 비교)\n",
    "# 기준결과의 accuracy가 a%, 비교결과가 b%인 경우\n",
    "#  -> 현재 결과의 accuracy 범위 : a+b-100(%) ~ a-b+100(%)\n",
    "standard = pd.read_csv('./log/standard_1028.csv')['ans']\n",
    "100 * sum(standard == submission['ans']) / len(standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "506b77d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     2879\n",
       "4     2583\n",
       "0     2023\n",
       "3     1549\n",
       "7      577\n",
       "13     565\n",
       "16     498\n",
       "10     487\n",
       "12     421\n",
       "6      405\n",
       "15     311\n",
       "9      302\n",
       "Name: ans, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission['ans'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6419fa20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "24179ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18765 17308 18852\n"
     ]
    }
   ],
   "source": [
    "# (자체) 각 class별로 정답개수 확인\n",
    "g_sum = 0\n",
    "a_sum = 0\n",
    "m_sum = 0\n",
    "device = torch.device('cuda')\n",
    "model = basemodel_resnet18.to(device)\n",
    "model.eval()\n",
    "\n",
    "for i in range(18900):\n",
    "    image, label = dataset_mask[i]\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "    \n",
    "    result = model(image)\n",
    "    if torch.argmax(result[0]) == label[0]:\n",
    "        g_sum += 1\n",
    "    if torch.argmax(result[1]) == label[1]:\n",
    "        a_sum += 1\n",
    "    if torch.argmax(result[2]) == label[2]:\n",
    "        m_sum += 1\n",
    "\n",
    "print(g_sum, a_sum , m_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36588613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제로 확인(in train set : idx = 0~18900)\n",
    "image, label = dataset_mask[3333]\n",
    "image = image.unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "device = torch.device('cuda')\n",
    "model = basemodel_resnet18.to(device)\n",
    "model.eval()\n",
    "\n",
    "result = model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04649d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- (Gender , Age, Mask) <--Pred  //  Real--> (Gender, Age, Mask) ----\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((tensor(0, device='cuda:0'),\n",
       "  tensor(0, device='cuda:0'),\n",
       "  tensor(0, device='cuda:0')),\n",
       " (0, 0, 0))"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('---- (Gender , Age, Mask) <--Pred  //  Real--> (Gender, Age, Mask) ----')\n",
    "(torch.argmax(result[0]), torch.argmax(result[1]), torch.argmax(result[2])), (label[0], label[1], label[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fa05a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터와 예측라벨, 실제라벨 확인\n",
    "print(torch.argmax(result[0]) * 3 + torch.argmax(result[1]) + torch.argmax(result[2]) * 6, label[0]*3 + label[1] + label[2]*6)\n",
    "transforms.ToPILImage()(image.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6bcc42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfdaa03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Level1_PJ",
   "language": "python",
   "name": "aistage1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "ee0b5d551368df5a794da4c38562111ecc0ffa964e80de1721a6d1bb425413ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
