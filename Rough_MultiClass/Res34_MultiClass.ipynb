{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b1d65a3-78de-4b01-a862-e6bd4c7ef135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15beaba-39cc-48da-8165-9f2abc16322a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('input/data')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b3beb99-f9c7-4026-989a-cbaa8de1f632",
   "metadata": {},
   "outputs": [],
   "source": [
    "!find . -regex \".*\\.\\_[a-zA-Z0-9._]+\" -delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa8d19de-5a8e-454f-a22b-129e94792395",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 12\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f69f93c-5675-433f-8a88-7bf8e411d51c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000002</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>52</td>\n",
       "      <td>000002_female_Asian_52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000004</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>54</td>\n",
       "      <td>000004_male_Asian_54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000005</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>58</td>\n",
       "      <td>000005_female_Asian_58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000006</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>59</td>\n",
       "      <td>000006_female_Asian_59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>006954</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006954_male_Asian_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2696</th>\n",
       "      <td>006955</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006955_male_Asian_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>006956</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006956_male_Asian_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2698</th>\n",
       "      <td>006957</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>20</td>\n",
       "      <td>006957_male_Asian_20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2699</th>\n",
       "      <td>006959</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006959_male_Asian_19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2700 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  gender   race  age                    path\n",
       "0     000001  female  Asian   45  000001_female_Asian_45\n",
       "1     000002  female  Asian   52  000002_female_Asian_52\n",
       "2     000004    male  Asian   54    000004_male_Asian_54\n",
       "3     000005  female  Asian   58  000005_female_Asian_58\n",
       "4     000006  female  Asian   59  000006_female_Asian_59\n",
       "...      ...     ...    ...  ...                     ...\n",
       "2695  006954    male  Asian   19    006954_male_Asian_19\n",
       "2696  006955    male  Asian   19    006955_male_Asian_19\n",
       "2697  006956    male  Asian   19    006956_male_Asian_19\n",
       "2698  006957    male  Asian   20    006957_male_Asian_20\n",
       "2699  006959    male  Asian   19    006959_male_Asian_19\n",
       "\n",
       "[2700 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir_path = '/opt/ml/input/data/train/'\n",
    "train_image_path = '/opt/ml/input/data/train/images/'\n",
    "\n",
    "dt_train = pd.read_csv(train_dir_path+'train.csv')\n",
    "dt_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "304c6a61-ff17-4777-8132-18def61c8102",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_image_path = []\n",
    "whole_target_label = []\n",
    "\n",
    "for path in dt_train['path']:\n",
    "    for file_name in [i for i in os.listdir(train_image_path+path) if '._' not in i]:\n",
    "        whole_image_path.append(train_image_path+path+'/'+file_name)\n",
    "        whole_target_label.append((path.split('_')[1], path.split('_')[3], file_name.split('.')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98de1eb4-55d4-41ba-8541-70fff9bb011e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_enc(x):\n",
    "    def gender(i):\n",
    "        if i == 'male':\n",
    "            return 0\n",
    "        elif i == 'female':\n",
    "            return 3\n",
    "    def age(j):\n",
    "        j = int(j)\n",
    "        if j < 30:\n",
    "            return 0\n",
    "        elif j >= 30 and j < 60:\n",
    "            return 1\n",
    "        elif j >= 60:\n",
    "            return 2\n",
    "    def mask(k):\n",
    "        if k == 'normal':\n",
    "            return 12\n",
    "        elif 'incorrect' in k:\n",
    "            return 6\n",
    "        else:\n",
    "            return 0\n",
    "    return gender(x[0]) + age(x[1]) + mask(x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af245588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc_genmsk(x):\n",
    "    # x 입력형식 : (성별, 나이, 마스크) 튜플\n",
    "    # 출력형식 : (나이, 성별-마스크 인코딩) 튜플\n",
    "    def gender(i):\n",
    "        if i == 'male':\n",
    "            return 0\n",
    "        elif i == 'female':\n",
    "            return 3\n",
    "    def mask(k):\n",
    "        if k == 'normal':\n",
    "            return 12\n",
    "        elif 'incorrect' in k:\n",
    "            return 6\n",
    "        else:\n",
    "            return 0\n",
    "    if int(x[1]) == 60:\n",
    "        return 70, (gender(x[0])+mask(x[2]))//3\n",
    "    return int(x[1]), (gender(x[0])+mask(x[2]))//3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0013779",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc_sepclass(x):\n",
    "    # x 입력형식 : (성별, 나이, 마스크) 튜플\n",
    "    # 출력형식 : (성별인코딩, 나이인코딩, 마스크인코딩)\n",
    "    def gender(i):\n",
    "        if i == 'male':\n",
    "            return 0\n",
    "        elif i == 'female':\n",
    "            return 1\n",
    "    def age(j):\n",
    "        j = int(j)\n",
    "        if j < 30:\n",
    "            return 0\n",
    "        elif j >= 30 and j < 60:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2\n",
    "    def mask(k):\n",
    "        if k == 'normal':\n",
    "            return 2\n",
    "        elif 'incorrect' in k:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    return gender(x[0]), age(x[1]), mask(x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "694474c7-7175-40e6-b071-e50218629f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_data = pd.Series(whole_image_path)\n",
    "sr_label = pd.Series(whole_target_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2cd7b29-5f03-444e-b781-902ca74f2085",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_Mask(Dataset):\n",
    "    def __init__(self, encoding=True, midcrop=True, transform=None):\n",
    "        self.encoding = encoding\n",
    "        self.midcrop = midcrop\n",
    "        self.data = sr_data\n",
    "        self.label = sr_label\n",
    "        self.transform = transform\n",
    "        \n",
    "        if encoding:\n",
    "            self.label = self.label.apply(enc_sepclass)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(sr_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X = cv2.cvtColor(cv2.imread(self.data[idx]), cv2.COLOR_BGR2RGB)\n",
    "        y = self.label[idx]\n",
    "        \n",
    "        if self.midcrop:\n",
    "            X = X[98:422, 30:354]\n",
    "        \n",
    "        if self.transform:\n",
    "            return self.transform(X), y\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2a22aff-e634-4f15-877c-752d450d01da",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mask = Dataset_Mask(transform = transforms.Compose([\n",
    "                                transforms.ToTensor()\n",
    "                            ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5788c44b-07ac-4820-a22d-b6ffa3fc81f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(dataset_mask) * 0.8)\n",
    "val_size = int(len(dataset_mask) * 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e285cb6-ee37-4a0c-a804-7506dcfebc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data size : 15120\n",
      "validation data size : 3780\n"
     ]
    }
   ],
   "source": [
    "mask_train_set, mask_val_set = torch.utils.data.random_split(dataset_mask, [train_size, val_size])\n",
    "print(f'training data size : {len(mask_train_set)}')\n",
    "print(f'validation data size : {len(mask_val_set)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37b1ced0-1e34-4d50-b2b2-dcdff8fd33f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_dataloader_mask = DataLoader(dataset = mask_train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_dataloader_mask = DataLoader(dataset = mask_val_set, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7dc2e58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class R34_RegCls(torchvision.models.ResNet):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.out1 = nn.Linear(512, 2, bias=True)\n",
    "        self.out2 = nn.Linear(512, 3, bias=True)\n",
    "        self.out3 = nn.Linear(512, 3, bias=True)\n",
    "        \n",
    "    def _forward_impl(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        # out1 : gender class\n",
    "        # out2 : age class\n",
    "        # out3 : mask class\n",
    "        out1 = self.out1(x)\n",
    "        out2 = self.out2(x)\n",
    "        out3 = self.out3(x)\n",
    "        \n",
    "        return out1, out2, out3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af71a4ef-a137-47b0-af94-eebedaa36014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "필요 입력 채널 개수 3\n",
      "네트워크 출력 채널 개수_gen/msk 2\n",
      "네트워크 출력 채널 개수_age 3\n",
      "R34_RegCls(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      "  (out1): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (out2): Linear(in_features=512, out_features=3, bias=True)\n",
      "  (out3): Linear(in_features=512, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "block = torchvision.models.resnet.BasicBlock\n",
    "layers = [3, 4, 6, 3]\n",
    "\n",
    "basemodel_resnet34 = R34_RegCls(block = block, layers = layers)\n",
    "print('필요 입력 채널 개수', basemodel_resnet34.conv1.weight.shape[1])\n",
    "print('네트워크 출력 채널 개수_gen/msk', basemodel_resnet34.out1.weight.shape[0])\n",
    "print('네트워크 출력 채널 개수_age', basemodel_resnet34.out2.weight.shape[0])\n",
    "print(basemodel_resnet34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11357fc4-548b-452e-bbd8-e60ef6261308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0264, -0.0118,  0.0335])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "nn.init.xavier_uniform_(basemodel_resnet34.out1.weight)\n",
    "stdv = 1. / math.sqrt(basemodel_resnet34.out1.weight.size(1))\n",
    "basemodel_resnet34.out1.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "nn.init.xavier_uniform_(basemodel_resnet34.out2.weight)\n",
    "stdv = 1. / math.sqrt(basemodel_resnet34.out2.weight.size(1))\n",
    "basemodel_resnet34.out2.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "nn.init.xavier_uniform_(basemodel_resnet34.out3.weight)\n",
    "stdv = 1. / math.sqrt(basemodel_resnet34.out3.weight.size(1))\n",
    "basemodel_resnet34.out3.bias.data.uniform_(-stdv, stdv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a1a2262-f820-4294-8086-3a48a5e5d686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"using {device}\")\n",
    "\n",
    "basemodel_resnet34.to(device)\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_EPOCH = 30\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(basemodel_resnet34.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64a36f92-aad5-4bbb-a920-29d50f0f058b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] name:[conv1.weight] shape:[(64, 3, 7, 7)].\n",
      "    val:[ 0.004  0.025 -0.02  -0.018 -0.03 ]\n",
      "[1] name:[bn1.weight] shape:[(64,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[2] name:[bn1.bias] shape:[(64,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[3] name:[layer1.0.conv1.weight] shape:[(64, 64, 3, 3)].\n",
      "    val:[ 0.044 -0.094  0.078 -0.065 -0.008]\n",
      "[4] name:[layer1.0.bn1.weight] shape:[(64,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[5] name:[layer1.0.bn1.bias] shape:[(64,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[6] name:[layer1.0.conv2.weight] shape:[(64, 64, 3, 3)].\n",
      "    val:[-0.023  0.043 -0.038 -0.07  -0.1  ]\n",
      "[7] name:[layer1.0.bn2.weight] shape:[(64,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[8] name:[layer1.0.bn2.bias] shape:[(64,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[9] name:[layer1.1.conv1.weight] shape:[(64, 64, 3, 3)].\n",
      "    val:[ 0.008 -0.065  0.001  0.023 -0.004]\n",
      "[10] name:[layer1.1.bn1.weight] shape:[(64,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[11] name:[layer1.1.bn1.bias] shape:[(64,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[12] name:[layer1.1.conv2.weight] shape:[(64, 64, 3, 3)].\n",
      "    val:[ 0.002  0.018 -0.04   0.061  0.018]\n",
      "[13] name:[layer1.1.bn2.weight] shape:[(64,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[14] name:[layer1.1.bn2.bias] shape:[(64,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[15] name:[layer1.2.conv1.weight] shape:[(64, 64, 3, 3)].\n",
      "    val:[-0.001  0.039 -0.022  0.076 -0.021]\n",
      "[16] name:[layer1.2.bn1.weight] shape:[(64,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[17] name:[layer1.2.bn1.bias] shape:[(64,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[18] name:[layer1.2.conv2.weight] shape:[(64, 64, 3, 3)].\n",
      "    val:[-0.031 -0.023  0.046 -0.033  0.009]\n",
      "[19] name:[layer1.2.bn2.weight] shape:[(64,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[20] name:[layer1.2.bn2.bias] shape:[(64,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[21] name:[layer2.0.conv1.weight] shape:[(128, 64, 3, 3)].\n",
      "    val:[ 0.023  0.011  0.005 -0.058 -0.042]\n",
      "[22] name:[layer2.0.bn1.weight] shape:[(128,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[23] name:[layer2.0.bn1.bias] shape:[(128,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[24] name:[layer2.0.conv2.weight] shape:[(128, 128, 3, 3)].\n",
      "    val:[-0.007  0.03  -0.025  0.03  -0.007]\n",
      "[25] name:[layer2.0.bn2.weight] shape:[(128,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[26] name:[layer2.0.bn2.bias] shape:[(128,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[27] name:[layer2.0.downsample.0.weight] shape:[(128, 64, 1, 1)].\n",
      "    val:[-0.108 -0.013  0.018  0.022  0.12 ]\n",
      "[28] name:[layer2.0.downsample.1.weight] shape:[(128,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[29] name:[layer2.0.downsample.1.bias] shape:[(128,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[30] name:[layer2.1.conv1.weight] shape:[(128, 128, 3, 3)].\n",
      "    val:[ 0.059 -0.072  0.     0.053  0.042]\n",
      "[31] name:[layer2.1.bn1.weight] shape:[(128,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[32] name:[layer2.1.bn1.bias] shape:[(128,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[33] name:[layer2.1.conv2.weight] shape:[(128, 128, 3, 3)].\n",
      "    val:[-0.078 -0.041 -0.005  0.017  0.024]\n",
      "[34] name:[layer2.1.bn2.weight] shape:[(128,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[35] name:[layer2.1.bn2.bias] shape:[(128,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[36] name:[layer2.2.conv1.weight] shape:[(128, 128, 3, 3)].\n",
      "    val:[ 0.036  0.059  0.043 -0.001  0.11 ]\n",
      "[37] name:[layer2.2.bn1.weight] shape:[(128,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[38] name:[layer2.2.bn1.bias] shape:[(128,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[39] name:[layer2.2.conv2.weight] shape:[(128, 128, 3, 3)].\n",
      "    val:[-0.047 -0.013 -0.047  0.066 -0.052]\n",
      "[40] name:[layer2.2.bn2.weight] shape:[(128,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[41] name:[layer2.2.bn2.bias] shape:[(128,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[42] name:[layer2.3.conv1.weight] shape:[(128, 128, 3, 3)].\n",
      "    val:[-0.006  0.002  0.067  0.024 -0.005]\n",
      "[43] name:[layer2.3.bn1.weight] shape:[(128,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[44] name:[layer2.3.bn1.bias] shape:[(128,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[45] name:[layer2.3.conv2.weight] shape:[(128, 128, 3, 3)].\n",
      "    val:[-0.012  0.015 -0.057  0.024  0.042]\n",
      "[46] name:[layer2.3.bn2.weight] shape:[(128,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[47] name:[layer2.3.bn2.bias] shape:[(128,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[48] name:[layer3.0.conv1.weight] shape:[(256, 128, 3, 3)].\n",
      "    val:[-0.003  0.01  -0.027 -0.004  0.019]\n",
      "[49] name:[layer3.0.bn1.weight] shape:[(256,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[50] name:[layer3.0.bn1.bias] shape:[(256,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[51] name:[layer3.0.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[-0.036  0.001  0.018  0.025 -0.038]\n",
      "[52] name:[layer3.0.bn2.weight] shape:[(256,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[53] name:[layer3.0.bn2.bias] shape:[(256,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[54] name:[layer3.0.downsample.0.weight] shape:[(256, 128, 1, 1)].\n",
      "    val:[ 0.063 -0.062 -0.091 -0.023  0.078]\n",
      "[55] name:[layer3.0.downsample.1.weight] shape:[(256,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[56] name:[layer3.0.downsample.1.bias] shape:[(256,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[57] name:[layer3.1.conv1.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[ 0.04  -0.028 -0.049  0.014  0.03 ]\n",
      "[58] name:[layer3.1.bn1.weight] shape:[(256,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[59] name:[layer3.1.bn1.bias] shape:[(256,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[60] name:[layer3.1.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[-0.004  0.005 -0.043  0.026 -0.061]\n",
      "[61] name:[layer3.1.bn2.weight] shape:[(256,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[62] name:[layer3.1.bn2.bias] shape:[(256,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[63] name:[layer3.2.conv1.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[ 0.03   0.029  0.01  -0.028 -0.038]\n",
      "[64] name:[layer3.2.bn1.weight] shape:[(256,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[65] name:[layer3.2.bn1.bias] shape:[(256,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[66] name:[layer3.2.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[-0.014 -0.008  0.045 -0.006 -0.016]\n",
      "[67] name:[layer3.2.bn2.weight] shape:[(256,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[68] name:[layer3.2.bn2.bias] shape:[(256,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[69] name:[layer3.3.conv1.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[ 0.007  0.012 -0.023 -0.006  0.042]\n",
      "[70] name:[layer3.3.bn1.weight] shape:[(256,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[71] name:[layer3.3.bn1.bias] shape:[(256,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[72] name:[layer3.3.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[ 0.042 -0.075  0.062  0.022 -0.036]\n",
      "[73] name:[layer3.3.bn2.weight] shape:[(256,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[74] name:[layer3.3.bn2.bias] shape:[(256,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[75] name:[layer3.4.conv1.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[ 0.017 -0.029 -0.042 -0.031  0.026]\n",
      "[76] name:[layer3.4.bn1.weight] shape:[(256,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[77] name:[layer3.4.bn1.bias] shape:[(256,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[78] name:[layer3.4.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[-0.012  0.016 -0.    -0.001  0.003]\n",
      "[79] name:[layer3.4.bn2.weight] shape:[(256,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[80] name:[layer3.4.bn2.bias] shape:[(256,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[81] name:[layer3.5.conv1.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[ 0.031 -0.014  0.001  0.02   0.037]\n",
      "[82] name:[layer3.5.bn1.weight] shape:[(256,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[83] name:[layer3.5.bn1.bias] shape:[(256,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[84] name:[layer3.5.conv2.weight] shape:[(256, 256, 3, 3)].\n",
      "    val:[-0.005  0.006  0.044  0.011 -0.007]\n",
      "[85] name:[layer3.5.bn2.weight] shape:[(256,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[86] name:[layer3.5.bn2.bias] shape:[(256,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[87] name:[layer4.0.conv1.weight] shape:[(512, 256, 3, 3)].\n",
      "    val:[ 0.033 -0.017  0.003  0.011  0.023]\n",
      "[88] name:[layer4.0.bn1.weight] shape:[(512,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[89] name:[layer4.0.bn1.bias] shape:[(512,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[90] name:[layer4.0.conv2.weight] shape:[(512, 512, 3, 3)].\n",
      "    val:[ 0.005  0.    -0.001 -0.004  0.004]\n",
      "[91] name:[layer4.0.bn2.weight] shape:[(512,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[92] name:[layer4.0.bn2.bias] shape:[(512,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[93] name:[layer4.0.downsample.0.weight] shape:[(512, 256, 1, 1)].\n",
      "    val:[-0.055 -0.028 -0.116 -0.006  0.063]\n",
      "[94] name:[layer4.0.downsample.1.weight] shape:[(512,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[95] name:[layer4.0.downsample.1.bias] shape:[(512,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[96] name:[layer4.1.conv1.weight] shape:[(512, 512, 3, 3)].\n",
      "    val:[-0.014  0.002  0.021 -0.027 -0.023]\n",
      "[97] name:[layer4.1.bn1.weight] shape:[(512,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[98] name:[layer4.1.bn1.bias] shape:[(512,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[99] name:[layer4.1.conv2.weight] shape:[(512, 512, 3, 3)].\n",
      "    val:[-0.006 -0.03  -0.014 -0.013 -0.002]\n",
      "[100] name:[layer4.1.bn2.weight] shape:[(512,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[101] name:[layer4.1.bn2.bias] shape:[(512,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[102] name:[layer4.2.conv1.weight] shape:[(512, 512, 3, 3)].\n",
      "    val:[ 0.007  0.009  0.029  0.009 -0.032]\n",
      "[103] name:[layer4.2.bn1.weight] shape:[(512,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[104] name:[layer4.2.bn1.bias] shape:[(512,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[105] name:[layer4.2.conv2.weight] shape:[(512, 512, 3, 3)].\n",
      "    val:[ 0.036 -0.01   0.029  0.023 -0.001]\n",
      "[106] name:[layer4.2.bn2.weight] shape:[(512,)].\n",
      "    val:[1. 1. 1. 1. 1.]\n",
      "[107] name:[layer4.2.bn2.bias] shape:[(512,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[108] name:[fc.weight] shape:[(1000, 512)].\n",
      "    val:[ 0.038  0.029  0.026 -0.038  0.009]\n",
      "[109] name:[fc.bias] shape:[(1000,)].\n",
      "    val:[-0.03   0.005  0.015 -0.037 -0.002]\n",
      "[110] name:[out1.weight] shape:[(2, 512)].\n",
      "    val:[ 0.048 -0.056 -0.011  0.063 -0.031]\n",
      "[111] name:[out1.bias] shape:[(2,)].\n",
      "    val:[ 0.031 -0.021]\n",
      "[112] name:[out2.weight] shape:[(3, 512)].\n",
      "    val:[-0.058  0.096 -0.097 -0.006  0.041]\n",
      "[113] name:[out2.bias] shape:[(3,)].\n",
      "    val:[-0.001  0.026 -0.022]\n",
      "[114] name:[out3.weight] shape:[(3, 512)].\n",
      "    val:[ 0.062 -0.048  0.06  -0.092  0.025]\n",
      "[115] name:[out3.bias] shape:[(3,)].\n",
      "    val:[-0.026 -0.012  0.033]\n",
      "Total number of parameters:[21,801,776].\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "n_param = 0\n",
    "for p_idx, (param_name, param) in enumerate(basemodel_resnet34.named_parameters()):\n",
    "    if param.requires_grad:\n",
    "        param_numpy = param.detach().cpu().numpy()\n",
    "        n_param += len(param_numpy.reshape(-1))\n",
    "        print (\"[%d] name:[%s] shape:[%s].\"%(p_idx,param_name,param_numpy.shape))\n",
    "        print (\"    val:%s\"%(param_numpy.reshape(-1)[:5]))\n",
    "print (\"Total number of parameters:[%s].\"%(format(n_param,',d')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c067d0e-38c9-4ab8-baba-88cf8dcb8c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10466/3501907160.py:17: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  p_gen, p_age, p_msk = F.softmax(p_gen), F.softmax(p_age), F.softmax(p_msk)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> train_loss : 0.11433611810207367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10466/3501907160.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  p_gen, p_age, p_msk = F.softmax(p_gen), F.softmax(p_age), F.softmax(p_msk)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> valid_loss : 0.010540408082306385\n",
      "valid acc : 0.7789115646258503\n",
      "epoch[1/30] completed\n",
      "> train_loss : 0.10401048511266708\n",
      "> valid_loss : 0.010556266643106937\n",
      "valid acc : 0.7806122448979592\n",
      "epoch[2/30] completed\n",
      "> train_loss : 0.11068312078714371\n",
      "> valid_loss : 0.008887164294719696\n",
      "valid acc : 0.8928571428571429\n",
      "epoch[3/30] completed\n",
      "> train_loss : 0.1003379374742508\n",
      "> valid_loss : 0.01029300782829523\n",
      "valid acc : 0.7959183673469388\n",
      "epoch[4/30] completed\n",
      "> train_loss : 0.08876042068004608\n",
      "> valid_loss : 0.00896081980317831\n",
      "valid acc : 0.8877551020408163\n",
      "epoch[5/30] completed\n",
      "> train_loss : 0.10144273936748505\n",
      "> valid_loss : 0.009222377091646194\n",
      "valid acc : 0.8673469387755102\n",
      "epoch[6/30] completed\n",
      "> train_loss : 0.10932314395904541\n",
      "> valid_loss : 0.008682835847139359\n",
      "valid acc : 0.9047619047619048\n",
      "epoch[7/30] completed\n",
      "> train_loss : 0.09647645056247711\n",
      "> valid_loss : 0.008630017749965191\n",
      "valid acc : 0.9013605442176871\n",
      "epoch[8/30] completed\n",
      "> train_loss : 0.10906515270471573\n",
      "> valid_loss : 0.008281538262963295\n",
      "valid acc : 0.9302721088435374\n",
      "epoch[9/30] completed\n",
      "> train_loss : 0.11707085371017456\n",
      "> valid_loss : 0.009869278408586979\n",
      "valid acc : 0.8299319727891157\n",
      "epoch[10/30] completed\n",
      "> train_loss : 0.10157978534698486\n",
      "> valid_loss : 0.008849252946674824\n",
      "valid acc : 0.8894557823129252\n",
      "epoch[11/30] completed\n",
      "> train_loss : 0.09329372644424438\n",
      "> valid_loss : 0.008368986658751965\n",
      "valid acc : 0.923469387755102\n",
      "epoch[12/30] completed\n",
      "> train_loss : 0.09244208037853241\n",
      "> valid_loss : 0.008191856555640697\n",
      "valid acc : 0.9387755102040817\n",
      "epoch[13/30] completed\n",
      "> train_loss : 0.10578352212905884\n",
      "> valid_loss : 0.008620219305157661\n",
      "valid acc : 0.9098639455782312\n",
      "epoch[14/30] completed\n",
      "> train_loss : 0.08863282203674316\n",
      "> valid_loss : 0.008021737448871136\n",
      "valid acc : 0.9472789115646258\n",
      "epoch[15/30] completed\n",
      "> train_loss : 0.0925593450665474\n",
      "> valid_loss : 0.007867841050028801\n",
      "valid acc : 0.95578231292517\n",
      "epoch[16/30] completed\n",
      "> train_loss : 0.09147484600543976\n",
      "> valid_loss : 0.00799868069589138\n",
      "valid acc : 0.9489795918367347\n",
      "epoch[17/30] completed\n",
      "> train_loss : 0.09883655607700348\n",
      "> valid_loss : 0.009417491033673286\n",
      "valid acc : 0.8520408163265306\n",
      "epoch[18/30] completed\n",
      "> train_loss : 0.08890804648399353\n",
      "> valid_loss : 0.00814039632678032\n",
      "valid acc : 0.9370748299319728\n",
      "epoch[19/30] completed\n",
      "> train_loss : 0.09547184407711029\n",
      "> valid_loss : 0.00834820419549942\n",
      "valid acc : 0.9200680272108843\n",
      "epoch[20/30] completed\n",
      "> train_loss : 0.09246863424777985\n",
      "> valid_loss : 0.008129911497235298\n",
      "valid acc : 0.9404761904761905\n",
      "epoch[21/30] completed\n",
      "> train_loss : 0.11556576192378998\n",
      "> valid_loss : 0.00830849539488554\n",
      "valid acc : 0.9302721088435374\n",
      "epoch[22/30] completed\n",
      "> train_loss : 0.09271303564310074\n",
      "> valid_loss : 0.00808575190603733\n",
      "valid acc : 0.9387755102040817\n",
      "epoch[23/30] completed\n",
      "> train_loss : 0.08851948380470276\n",
      "> valid_loss : 0.007907357066869736\n",
      "valid acc : 0.9540816326530612\n",
      "epoch[24/30] completed\n",
      "> train_loss : 0.10242341458797455\n",
      "> valid_loss : 0.007873774506151676\n",
      "valid acc : 0.95578231292517\n",
      "epoch[25/30] completed\n",
      "> train_loss : 0.10438024252653122\n",
      "> valid_loss : 0.007940459996461868\n",
      "valid acc : 0.9540816326530612\n",
      "epoch[26/30] completed\n",
      "> train_loss : 0.11028653383255005\n",
      "> valid_loss : 0.008150397799909115\n",
      "valid acc : 0.9438775510204082\n",
      "epoch[27/30] completed\n",
      "> train_loss : 0.10181999206542969\n",
      "> valid_loss : 0.008049221709370613\n",
      "valid acc : 0.9455782312925171\n",
      "epoch[28/30] completed\n",
      "> train_loss : 0.09669055044651031\n",
      "> valid_loss : 0.008920526131987572\n",
      "valid acc : 0.8877551020408163\n",
      "epoch[29/30] completed\n",
      "> train_loss : 0.0968829095363617\n",
      "> valid_loss : 0.007961098104715347\n",
      "valid acc : 0.9506802721088435\n",
      "epoch[30/30] completed\n"
     ]
    }
   ],
   "source": [
    "# print되는 값들은 정확하지 않을 수도 있습니다 (진행상황 보려고 대강 확인안하고 만들었어요)\n",
    "\n",
    "total_sum = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    loss_value = 0\n",
    "    basemodel_resnet34.train()\n",
    "\n",
    "    for inputs, labels in train_dataloader_mask:\n",
    "        inputs = inputs.to(device)\n",
    "        gen, age, msk = labels\n",
    "        gen = gen.to(device)\n",
    "        age = age.to(device)\n",
    "        msk = msk.to(device)\n",
    "        \n",
    "        p_gen, p_age, p_msk = basemodel_resnet34(inputs)\n",
    "        p_gen, p_age, p_msk = F.softmax(p_gen), F.softmax(p_age), F.softmax(p_msk)\n",
    "        loss = criterion(p_gen, gen) + criterion(p_age, age) + criterion(p_msk, msk)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss = loss.item() / len(inputs)\n",
    "    print('> train_loss : {}'.format(train_loss))     \n",
    "    \n",
    "    \n",
    "    basemodel_resnet34.eval()\n",
    "    for idx, (inputs, labels) in enumerate(val_dataloader_mask):\n",
    "        inputs = inputs.to(device)\n",
    "        gen, age, msk = labels\n",
    "        gen = gen.to(device)\n",
    "        age = age.to(device)\n",
    "        msk = msk.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            p_gen, p_age, p_msk = basemodel_resnet34(inputs)\n",
    "            p_gen, p_age, p_msk = F.softmax(p_gen), F.softmax(p_age), F.softmax(p_msk)\n",
    "        loss = criterion(p_gen, gen) + criterion(p_age, age) + criterion(p_msk, msk)\n",
    "        \n",
    "        acc_1 = (gen == torch.argmax(p_gen, -1)).sum().item()\n",
    "        acc_2 = (age == torch.argmax(p_age, -1)).sum().item()\n",
    "        acc_3 = (msk == torch.argmax(p_msk, -1)).sum().item()\n",
    "    \n",
    "    valid_loss = loss / len(inputs)\n",
    "    print('> valid_loss : {}'.format(valid_loss))    \n",
    "    print(f'valid acc : {(acc_1 + acc_2 + acc_3) / (3 * len(inputs))}')        \n",
    "        \n",
    "    print(f\"epoch[{epoch+1}/{NUM_EPOCH}] completed\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afda23d0-6ad9-408b-99c9-705ae5fd8347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageID</th>\n",
       "      <th>ans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cbc5c6e168e63498590db46022617123f1fe1268.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0e72482bf56b3581c081f7da2a6180b8792c7089.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b549040c49190cedc41327748aeb197c1670f14d.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4f9cb2a045c6d5b9e50ad3459ea7b791eb6e18bc.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>248428d9a4a5b6229a7081c32851b90cb8d38d0c.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        ImageID  ans\n",
       "0  cbc5c6e168e63498590db46022617123f1fe1268.jpg    0\n",
       "1  0e72482bf56b3581c081f7da2a6180b8792c7089.jpg    0\n",
       "2  b549040c49190cedc41327748aeb197c1670f14d.jpg    0\n",
       "3  4f9cb2a045c6d5b9e50ad3459ea7b791eb6e18bc.jpg    0\n",
       "4  248428d9a4a5b6229a7081c32851b90cb8d38d0c.jpg    0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "test_dir_path = '/opt/ml/input/data/eval/'\n",
    "test_image_path = '/opt/ml/input/data/eval/images/'\n",
    "\n",
    "submission = pd.read_csv(test_dir_path+'info.csv')\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "566c2df1-a083-4e27-a655-f2c73d15f762",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = [os.path.join(test_image_path, img_id) for img_id in submission.ImageID]\n",
    "test_image = pd.Series(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9c14322-45a8-40a5-aded-f335204484b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test_Dataset(Dataset):\n",
    "    def __init__(self, midcrop=True, transform=None):\n",
    "        self.midcrop = midcrop\n",
    "        self.data = test_image\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(test_image)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.cvtColor(cv2.imread(self.data[idx]), cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.midcrop:\n",
    "            img = img[98:422, 30:354]\n",
    "            \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "138db932-cc09-4f85-89d8-e7485659f7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test inference is done!\n"
     ]
    }
   ],
   "source": [
    "dataset = Test_Dataset(transform = transforms.Compose([\n",
    "                            transforms.ToTensor()\n",
    "                        ]))\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "# 모델을 정의합니다. (학습한 모델이 있다면 torch.load로 모델을 불러주세요!)\n",
    "device = torch.device('cuda')\n",
    "model = basemodel_resnet34.to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "all_predictions = []\n",
    "for images in loader:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        p_gen, p_age, p_msk = model(images)\n",
    "        \n",
    "        ar_gen = p_gen.argmax(dim=-1)\n",
    "        ar_age = p_age.argmax(dim=-1)\n",
    "        ar_msk = p_msk.argmax(dim=-1)\n",
    "        \n",
    "        total = ar_gen * 3 + ar_age + ar_msk * 6\n",
    "        \n",
    "        all_predictions.extend(total.cpu().numpy())\n",
    "submission['ans'] = all_predictions\n",
    "\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(test_dir_path, 'submission.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ddc8bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9bf9802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74.22222222222223"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 결과비교 (자체, 현재 제출된 것 중 가장 결과가 좋은 것과 비교)\n",
    "# 기준결과의 accuracy가 a%, 비교결과가 b%인 경우\n",
    "#  -> 현재 결과의 accuracy 범위 : a+b-100(%) ~ a-b+100(%)\n",
    "standard = pd.read_csv('./log/standard_1028.csv')['ans']\n",
    "100 * sum(standard == submission['ans']) / len(standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506b77d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6419fa20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "24179ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18780 17423 18869\n"
     ]
    }
   ],
   "source": [
    "# (자체) 각 class별로 정답개수 확인\n",
    "g_sum = 0\n",
    "a_sum = 0\n",
    "m_sum = 0\n",
    "device = torch.device('cuda')\n",
    "model = basemodel_resnet34.to(device)\n",
    "model.eval()\n",
    "\n",
    "for i in range(18900):\n",
    "    image, label = dataset_mask[i]\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "    \n",
    "    result = model(image)\n",
    "    if torch.argmax(result[0]) == label[0]:\n",
    "        g_sum += 1\n",
    "    if torch.argmax(result[1]) == label[1]:\n",
    "        a_sum += 1\n",
    "    if torch.argmax(result[2]) == label[2]:\n",
    "        m_sum += 1\n",
    "\n",
    "print(g_sum, a_sum , m_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "36588613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제로 확인(in train set : idx = 0~18900)\n",
    "image, label = dataset_mask[3333]\n",
    "image = image.unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "device = torch.device('cuda')\n",
    "model = basemodel_resnet34.to(device)\n",
    "model.eval()\n",
    "\n",
    "result = model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "04649d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- (Gender , Age, Mask) <--Pred  //  Real--> (Gender, Age, Mask) ----\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((tensor(0, device='cuda:0'),\n",
       "  tensor(0, device='cuda:0'),\n",
       "  tensor(0, device='cuda:0')),\n",
       " (0, 0, 0))"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('---- (Gender , Age, Mask) <--Pred  //  Real--> (Gender, Age, Mask) ----')\n",
    "(torch.argmax(result[0]), torch.argmax(result[1]), torch.argmax(result[2])), (label[0], label[1], label[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fa05a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터와 예측라벨, 실제라벨 확인\n",
    "print(torch.argmax(result[0]) * 3 + torch.argmax(result[1]) + torch.argmax(result[2]) * 6, label[0]*3 + label[1] + label[2]*6)\n",
    "transforms.ToPILImage()(image.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6bcc42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfdaa03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Level1_PJ",
   "language": "python",
   "name": "aistage1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
